{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification: German Credit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset containing 1000 samples, with features such as credit amount and duration, residence, gender, age etc, which are used to predict the credit rating of a person into good or bad. The credit rating is thus the target label with class 0 indicating bad credit rating and class 1 indicating good credit rating.\n",
    "\n",
    "A preprocessed version of the dataset is used for these experiments, where all categorical variables have already been onehot-encoded, while the numerical features have their raw unnormalized values.\n",
    "\n",
    "The aim is to build a CPO model for training a binarized neural network for this dataset and finally improve the model by adding constraints that ensure robustness wrt to perturbations of the training examples. In this dataset, robustness is defined with respect to the credit amount and credit duration variables, imposing constraints that small perturbations to these variables alone should not cause a change in the predicted class. \n",
    "\n",
    "Keeping in mind the issue of scalability that arises while working with CPO, only small samples have been used for training, while all remaining examples are a part of the validation or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 09:54:52.910002: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-21 09:54:52.937829: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-21 09:54:52.938521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 09:54:56.796146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import larq as lq\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from larq.layers import QuantDense\n",
    "from larq import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "import shap\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_rating</th>\n",
       "      <th>credit_duration_months</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>installment_rate(perc_disp_income)</th>\n",
       "      <th>residence_duration</th>\n",
       "      <th>age</th>\n",
       "      <th>bank_credits</th>\n",
       "      <th>dependents</th>\n",
       "      <th>telephone_yes</th>\n",
       "      <th>foreign_worker_yes</th>\n",
       "      <th>...</th>\n",
       "      <th>other_credits_bank</th>\n",
       "      <th>other_credits_none</th>\n",
       "      <th>other_credits_stores</th>\n",
       "      <th>apartment_type_for_free</th>\n",
       "      <th>apartment_type_own</th>\n",
       "      <th>apartment_type_rent</th>\n",
       "      <th>occupation_highly-qualified</th>\n",
       "      <th>occupation_skilled-employee</th>\n",
       "      <th>occupation_unemployed-non-resident</th>\n",
       "      <th>occupation_unskilled-resident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>14027</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1882</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1473</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3069</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>763</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_rating  credit_duration_months  credit_amount  \\\n",
       "0              0                      60          14027   \n",
       "1              0                      18           1882   \n",
       "2              1                      18           1473   \n",
       "3              1                      24           3069   \n",
       "4              1                      12            763   \n",
       "\n",
       "   installment_rate(perc_disp_income)  residence_duration  age  bank_credits  \\\n",
       "0                                   4                   2   27             1   \n",
       "1                                   4                   4   25             2   \n",
       "2                                   3                   4   39             1   \n",
       "3                                   4                   4   30             1   \n",
       "4                                   4                   1   26             1   \n",
       "\n",
       "   dependents  telephone_yes  foreign_worker_yes  ...  other_credits_bank  \\\n",
       "0           1              1                   1  ...                   0   \n",
       "1           1              0                   1  ...                   1   \n",
       "2           1              1                   1  ...                   0   \n",
       "3           1              0                   1  ...                   0   \n",
       "4           1              1                   1  ...                   0   \n",
       "\n",
       "   other_credits_none  other_credits_stores  apartment_type_for_free  \\\n",
       "0                   1                     0                        0   \n",
       "1                   0                     0                        0   \n",
       "2                   1                     0                        0   \n",
       "3                   1                     0                        1   \n",
       "4                   1                     0                        0   \n",
       "\n",
       "   apartment_type_own  apartment_type_rent  occupation_highly-qualified  \\\n",
       "0                   1                    0                            1   \n",
       "1                   0                    1                            0   \n",
       "2                   1                    0                            0   \n",
       "3                   0                    0                            0   \n",
       "4                   1                    0                            0   \n",
       "\n",
       "   occupation_skilled-employee  occupation_unemployed-non-resident  \\\n",
       "0                            0                                   0   \n",
       "1                            1                                   0   \n",
       "2                            1                                   0   \n",
       "3                            1                                   0   \n",
       "4                            1                                   0   \n",
       "\n",
       "   occupation_unskilled-resident  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('german_credit_fullOneHot.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 62 columns):\n",
      " #   Column                                                         Non-Null Count  Dtype\n",
      "---  ------                                                         --------------  -----\n",
      " 0   credit_rating                                                  1000 non-null   int64\n",
      " 1   credit_duration_months                                         1000 non-null   int64\n",
      " 2   credit_amount                                                  1000 non-null   int64\n",
      " 3   installment_rate(perc_disp_income)                             1000 non-null   int64\n",
      " 4   residence_duration                                             1000 non-null   int64\n",
      " 5   age                                                            1000 non-null   int64\n",
      " 6   bank_credits                                                   1000 non-null   int64\n",
      " 7   dependents                                                     1000 non-null   int64\n",
      " 8   telephone_yes                                                  1000 non-null   int64\n",
      " 9   foreign_worker_yes                                             1000 non-null   int64\n",
      " 10  telephone_no                                                   1000 non-null   int64\n",
      " 11  foreign_worker_no                                              1000 non-null   int64\n",
      " 12  account_balance_little                                         1000 non-null   int64\n",
      " 13  account_balance_moderate                                       1000 non-null   int64\n",
      " 14  account_balance_na                                             1000 non-null   int64\n",
      " 15  account_balance_rich                                           1000 non-null   int64\n",
      " 16  previous_credit_payment_status_all_credits_paid_duly           1000 non-null   int64\n",
      " 17  previous_credit_payment_status_delay_paying_in_past            1000 non-null   int64\n",
      " 18  previous_credit_payment_status_existing_credits_paid_till_now  1000 non-null   int64\n",
      " 19  previous_credit_payment_status_no_credits_taken                1000 non-null   int64\n",
      " 20  previous_credit_payment_status_other_credits_existing          1000 non-null   int64\n",
      " 21  credit_purpose_business                                        1000 non-null   int64\n",
      " 22  credit_purpose_car(new)                                        1000 non-null   int64\n",
      " 23  credit_purpose_car(used)                                       1000 non-null   int64\n",
      " 24  credit_purpose_domestic_appliances                             1000 non-null   int64\n",
      " 25  credit_purpose_education                                       1000 non-null   int64\n",
      " 26  credit_purpose_furniture/equipment                             1000 non-null   int64\n",
      " 27  credit_purpose_others                                          1000 non-null   int64\n",
      " 28  credit_purpose_radio/television                                1000 non-null   int64\n",
      " 29  credit_purpose_repairs                                         1000 non-null   int64\n",
      " 30  credit_purpose_retraining                                      1000 non-null   int64\n",
      " 31  savings_little                                                 1000 non-null   int64\n",
      " 32  savings_moderate                                               1000 non-null   int64\n",
      " 33  savings_na                                                     1000 non-null   int64\n",
      " 34  savings_quite_rich                                             1000 non-null   int64\n",
      " 35  savings_rich                                                   1000 non-null   int64\n",
      " 36  employment_duration_1<_<4_year                                 1000 non-null   int64\n",
      " 37  employment_duration_4<_<7_year                                 1000 non-null   int64\n",
      " 38  employment_duration_<1_year                                    1000 non-null   int64\n",
      " 39  employment_duration_>7_year                                    1000 non-null   int64\n",
      " 40  employment_duration_unemployed                                 1000 non-null   int64\n",
      " 41  marital_status_female:divorced/married                         1000 non-null   int64\n",
      " 42  marital_status_male:divorced                                   1000 non-null   int64\n",
      " 43  marital_status_male:married                                    1000 non-null   int64\n",
      " 44  marital_status_male:single                                     1000 non-null   int64\n",
      " 45  guarantor_co-applicant                                         1000 non-null   int64\n",
      " 46  guarantor_guarantor                                            1000 non-null   int64\n",
      " 47  guarantor_none                                                 1000 non-null   int64\n",
      " 48  current_assets_car_other                                       1000 non-null   int64\n",
      " 49  current_assets_life_insurance                                  1000 non-null   int64\n",
      " 50  current_assets_real_estate                                     1000 non-null   int64\n",
      " 51  current_assets_unknown                                         1000 non-null   int64\n",
      " 52  other_credits_bank                                             1000 non-null   int64\n",
      " 53  other_credits_none                                             1000 non-null   int64\n",
      " 54  other_credits_stores                                           1000 non-null   int64\n",
      " 55  apartment_type_for_free                                        1000 non-null   int64\n",
      " 56  apartment_type_own                                             1000 non-null   int64\n",
      " 57  apartment_type_rent                                            1000 non-null   int64\n",
      " 58  occupation_highly-qualified                                    1000 non-null   int64\n",
      " 59  occupation_skilled-employee                                    1000 non-null   int64\n",
      " 60  occupation_unemployed-non-resident                             1000 non-null   int64\n",
      " 61  occupation_unskilled-resident                                  1000 non-null   int64\n",
      "dtypes: int64(62)\n",
      "memory usage: 484.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking if the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFsCAYAAAAdTcpMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLC0lEQVR4nO3deVxU5eIG8Gc2Bhg2AWUXRMR9V1zQNJdcSm0zrcwls7J9Ucur5ZKmXbdullp21VtaLlfT8pdb5pb7hrmCC+CGgiggIDDL+/uDmOvEjjOcMzPP9/PhI8ycOfMMI/PMOfOe9yiEEAJEREQkKaXUAYiIiIiFTEREJAssZCIiIhlgIRMREckAC5mIiEgGWMhEREQywEImIiKSARYyERGRDLCQiYiIZICFTGQlSUlJUCgUUCgUWLZsWbHrJ0+ebL6eSsffEzkrFjJJoqCgAD/++COGDh2KBg0awM/PDxqNBv7+/mjdujVGjx6N3377DSaTSeqodisiIsJcbPd/Ff2eO3TogPHjxyMpKUnqqEQEFjJJYN26dahfvz6ee+45fP/994iPj8ft27dhMBiQnp6OY8eOYdGiRejZsycaNmyI//u//5M6crXo2rUrFAoFunbtatP7Kfo9HzhwADNnzkSjRo3w3Xff2fQ+ly1bZn5DwDcARCVjIVO1+uSTT/DUU0+ZX5R79uyJ+fPnY/v27Th69Ci2bduGL7/8Er169YJSqURCQgImTJggbWgrmTx5MoQQqO7zuQQHB+PkyZPmr0OHDuGHH35Anz59AAD37t3Diy++iP3791drrtJI9Xsikppa6gDkPJYuXYqPP/4YAFCrVi2sXr0aXbp0KbZcjx498Prrr+PUqVN49913kZaWVt1RHYpGo0GTJk0sLmvbti2effZZvP/++5g7dy6MRiOmT5+OjRs3SpSSiLiFTNXi2rVreOONNwAAOp0Ou3btKrGM79ekSRNs2bIFY8aMqY6ITumTTz6BVqsFAOzYsYOf2RNJiIVM1WLevHnIzc0FAEydOhUNGjSo0O2USiWGDBlicVlJo5nXrVuHvn37Ijg4GGq1usTPYS9cuIB3330XTZs2hbe3N9zc3BAZGYnhw4fjyJEj5WYxGo1YsGAB2rVrBy8vL3h7e6NVq1aYPXs28vPzy719aaOHhw8fDoVCgV27dgEAdu3aVWwgVkRERLnrrwp3d3dERkYCAHJzc5Genl7icqdOncK0adPQq1cvhIaGQqvVwsPDA/Xq1cOwYcNw4MCBEm+3c+dOKBQKjBgxwnxZnTp1ij2+nTt3mq8vb5R10WC14cOHAwDi4+MxatQoREREQKvVIiAgAE888USpme5nMBjwxRdfICYmBl5eXvDx8UGbNm0wb948FBQUlDtynsiqBJGNmUwm4e/vLwAInU4nsrKyHmh9iYmJAoAAIJYsWSJeeOEF889FX126dLG4zaxZs4RGoym2XNGXQqEQH330Uan3effuXdG5c+dSb9+qVStx7Ngx889Lly4tto5JkyaZr7/fsGHDSl1v0Vd4eHilf0/h4eEVum3z5s3N93Pnzp1i1+/YsaPcfADEhx9+WOXb7tixo9zf098f17Bhw8S6deuEu7t7ietUqVRi5cqVpT7uzMxM0b59+1IzxcTEiOPHj5f5nBJZEz9DJps7ffo0bt26BQDo3LkzPD09rbbuzz//HH/++Sc6d+6M0aNHIzo6GhkZGRYjeWfNmoVx48YBAJo1a4bRo0ejXr168PHxQXx8PL788kvs378fn3zyCfz9/fHWW28Vu58hQ4Zgz549AICYmBi8++67qFevHm7evIlly5ZhzZo1eOWVV6r0GKZPn44xY8ZgxIgROHLkCNq0aYOlS5daLOPi4lKldZfHYDDg/PnzAABvb2/4+PiUuIxOp8Ojjz6Kbt26oUGDBvDy8kJqaipOnz6NL774AsnJyZg5cyaio6Mttobbtm2LkydPYsOGDZg4cSIAYMuWLQgODra4jzp16lQ6+8mTJ7Fq1SoEBQXh/fffR5s2bSCEwJYtWzBz5kzk5eXh5ZdfRrdu3VCzZs1itx88eLB5Kzo2NhZvvvkmoqKikJaWhuXLl2PFihV49dVXK52LqMqkfkdAjm/58uXmrYwJEyY88Pru30IGIIYOHSpMJlOJy54+fdq8ZTxp0qQSlzMajWLIkCECgPDw8BC3b9+2uH7jxo3m++rbt6/Q6/XF1jFlyhSLTJXZQi7SpUuXErfuq6oiW8hz5swxZxo5cmSJy6SlpZW45VwkPz9f9OzZ03xfBoOh2DJLly41309iYmKZuSu6hQxAtG7dWmRmZhZb5v7/c3Pnzi12/fr1683XP/nkk8JoNBZbZvbs2eU+p0TWxM+Qyebu/1yyVq1aVl23j48Pvvzyy1I/b5wzZw70ej3atGmDSZMmlbicUqnE/PnzodVqkZ2djf/+978W1y9YsAAAoNVqsXjxYqjVxXcsTZw4sdhIZrm6d+8eTp06hbFjx+KDDz4AUPi8/OMf/yhxeX9//xK3nIu4uLhg1qxZAIDk5GTExcVZO3KplixZAi8vr2KXP/fcc+at8KI9G/dbtGgRAMDNzQ2LFi2CUln8pfC9995Dq1atrJyYqHQsZLK5u3fvmr/X6XRWXXe/fv3K3AX+yy+/AACeeuqpMqdi9PHxQdOmTQHA4nhco9FoHnD0yCOPFNvVWkSpVGLYsGGVjV8tkpOTLQZQubu7o2nTppg9ezYMBgO6du2KHTt2mAd3lSc/Px+XL1/GmTNncOrUKZw6dcrimOETJ07Y6qFYaNq0KZo1a1bidQqFAi1btgQAXLp0yeI6g8FgHkDXu3fvEndnF63jhRdesGJiorLxM2SyufsLMycnx6rrLu0FGSgsoqJjmMePH4/x48dXaJ03btwwf3/x4kXz6PC2bduWebuYmJgKrV9OvL298frrr6NRo0ZlLpeTk4MvvvgCK1euxOnTp2E0Gktdtmi8gK2VN1Lf19cXgOUbQqDwOb137x4AoHXr1mWuo02bNg+QkKhyWMhkc35+fubvb968adV116hRo9TrUlNTq7TOogIGgNu3b5u/L293e0BAQJXuz9aCg4OxZcsW889paWk4fPgw5s2bhxs3buCZZ57Bjz/+iEGDBpV4+6SkJHTr1g2JiYkVur+isrM1d3f3Mq8v2g399zcPd+7cMX9f2tZxRa8nsiYWMtlc8+bNzd8fO3bMqutWqVSlXnf/C/HHH3+MgQMHVmidpe1Wt9ezD5U0U9fDDz+MIUOGICYmBteuXcPLL7+MDh06oHbt2sVu/8ILLyAxMdF8PPHgwYPRsGFD1KxZEy4uLlAoFDCZTObnQnDKS6IqYSGTzTVu3Bj+/v64desW9uzZg6ysrBIH4ljb/VvmJZVSRdy/BV7e1r21t/5tLTg4GIsWLUK/fv2QlZWFCRMm4Pvvv7dY5ty5c/jjjz8AAP/4xz8wbdq0Etd1/54Eubv/OS1vWlZO20rViYO6yOYUCoV5wFNOTg6+/fbbarnfyMhIeHt7AwD27t1bpXXUrVsXbm5uAIDDhw+XuWx515dHii3wxx57DJ06dQIA/PDDDzhz5ozF9adPnzZ/X9oubQDlznQmp70LdevWhaurKwDg6NGjZS5bkRnciKyFhUzV4t133zV/5vfxxx/j3LlzFbqdyWTCihUrqnSfKpUKffv2BQBs3boVZ8+erfQ67p+Gc+vWrUhJSSk153/+858q5SxSVBIVmYbTmj766CMAhY9h+vTpFtcZDAbz92UNyCs6jKg0RY8NqP7H93dqtRoPPfQQAGDz5s2lbgULIYrtMSCyJRYyVYuQkBB8+eWXAApf2Lt06WI+9KQ0Z86cQe/evc3HuFbF+PHjoVKpYDKZ8PTTT+Pq1aulLms0GrFixYpiy4wePRpAYZG88sorJY4wnjFjBk6ePFnlnAAQFBQEoPAwner8HPaRRx4xjyZetWoVLly4YL6uXr165u9Lm8t54cKF2LBhQ5n3UfTYgMJRzlIrmlXt3r17ePXVV0s8qcbcuXOtPuaBqCwsZKo2I0aMwNSpUwEUjoDu2rUrevXqhQULFmDHjh04fvw4tm/fjoULF+Kxxx5Ds2bNsG3btge6z6LjbYHCgm/SpAnGjRuHzZs34/jx49i/fz9+/PFHvPXWWwgLC8OQIUOQkZFhsY5+/fqhX79+AAqPa46NjcWqVatw7NgxbN68GYMHD8bEiRMf+BCZjh07Aij83bz33ns4evQoLly4gAsXLiA5OfmB1l2eonNOG41GzJgxw3x5y5YtzZ+9f/311xg0aBA2btyIo0ePYsOGDRg4cCBee+01xMbGlrn+li1bmreSP/roI2zbtg0JCQnmx1ddI7OLPPnkk3jkkUcAFJ6Y5KGHHsLq1atx7NgxbNmyBS+88ALGjBljcSibnHa7k4OSdqIwckZr164VERERFTrpQOPGjcWWLVssbn//1JkVnc7wm2++KfUkBPd/ubi4iPPnzxe7fVZWloiNjS31di1bthRHjx59oKkz7969KyIjI6v95BJCFJ4ApHHjxgKA0Gg0Ijk52Xzd8ePHRY0aNUp97E2bNhXXr183/zxp0qQS72PcuHFWP7lEWYpO2lHa479z546IiYkp8zk9cuSI+eeyTlRBZA3cQqZq9+STTyI+Ph4rVqzAkCFDUL9+fdSoUQNqtRq+vr5o1aoVXnvtNfz+++84efKkeUvmQYwaNQqXLl3ClClTEBsbC39/f6jVauh0OkRHR+Opp57CokWLcO3aNURFRRW7vaenJ3bu3In58+ejbdu28PDwgKenJ1q0aIEZM2Zg37595okoqsrDwwP79u3D22+/jYYNG5Z7nK01KRQK89SZer0en332mfm6Fi1aIC4uDq+++irCw8Oh0Wjg6+uLmJgYzJ49G4cOHbLYJV2amTNnYvHixejcuTN8fX3LPGStOvj4+OCPP/7AvHnz0Lp16xKf0/szFg0QJLIVhRA8aJCIqCTLly83T5954cIF1K1bV+JE5Mi4hUxEVIoff/wRQOGMXRWd65uoqljIRGRVkydPRosWLaSOYeHvmYYPH45evXqVOZjs22+/xa+//goAGDp0qM0HdS1btqzMs2qR42MhEzmh4cOHW5wBys/PD71798aff/5ZbRnWrl2Lrl27wtvbGx4eHmjWrBmmTp1aLbN+/etf/8KAAQMQHh6Ot956C40bN8azzz6Lw4cPY+XKlXjiiScwatQoAIVzlBedmKRr167m35mrqyuio6MxY8aMSh+mFhERgc8//9ziskGDBiEhIcEqj4/sEwuZyEn17t0bKSkpSElJwfbt26FWq/HYY49Vy31PmDABgwYNQtu2bbFp0yacOnUKc+bMwYkTJ0qdjKOgoMBq9+/t7Q13d3ekpaVh/vz5OHPmDFauXImYmBg8++yzWL9+PYDC46c3bdpkMQ3rqFGjkJKSgvj4eIwfPx4ff/xxuROjVISbm5vVzxdO9oWFTOSktFotAgMDERgYiBYtWuDDDz/ElStXLGau+uCDDxAdHQ13d3dERkbio48+gl6vt1jPzJkzERAQAE9PT4wcORJ5eXll3u+hQ4fw6aefYs6cOZg1axY6duyIiIgI9OzZE2vXrjVPs1q0m/nbb79FnTp1zMcxZ2Rk4KWXXkLNmjXh5eWFbt26FTsHc3mZhg8fjlWrVmHhwoUICwsrlrFt27b45z//ifj4ePN5lYu4u7sjMDAQ4eHhGDFiRLHj5S9evIgBAwYgICAAHh4eaNu2LX777Tfz9V27dkVycjLeffdd89Y2UHyXddHj//777xEREQFvb28MHjzY4nSSd+/exfPPPw+dToegoCDMmzcPXbt2xTvvvFPmc0DyxEImImRnZ2P58uWIioqy2Br09PTEsmXLcObMGfzrX//C4sWLMW/ePPP1q1evxuTJk/Hpp5/iyJEjCAoKwoIFC8q8rxUrVsDDwwOvvfZaidffX0oXLlzA2rVrsW7dOsTFxQEABg4ciNTUVGzatAlHjx5Fq1at0L17d/Ou7opm0mq1ePXVV3Hy5El06NDBvOWbkpKC/fv3Y+zYsRbn8v47IQT27NmDc+fOwcXFxeJ32bdvX2zfvh3Hjx9H79690a9fP1y+fBlA4UQkoaGhmDp1qvn+SnPx4kWsX78eGzduxMaNG7Fr1y7MnDnTfP17772HvXv34ueff8a2bduwZ88ezi5mz6Q9DJqIpDBs2DChUqmETqcTOp1OABBBQUHi6NGjZd5u1qxZonXr1uafO3ToIF577TWLZdq1ayeaN29e6jr69OkjmjVrVm7GSZMmCY1GI1JTU82X7dmzR3h5eYm8vDyLZevWrSu+/vrrCmcaNmyYGDBggPnnLl26iLfffrvcTF26dBEajUbodDqh0WgEAOHq6ir27t1b5u0aN24s5s+fb/45PDxczJs3z2KZpUuXCm9vb/PPkyZNEu7u7iIrK8t82dixY0W7du2EEIWT1Wg0GrFmzRrz9RkZGcLd3b1Cj4Xkh1vIRE7q4YcfRlxcHOLi4nDo0CH06tULffr0sZimc9WqVYiNjUVgYCA8PDwwceJE85YeAJw9exbt2rWzWG+HDh3KvF9RiQFQ4eHhqFmzpvnnEydOIDs7G35+fvDw8DB/JSYmmufIrkqmynj++ecRFxeHvXv3ok+fPpgwYYJ52lOgcAt5zJgxaNiwIXx8fODh4YGzZ89a/N4qKiIiwmIrPSgoCKmpqQAK5zzX6/UW03t6e3ujfv36D/DoSEo8HzKRk9LpdBazkn377bfw9vbG4sWLMW3aNOzfvx/PP/88pkyZgl69esHb2xsrV67EnDlzHuh+o6Oj8ccff0Cv10Oj0ZSb8X7Z2dkICgrCzp07iy1bXYcMeXt7m39vq1evRlRUFNq3b48ePXoAAMaMGYNt27Zh9uzZiIqKgpubG55++ukqDUr7++9HoVCUeCIMcgzcQiYiAIUv9kql0nxs7r59+xAeHo4JEyagTZs2qFevXrGTXDRs2BAHDx60uOzAgQNl3s9zzz2H7OzsUj9r/vvJPe7XqlUr3LhxA2q1GlFRURZf/v7+Vc7k4uJS4lm8yuPh4YG3334bY8aMMW/57927F8OHD8cTTzyBpk2bIjAwEElJSVa5v/tFRkZCo9FYnIc7MzOTh07ZMRYykZPKz8/HjRs3cOPGDZw9exZvvvkmsrOzzWe2qlevHi5fvoyVK1fi4sWL+OKLL/DTTz9ZrOPtt9/GkiVLsHTpUiQkJGDSpEk4ffp0mffbrl07jBs3Du+//z7GjRuH/fv3Izk5Gdu3b8fAgQPLPK90jx490KFDBzz++OPYunUrkpKSsG/fPkyYMAFHjhypcqaIiAgcPHgQSUlJuHXrVqW2Ql955RUkJCRg7dq1AAp/b0WD0E6cOIHnnnuu2PoiIiKwe/duXLt2Dbdu3arwfd3P09MTw4YNw9ixY7Fjxw6cPn0aI0eOhFKp5Jmp7BQLmchJbd68GUFBQQgKCkK7du1w+PBhrFmzBl27dgUA9O/fH++++y7eeOMNtGjRAvv27cNHH31ksY5Bgwbho48+wrhx49C6dWskJyebzx9dls8++ww//PADDh48iF69eqFx48Z477330KxZM/NhTyVRKBT49ddf8dBDD2HEiBGIjo7G4MGDkZycjICAgCpnGjNmDFQqFRo1aoSaNWtW6vNeX19fDB06FJMnT4bJZMLcuXNRo0YNdOzYEf369UOvXr3QqlUri9tMnToVSUlJqFu3rsVn5JU1d+5cdOjQAY899hh69OiB2NhYNGzY0HyIGNkXnlyCiMhB5OTkICQkBHPmzMHIkSOljkOVxEFdRER26vjx4zh37hxiYmKQmZmJqVOnAgAGDBggcTKqChYyEZEdmz17NuLj4+Hi4oLWrVtjz5495gFuZF+4y5qIiEgGOKiLiIhIBljIREREMsBCJiIikgEWMhERkQxwlDWR3JhMQPZN4G4KkJcJFOQABdl/feUA+dn/u0wYAYUSUKgApeq+f5WFX2pXwNUL0HoV/uvqA7j7ATp/wN0fULuUG4eIqgcLmai6GfKBWwmFX5lXgawUIOtaYQFnXS8sY5OherK4egM+tQGfcKBGxF//hv/vX41b9eQgIh72RGQzhnzg1nkg7RyQerbw37RzwO3Ewi1be+ARCAQ0BgKbAkHNgMDmgF9dgHMlE1kdC5nIWrKuA5cPAFcOFn7dOFl9W7rVycXjvpJuDtTuCPhHlX87IioTC5moKoQAbvwJXD4IXDkAXDkEZF6ROpV0PIOAiE5/fXUu3IomokphIRNVVF4mcPF3IGErcOE3ICdV6kTy5RVSWM51HgLq9QI8qn5GIyJnwUImKkvqWSBhC3B+W+GWsCPugrY1hRIIjQEaPgY0eAzwrSN1IiJZYiET/V3qWeDP1cCptUBGstRpHE+txn+V86OFn0ETEQAWMlGhzGvAqf8Cf64Bbp6UOo3zqBEBNH8WaPE84BMmdRoiSbGQyXnlZQJnNhRuDSfvBYRJ6kTOS6Es/Ly5xRCgYT9A4yp1IqJqx0Im53P9OHD4W+DUOkCfK3Ua+jtXb6DJ00DL54GQ1lKnIao2LGRyDoYC4PQ64ODXwPVjUqehigpqDnR4A2j8JKDixILk2FjI5Niy04Aj/waOLCmckpLsk1co0O4VoPXwwjm5iRwQC5kcU8YV4I+5wPEVgDFf6jRkLVovoNVQoN2rHARGDoeFTI4l4zKwZw4Q9wNgLJA6DdmKUg00ehzo8gFQM1rqNERWwUImx3An6a8i/hEw6aVOQ9VFoQKaDQIeHl941ioiO8ZCJvt2JwnYPQs4sZKzaDkzlQvQahjw0FjAM0DqNERVwkIm+5SXVVjEBxdx1zT9j8YdiHkZ6PQO4FZD6jRElcJCJvtiMgHHvwN+nwbkpEmdhuRK6w08NAZoPxpQaaROQ1QhLGSyH4l7gM3jObUlVZxfPaDPTCCqh9RJiMrFQib5u50IbJ0InNsodRKyV/UfBfp8xkOlSNZYyCRfJiOwbz6wcwZgyJM6Ddk7jTvQZVzhzF/cjU0yxEImebp5GtjweuG800TWVLMh0H8+ENZW6iREFljIJC+GAmDPbGDPXB5PTLajUAGxbwNdxwNqF6nTEAFgIZOcXDsKbHgDSD0jdRJyFgFNgCe+BgKbSJ2EiIVMMmA0ADumAXu/AIRR6jTkbFQuQNcPgdh3AKVK6jTkxFjIJK2My8B/XwSuHpY6CTm70BjgiUWAX12pk5CTYiGTdM78DPz8BpCXKXUSokIad+DRuUCLZ6VOQk6IhUzVz5APbJkAHF4sdRKikrUeUXjcslordRJyIixkql7pF4E1w4AbnG2LZC64FfDMf3gWKao2LGSqPmd+BtaPBgqypU5CVDFuvsBTizn1JlULFjJVj12zgB3TAfC/G9kZhRLo8kHhl0IhdRpyYCxksi19XuHArZNrpE5C9GCiewNPLwFcdFInIQfFQibbyU4FVj7HQ5rIcQS1AJ5bDXgGSJ2EHBALmWwj5U/gx2eBrKtSJyGyLu/awPNrgFoNpE5CDoaFTNYXvwn470hAnyN1EiLbcPUGBq0A6nSWOgk5EBYyWdeJVcCG1wCTQeokRLalcgEGfAU0e0bqJOQglFIHIAdy+Fvgp1dYxuQcjAXAupeBPXOkTkIOglvIZB175gLbp0idgkgaD40Duk2QOgXZORYyPbhtk4C9n0udgkhand4FekyWOgXZMRYyVZ3JBPz6PnBkidRJiOSh45vAI9OkTkF2ioVMVSMEsOENIG651EmI5KX9a0DvGVKnIDvEQV1UNZs+YBkTleTAAuDXsYVvWokqgYVMlbd9KnDoa6lTEMnXoW8KS5moEljIVDl/zONhHkQVcXgxsHOm1CnIjrCQqeIOLQZ+myx1CiL7sXMGBz1ShXFQF1VM3I+F5zLm6ROJKkehAp75D9Cwn9RJSOZYyFS+hK3Aj4MBYZQ6CZF9UrsCQ9YBEbFSJyEZ4y5rKtuNk8B/R7CMiR6EIa/w7Gc3T0udhGSMhUyly0oBfhgEFGRLnYTI/uVnAsufAjJ5SlIqGQuZSpSnN+LMlm+BrGtSRyFyHHdTgFVDAH2e1ElIhljIVKIxa06g79FW2BT6DoRCJXUcIsdx/Tiw8V2pU5AMcVAXFfPl7+cxe2uC+ec3wpLwftYMKPLvSpjKOSw8XICFRwqQlGECADSupcLHD7mgTz0NACDPIPD+ljysPG1AvkGgV5QaC/q6IsCj9PfWQghM2pmPxcf0yMgTiA1TYeGjrqjnV/hGK98g8NIvedhwTo9ADyUWPOqKHpFq8+1n7c3H5UwT5vd1s+Ejd0K9PwPavyp1CpIRbiGThW1nbmLOtgSLy768EoGX1DNg8KotUSrnEeqlwMweWhx9WYcjL+vQLUKFASvv4XRq4aC6dzfn4ZcEA9YMdMOu4Tpcvyvw5Op7Za7zn3sL8MXBAix61BUHX9JB56JAr+W5yDMUvhf/5qgeR68bsX+kDi+31uC5tfdQ9D498Y4Ji4/pMb27q20fuDPaOgFI3CN1CpIRFjKZJafn4N1VcSVOwbs93RePZE/G3Vptqj+YE+lXX4O+9TSo56dCtJ8K07u7wsMFOHDViMw8gX8f12NuL1d0q6NG62AVlg5wxb4rRhy4aihxfUIIfH6wABMf0mJAAw2aBajw3eNuuH5XYP25wtucvWVE//pqNK6lwuttXZCWK3Art/A/wej/u4fPemjhpVVU2+/AaZgMwJrhQMYVqZOQTLCQCQCgN5rw1o/HkZ1f8gs7AFzKdUX7a+8gKXRANSZzXkaTwMpTeuTogQ5hKhxNMUJvgsXu5Ab+KtT2VmD/lZIPS0vMELiRLSxu4+2qQLtQlfk2zQNU+OOyEff0AlsuGhDkoYC/uwIr/tTDVa3AEw01tn2gziz3FrDqeUBf9l4Ocg7q8hchZzBrSzxOXM0sd7kcoxJdLwzC0nqh6HplARScucvqTt40osO/c5BnADxcgJ8GuaFRTRXibujhogJ8XC23VgN0CtzILvl5uJFtMi9T7DY5hde92FKDP28a0WhBNvzdFVg90A138oCPd+Zh5zAdJv6eh5Wn9Kjrq8SS/m4I8eL7eKtKOQFsHg/0+1zqJCQx/mURdiekYfGeS5W6zYjzsVgYMBlCo7NRKudV31+JuFc9cPAlHUa3ccGw9Xk4k2a7iVk0KgW+etQNiW974vAoD3Sqrcb7W/PwVowLjt8wYv05A0686oH2ISq8tZmH69jE0aXAuV+lTkESYyE7uVvZ+Xhv9Ykqnbr1n8n18KbrDBg9gq0fzIm5qBSI8lWidbAKM3q4onmAEv86UIBADwUKjEBGnuWTdTNHINCj5M94A/8afX0zp4Tb6Er+89+RaMDpVCPeiHHBziQj+tZTQ+eiwDONNdiZxBnbbObnN4C7N6VOQRJiITsxIQTeW30Ct7Lzq7yOjWn+eCxvKnL9m1sxGd3PJIB8I9A6SAWNEth+6X+f88ffMuJypkCHsJKPFa/jo0Cgh8LiNln5AgevGku8TZ5B4PVf8/D1Y25QKRUwmgD9Xx2sNxV+rk02kpteeAIXHonqtFjITmzxnkvYnZD2wOs5m+2ODjffx7WQPlZI5dzG/5aH3ckGJGWYcPKmEeN/y8POJCOeb6qBt6sCI1tq8N7WPOxINODodSNGbMhDh1AV2ofeN9Dry2z8dFYPAFAoFHinnQum7cnHz/F6nLxpxNCf7iHYU4HHGxQfQvLJrnz0radGy6DCso6trcK6c3r8edOILw8VILY2h53Y1MXtwIGFUqcgifCvy0mdvJqJWVvirba+TL0asRdfwI/1wtDhyjdWW6+zSc0RGPrTPaRkC3hrFWgWoMSWIe7oWbfwT3Veb1cot+ThqdW5yDcCveqqseBRy2OE49NNyMz/31bWuFgX5OgFXv4lDxl5Ap1qq7B5iDtc1Za7uU+lGrH6jAFxr/xvXMDTjdTYmaRG56U5qO+nxA9Pudvw0ROAwnOOR3YBAhpLnYSqGWfqckJ6owmPfrEHCTdtc9KIj+ucxYi0WVAYOACIqEpqNQJe3gmotVInoWrEXdZO6Jvdl2xWxgAwNbEhxnrMgFFXy2b3QeTQUs8Au2dLnYKqGbeQnUxyeg4embcb+QaTze+rhVc2fvT8F9zSeQ5YokpTaoBXdgMBjaROQtWEW8hOZuL6U9VSxgAQl+WBTmnjkBrcvVruj8ihmPTAL28Bpur5eyXpsZCdyPrj17Dn/K1qvc/0Ag06JI7A8drDqvV+iRzBZQRg/eHzUsegasJCdhIZuQX4ZOMZSe7bKJR4IqEXVgaPh1C5SJKByJ7ovetghv9MPHThOXy8KemB5gog+8FCdhKf/noW6TkFkmb48FJTTPL+FCY3P0lzEMmVULngUNhItEqfgq+vFp7uNCvPgE//76zEyag6sJCdwMFL6Vhz9KrUMQAA310PxrNiOvJrREsdhUhWsmq1xUjXeXjmfHfcNVhOEbHu+DUcvJQuUTKqLhxl7eBMJoG+X+zBuRt3pY5iIUhbgI3BS+CXslvqKESSMrnWwFrfURiX2BxClH7e6foBntj0dmcolTw3taPiFrKDWx93TXZlDAAp+S5on/wKzoQ9K3UUIslcDu2HngWzMfZSizLLGADib97FuuPXqikZSYGF7MAKDCbM3ZYgdYxS6U0K9D3fDxtCxkAoOYsrOQ+9dyQ+9Z+Jhy48i4u5bhW+3bxtCcg38IxbjoqF7MCWH0jG1Tv3pI5RrrcvtsJM32kQWm+poxDZ1P8GbU3GN38N2qqMaxn3sPzAZRskIzngZ8gOKjvfgC7/3CH5yOrK6OJ3B9+qZ0OTmSh1FCKry6rVFu/kDMfv6TUeaD1+OhfsGvcwPLTcq+RouIXsoL7ZfcmuyhgAdqXXQPesj5AZ0F7qKERWY3KtgTXB49D8yjsPXMYAkJ5TgMW7L1khGckNt5AdUNrdfHSdtQM5Bfb5WZObyohfItcj6spaqaMQPZDLof0w4vrjlfqcuCJ0LirsHvcw/Dx4NihHwi1kBzT/9/N2W8YAcM+oQo/zT2Fr6FsQCv4XJftT1UFbFZVTYMT83y9Yfb0kLW4hO5grt3PRbc5O6I2O8bS+U/sS3s6YCUWB7U4XSWQtQuWCw8EvYGRil2KTe1ibi0qJ7e93QZivu03vh6oPNz8czL//SHSYMgaAzy9HYrR2BgyeoVJHISpTWTNt2UKB0YRv9/CzZEfCQnYgmff0WHPkitQxrG5zmh/65E5Bdq3WUkchKsbag7YqY83Rq8jM1VfrfZLtsJAdyIqDyXb92XFZzue4od31d3A5tJ/UUYjMkkP7V3imLVvILTBixaHkar9fsg0WsoPQG034z74kqWPYVI5BhYcuPIvdYaMhwPl8STpFg7a6XBhsk0FblfGffUnQG02SZiDrYCE7iJ/jruNmlnOcM3Xo+c74OmAShIaDWah6PehMW7ZwMysfv5y4LnUMsgIWsoNY7GSDO2YmR+Md909h9AiSOgo5iayAGIzQVt+grcr4dg9nt3MELGQHsOd8mizP6GRrG27WQv/8qcj1byZ1FHJgJtcaWB38AZpffhs7b1fvoK2KOpOShX0Xbkkdgx4QC9kBLHbid8en7+rQ4eYYpIT0kjoKOaCiQVvjLpV9rmI5cLa9ZI6IhWznLqZlY3dCmtQxJJWpV6PjpaE4FDZS6ijkIOQ0aKuidiak4WIaJ9CxZyxkO7f26FWpI8iCEAo8c747vg+aAKHi/L5UNULlgoNhL6FF+hTZDNqqKCGANUf4emDPWMh2zGQSWH/8mtQxZOWjxMb40GsGTO41pY5CdqZo0Nag892QY1BJHadKNsRdg8nkODP1ORsWsh3bfykd1zPzpI4hO6tSAvG0cTryfBtIHYXsgD0M2qqolMw87LuYLnUMqiIWsh3j7urSHcv0QOdb45EW3E3qKCRj9jRoq6LWHePrgr1iIdupnHwDNp++IXUMWUsr0KB94os4UfsFqaOQzOi9IzHN/zO7GrRVUZtP30BugUHqGFQFLGQ79evJFOQ66LzV1mQUSgxI6IPVIR9AKDVSxyGJ3T9o69urYVLHsYncAiM2neSbdXvEQrZT645xMFdljLvYHFN8psPk5it1FJKIIwzaqqh1x7nb2h6xkO3Q1Tu5OJDIgRuVtex6KJ7HdBT4REkdhaqRIw3aqqj9F9ORknlP6hhUSSxkO/TLiRQIHtlQJfvveOPhjAm4ExgrdRSqBo44aKsiTALYEMcTTtgbFrId+v3cTakj2LVreVq0vzwa58IGSR2FbMSRB21V1LYzfJ2wNyxkO5ORW4BjlzOkjmH38k1K9D4/ABtD34NQOPbnic5EqFxwwMEHbVVU3JUMZOQWSB2DKoGFbGd2JaTByJl4rOaNC23wT/9PILReUkehB5QV0A4jtPMw2AkGbVWE0SSwy8nnubc3LGQ78/u5VKkjOJyFVyIwUj0Deu8IqaNQFZjcfLEq+EM0v/yW0wzaqqgdfL2wKyxkO8J3vLbze3oN9Mz6GFkBMVJHoUpICu2PHvmz8cGlZk41aKuidiWkcW5rO8JCtiPHLt9BRq5e6hgOK+meKzpcfQuXQp+QOgqVo8AnEp/4fYauFwbjUq6r1HFk606uHsev3JE6BlUQC9mOcHe17eUYleh2YSC2h70JoeCfh9wUDdpqeWsK/n3NuQdtVdSOc9yrZi/4imNH+HlQ9Rl5vgO+rDUFwkUndRT6CwdtVQ3fyNsPFrKduJGZh3M37kodw6nMSa6L17QzYPAMkTqKUysatNUs2Xlm2rKmMylZuJnF07TaAxaynTiSfFvqCE5pU5o/+uZORXbNllJHcUr3D9qiqjuYyNcPe8BCthPHkjOkjuC0EnLc0CHlXVwJfVTqKE6Dg7as6/hlDuyyByxkO8GRktK6a1Cj84XnsTfsFQjw8Bpb4aAt2zjO2f3sAgvZDuQbjDh9LUvqGATg+fNd8O/AjyHUzjk/si1x0JbtnLmehXwDz58udyxkO3D6ehYKjCapY9BfpiXVxxjdpzDqAqSO4hA4aMv2CowmnL7ON/Vyx0K2A8eSubtabtbeDMDjBdNwz6+J1FHsGgdtVR/utpY/FrIdOH4lQ+oIVIKTd3XomDoON0J6Sh3F7nDQVvXjwC75YyHbgePcQpatO3o1OlwajiO1X5Q6il3goC3pcAtZ/ljIMnczKw/XM3lQv5wJocDTCT2wIvgfECqt1HFkKyugHYa7ctCWVK5l3EPqXb6WyBkLWebOcCCG3ZhwqQkmek2Hyc1f6iiyYnLzxcrg8WiW/DZ2pXPQlpQ4sEveWMgydyE1W+oIVAkrUoLxjGk68n3rSx1FFooGbX14qanUUQjApbQcqSNQGVjIMsdCtj9HMj3ROf0fSA/qInUUyXDQljwl3WIhy5la6gBUtgtpLGR7lJqvQfvkl7GubiiaXlkhdZxqI1QuOBg8DCMTH+LnxDKUyEKWNW4hyxy3kO2X3qRAv/OPYl3IWAilRuo4Nve/QVsPs4xlioUsbyxkGbuTU4DMe3qpY9ADeu9iS0yrMQ0mVx+po9gEB23Zj+uZ95Cn5xSacsVClrHk27lSRyAr+fe1MAxVzkCBT6TUUawqKXQAB23ZESGA5HS+rsgVC1nGktO5e8mR/HHbG90yPkJGYEepozyw/w3aGsRBW3Ym8RY/BpMrDuqSMb6TdTxX87Rod/k1bKwbgnpX1kgdp9KESosDwUPxEgdt2a1L/BxZtriFLGNXuMvaIeWblOh5/glsCn0HQmE/pZYZ0B7DXefiWQ7asms89Em+WMgylp5TIHUEsqHRF2Iwx/8TCK2n1FHKVDRoq3nyWxy05QDS7uZLHYFKwUKWMRay4/vySgReUs+Awau21FFKxEFbjud2Lo/ckCsWsozdYSE7he3pvngkezLu1mojdRSzAp+6mMpBWw6JryvyxUKWMf7hOI9Lua5of+0dJIUOkDSHUGmxP2wUWt6ajCU8PaJD4uuKfLGQZarAYMLdfIPUMaga5RiV6HphEHaEvQ4BRbXfPwdtOYe7+QbojSapY1AJWMgydSeX72Kd1YjzsVgYMBlCo6uW++OgLefDrWR5YiHLVHo2/2Cc2T+T6+FN1xkwegTb9H6SQgege94sDtpyMrf5hl+WWMgyxS1k2pjmj8fypiLXv7nV133/oK3Ee25WXz/J221uIcsSC1mm+AdDAHA22x0dbr6PayF9rLK+wkFbL3PQlpO7k8NDn+SIhSxTORzQRX/J1KsRe/EF7A97+cHWYx601ZWDtpxcVh4LWY5YyDJlFELqCCQzz57viiVBH0GoK3dcMAdt0d9xlLU8sZBlymRiIVNxUxMbYqzHDBh1tSq0fGLo4+iWx5m2yJLeyNcXOWIhyxT7mErz3xsBeEo/Dff8Gpe6TNGgrYcvPIOke5xpiyxxC1meWMgyZWQjUxnisjzQKW0cUoO7W1zOQVtUEQYWsiyxkGXKxM+QqRzpBRp0SByB47WHASgctDVMO4+Dtqhc7GN5UksdgErGQqaKMAolnkjohX61WuGX5JpSxyE7oaj+mVmpAriFLFN8B0uV8Usqy5gqjn0sTyxkmeIWMhHZCreQ5YmFLFM87ImIbEXBRpYlFrJMublwUA4R2YZWzZd+OeKzIlNerhqpIxCRg/J05XheOWIhyxT/YIjIVviGX55YyDLlyT8YIrIRvr7IEwtZpriFTES24uXG1xc5YiHLFAuZiGyFW8jyxEKWKQ8WMhHZiBdfX2SJhSxTHHRBRLbCLWR5YiHLlKtGBY2KB+8TkXW5apRw4XHIssRnRcZquLtIHYGIHAz3vskXC1nGQmq4SR2BiBxMsA9fV+SKhSxjYTXcpY5ARA4mzJevK3LFQpaxUG4hE5GV1fbl64pcsZBljO9kicjauOdNvljIMsYtZCKyNr7Rly8WsozxnSwRWRtfV+SLhSxjwT5uUPJQZCKyEpVSgWAfV6ljUClYyDLmolYiwIt/PERkHYFerlCr+LIvV3xmZI67l4jIWsI4wlrWWMgyVz/QU+oIROQg6tb0kDoClYGFLHNNQrykjkBEDqJpiLfUEagMLGSZa8I/ICKykqahfD2RMxayzEUHePLMLET0wFzUSkQH8CMwOeMrvcxpVEo04OfIRPSAGgZ6QsMR1rLGZ8cOcLc1ET0o7q6WPxayHeBADCJ6UHwdkT8Wsh1oEsw/JCJ6MNzTJn8sZDtQP9ATLvzsh4iqSKtWoj4HdMkeX+XtgItaicY8HpmIqqhRsBenzLQDfIbsRKcof6kjEJGd6ljXT+oIVAEsZDvBQiaiqupcr6bUEagCWMh2olV4DehcVFLHICI7o3NRoXV4DaljUAWwkO2ERqVEu0judiKiymkf6ccJQewEnyU70rked1sTUeXwdcN+sJDtCP+wiKiyOvHzY7vBQrYjUbU8EejlKnUMIrITIT5uiKrFcyDbCxaynenErWQiqiAenWFfWMh2pks0dz8RUcV0jmYh2xMWsp3p3rAW3DQ8/ImIyuaqUeLh+rWkjkGVwEK2M+4uanRrwD8yIipb9wYB0GnVUsegSmAh26F+zYOljkBEMsfXCfvDQrZDDzeoCU++8yWiUni6qvFwA443sTcsZDukVavQs3GA1DGISKZ6NQ6EVs2xJvaGhWyn+nN3FBGVgq8P9omFbKc6RfnDV+cidQwikhl/DxfE8vhju8RCtlNqlRK9mwRKHYOIZObRpkFQKRVSx6AqYCHbsQHcLUVEf9O/BV8X7BUL2Y61i/RD3Zo6qWMQkUxEB3igdbiv1DGoiljIdm5I+3CpIxCRTAztECF1BHoALGQ791TrUE6lSUTwclXjyVYhUsegB8BCtnNerhoe4kBEGNgmDO4unDDInrGQHcDQjtxtTeTMlApgGHdX2z0WsgNoHOyNdnU4kIPIWT1cvxZq+7lLHYMeEAvZQbzYqY7UEYhIIsM6RkgdgayAhewgejYMQDjfIRM5nciaOnSux5m5HAEL2UEolQq8GMutZCJnM7xjBBQKzszlCFjIDmRQ2zAEerlKHYOIqkmAlxbPtAmTOgZZCQvZgbhqVHjt4bpSxyCiavL6w1Fw5TwEDoOF7GAGt62NYG9uJRM5uhAfNwxuW1vqGGRFLGQH46JW4vVuUVLHICIbe/3hKLio+RLuSPhsOqBn2oQhzNdN6hhEZCNhvm4Y2CZU6hhkZSxkB6RRKfHmw/WkjkFENvJWt3rQqPjy7Wj4jDqoJ1uFIILHJRM5nEh/HZ5sxa1jR8RCdlBqlRJvduNWMpGjebtHPaiUPO7YEbGQHdgTLUPQKMhL6hhEZCUNg7zQrxnP7uaoWMgOTKlU4JPHG4OT+BA5hsn9GkHJrWOHxUJ2cK3DffFkS37eRGTvHm0WhHaRflLHIBtiITuB8X0bwNOVJy4nslduGhUm9G0odQyyMRayE/D30OK9ntFSxyCiKnqta10E+3BuAUfHQnYSQztEoEGgp9QxiKiSImvq8EoXzlHvDFjITkKlVOCTx5tIHYOIKmnagCacItNJ8Fl2Im0jfPFkyxCpYxBRBT3eIhgdo/yljkHVhIXsZMb3bYga7hqpYxBROXzcNZj4WCOpY1A1YiE7mZqeWnz6RFOpYxBROaY/3hT+HlqpY1A1YiE7oT5Ng7jrmkjGnmwZgkebBUkdg6qZQgghpA5B1e9unh69P9+Daxn3pI7i0K4ufBHGrNRil3u0fBR+j4yGMBTg9u//Ru7Z3RBGPdzqtILvI6Oh0tUodZ1CCGT+sQLZJ7bAlJ8DbUhD+D7yGjS+hW+yhEGP9M1fIPf8Aah0NeD7yGtwi2hhvn3mwbUwZqXBt+erVn+89OBCfNyw+Z3O8HTlR0vOhoXsxA5cSsdziw/AxP8BNmPMzQRMJvPPBbeSkbpqIgKe/RSutZshfctXuHfxCPwefQdKrQ63ty2EQqFE4JBZpa4z88B/kXlgDfwffRdq7wBk7FkOfVoSgl9aCIXaBVlHf0H28V/hP+BD3Lt0FFmH1iL0jeVQKBTQZ9xA6uqPETTscyi1PBuY3CgVwI+j2nNGLifFXdZOrH2kH0Z2qiN1DIemcveGyqOG+evehUNQ+wRBG9YUpvwcZP+5DTW6jYRbeHNoA6Pg3/cd5F87i/xr50pcnxACd49sgHeHQXCv1x4uterA/7H3YMi+jdyE/QAAffoVuEW1g0vNcHi2ehSm3EyY7mUBAG5vXYAaXYezjGVq1EORLGMnxkJ2cmN61eeEIdVEGPXIObMTHs16QqFQIP/GBcBksNidrPELg8qrJvKvl1zIhsybMObcsbiNUquDNri++TYuteog/+oZmPT5yEs8BpWHL5RuXsg+vQMKtQvcozva8mFSFTUK8sL7PetLHYMkxEJ2clq1CvMGtYCLiv8VbC034QBMednQNekOADDl3AFUaihdPSyWU+l8YMy5U+I6jNmFlyt1Ppa3cfeBMScDAODRtCc0terg+r9fQ+b+1fAf8AFMednI/GMFfHu8gju7v8e1r0fh5qqPYLh7y7oPkqpEq1bi88EtOAGIk+OzT2gY5IV/9G0gdQyHl/3nVrhFtoba07a7JBUqNfweGY3QV/+NoGHz4BraGHd+/zc8W/dDwc1LuHd+P4JGzIc2uAHu/PaNTbNQxYzv0wDRAdxT5exYyAQAGB5bB0+14mkabcWQmYq85BPwaN7LfJlSVwMwGmDKy7ZY1piTUeooa5VH4eWmv7aGzbfJzYDqb1vNRfKS/4Q+PRmerR5D3uU/4RbZBkoXV7g36IS8yyer/qDIKp5sFYLhsRzLQSxkus/0J5qgeai31DEcUvbJbVC5e8OtblvzZdrAKECpxr3kE+bL9OlXYcxKgza45D0Wau8AqHQ1kJccZ77MlJ+L/OvxJd5GGApwe9tC+PV6AwqlChAmCJPxrxsaIYSp2G2o+jQP9eZEPWTGQiYzV40Ki15ozdmBrEwIE7JP/gZdk+6FpfgXpVYHj2Y9cef3b5GX/Cfyb1xA+q+fQxvcANqQ/5XrtcWvIjdhHwBAoVDAs80AZO5bhdzzB1GQloRb/zcXag9fuEd3KHbfGftWwi2yDVwCCs8WpA1phNyEfShITcTdYxvhGsJz7ErF30OLRS+0hqtGVf7C5BR41nqyEOTthgXPt8Lz3x6A3sgDlK0hLykOxqw0eDTrWew63+6jcFuhRNr6TyGMerjWaQW/nq9ZLGO4fRWm/Fzzz17tnoLQ5yF9y3yY8nLgGtoItZ6ZCoXaxeJ2BWlJyD23B0HD55svc28Qi7wrJ3FjxQfQ+IXAv99YKz9aqgiNSoGFQ1ohyJvnOKb/4cQgVKLv9ifh4w2npY5B5JCmPd4EQ9qHSx2DZIa7rKlEQztE4Jk2HORFZG3PxtRmGVOJWMhUqk8eb4KWtX2kjkHkMNqE18CU/o2ljkEyxUKmUmnVKiwZ1hZRtTzKX5iIylS3pg7fDG3DyT+oVPyfQWWqoXPB9yNjEOLDwSdEVRXi44blL7WDr86l/IXJabGQqVxB3m74bmQM/PhiQlRpfn+9qeWIaioPC5kqpG5NDywbEQMPLY+UI6ooT60a/3kxBpE1+bEPlY+FTBXWNNQb3wxtzc/AiCpAq1bi22Ft0CSEs99RxfCVlSqlY11/zH+2JVRKhdRRiGRLrVTgq+da8dzGVCksZKq0Xo0DMePJplCwk4mKUSiAfz7dDD0aBUgdhewMC5mq5Jk2YZj9dHNuKRPdR6VU4LOnmuFJnjmNqoBTZ9ID2XQyBW+vjEOBkWcNIufmolLi88Et0LdpkNRRyE6xkOmB7YxPxavLjyJPz1Im5+T215nSukTXlDoK2TEWMlnFocTbGLnsMO7mG6SOQlStPLVqLBnRFm0jfKWOQnaOhUxW8+fVDAxbcgh3cvVSRyGqFr46F3z3YgwPbSKrYCGTVSXcvIsh3x5E6t18qaMQ2VSglyuWvxSDqFqeUkchB8FCJqtLTs/BiGWHcSktR+ooRDYR6a/Df16MQZivu9RRyIGwkMkmsvL0ePOH49iVkCZ1FCKreii6JuY/2xLebhqpo5CDYSGTzRhNAjN+PYtv/0iUOgqRVbzUqQ7G923I4+/JJljIZHP/PXoV//jpJAoMPCyK7JOLWolPn2iKp1tzwg+yHRYyVYujyXfw6vKjSONgL7IztTy1WPRCa7SqXUPqKOTgWMhUbVIy72HUd0dw6lqW1FGIKqRZqDe+eaENAr1dpY5CToCFTNUqT2/Eh2v/xPq461JHISrTEy1DMOPJpnDVqKSOQk6ChUySWHPkCib9fBq5BUapoxBZ8NSqMfXxxniiJT8vpurFQibJXEzLxps/HMeZFO7CJnloE14D8wa14PHFJAkWMkkq32DEPzfHY8neRPB/IklFrVTgzW718Ea3KB7SRJJhIZMs7LtwC2PWnMD1zDypo5CTCfdzx7xBLTiKmiTHQibZyLynx8cbTmEDB3xRNXm6dSim9G8MnVYtdRQiFjLJz5bTNzDl59PcWiabCfZ2xaT+jdGrcaDUUYjMWMgkS7kFBvzrt/P49x+JMJj4X5SsQ61U4MVOdfBOj3pwd+FWMckLC5lk7dyNLEz86RSOJN+ROgrZubYRNTDt8aaoH8jTJZI8sZBJ9oQQWH3kCmZuOoc7uXqp45Cd8dW54MM+DTCwdSgUCo6gJvliIZPduJNTgBmbzmLN0as8RIrKpVAAg9uG4YPeDeDj7iJ1HKJysZDJ7hy/fAf/3ByP/ZfSpY5CMhUT4YvxfRugJQ9lIjvCQia7tffCLczaEo+4KxlSRyGZaBDoiXG966NbgwCpoxBVGguZ7N7W0zcwZ2sC4m/elToKSaS2rzve6VEPj7cIgZIzbZGdYiGTQzCZBH4+cR3zfktAcnqu1HGomoTWcMOb3aLwVKtQqFVKqeMQPRAWMjkUg9GE1UeuYtGui7h8m8XsqEJruOG1rlEY2CYUGhYxOQgWMjkkk0lg29mbWPJHIg4m3pY6DllJTB1fvBhbBz0bBfAkEORwWMjk8E5fz8SSP5Lwy4nrKDCapI5DleSiVqJfs2C82CkCjYO9pY5DZDMsZHIaaXfz8f2BZPxwMBm3sgukjkPl8PfQYkj72hjSPhz+Hlqp4xDZHAuZnE6+wYif465jzdGrOJx0m5OMyIhCAbQJr4HBbWujX/NguKj5+TA5DxYyObVrGfew/vg1rD9+DedTs6WO47Tq1fLA4y1D0L95MMJ83aWOQyQJFjLRX05dy8T649fw84nrSL2bL3Uchxfo5Yr+LYIxoEUwPxsmAguZqBiTSWDfxXSsj7uG38+l4nYOP2+2Fh93DXo1CsSAlsFoX8ePk3gQ3YeFTFQGk0ng5LVM7IxPw86EVJy4kgGenrnilAqgeZgPukTXRJfommge6sMSJioFC5moEjJyC7D7/C3sik/DroQ03Mrmru2/q+WpxUN/FXDnev480xJRBbGQiapICIHT17NwMPE2TlzJwImrGU45bWdtX3c0C/VGizAfxEb5o2GQl9SRiOwSC5nIijJyCxB3JQMnrmTixNUM/Hk1w6GOea7pqUXzUG80C/VB8zAfNAvxRg0dt4CJrIGFTGRjV27n4vT1TCTeykVyeg6S0wv/TcnKk+Ux0AoFEOTlitp+7gj31SHc3x2R/h5oHuaNIG83qeMROSwWMpFE8vRGXL2Ti6RbuUhKz8Hl27m4kZmH2zkFuJ1TgPScAmTl6a1a2q4aJbzdNPBxc4G3mwbe7hqE+Lihtq87wv0Kv8J83aFVq6x3p0RUISxkIhkzGE24m2fA3TwDsvL0uJtnQHa+AcYSh3pbXqZWKuHtroGPm8ZcvixaIvliIRMREckAJ4olIiKSARYyERGRDLCQiYiIZICFTEREJAMsZCIiIhlgIRMREckAC5mKUSgUWL9+vdQxLNyfKSkpCQqFAnFxcZJm+ruIiAh8/vnnUscgIjvFQpahGzdu4O2330ZUVBRcXV0REBCA2NhYLFy4ELm58jh5wY0bN/Dmm28iMjISWq0WYWFh6NevH7Zv327z+w4LC0NKSgqaNGkCANi5cycUCgUyMjLKvF3RckVfNWvWRN++fXHy5MlK3f+yZcvg4+NT7PLDhw/j5ZdfrtS6iIiKqKUOQJYuXbqE2NhY+Pj44NNPP0XTpk2h1Wpx8uRJfPPNNwgJCUH//v0lzZiUlGTOOGvWLDRt2hR6vR5btmzB66+/jnPnzpV4O71eD41G88D3r1KpEBgYWOXbx8fHw8vLC9evX8fYsWPx6KOP4sKFC3BxebCTJNSsWfOBbk9ETk6QrPTq1UuEhoaK7OzsEq83mUzm75OTk0X//v2FTqcTnp6eYuDAgeLGjRsWyy9YsEBERkYKjUYjoqOjxXfffWdxfUJCgujcubPQarWiYcOGYuvWrQKA+Omnn0rN2KdPHxESElJixjt37pi/ByAWLFgg+vXrJ9zd3cWkSZOEEEKsX79etGzZUmi1WlGnTh0xefJkodfrK5wpMTFRABDHjx83f3//17Bhw0rMvWPHDgHAIuPPP/8sAIgTJ06YL5szZ45o0qSJcHd3F6GhoWL06NHi7t27Fuu4/6vocYWHh4t58+ZZPP7FixeLxx9/XLi5uYmoqCixYcMGi0wbNmwQUVFRQqvViq5du4ply5YVy0hEzoGFLCO3bt0SCoVCzJgxo9xljUajaNGihejUqZM4cuSIOHDggGjdurXo0qWLeZl169YJjUYjvvrqKxEfHy/mzJkjVCqV+P33383raNKkiejevbuIi4sTu3btEi1btiyzkNPT04VCoRCffvppuRkBiFq1aoklS5aIixcviuTkZLF7927h5eUlli1bJi5evCi2bt0qIiIixOTJkyuc6f5CNhgMYu3atQKAiI+PFykpKSIjI6PEPH8v5IyMDPHcc88JAOLs2bPm5ebNmyd+//13kZiYKLZv3y7q168vRo8eLYQQIj8/X3z++efCy8tLpKSkiJSUFHNZl1TIoaGh4ocffhDnz58Xb731lvDw8BDp6elCCCEuXbokNBqNGDNmjDh37pz48ccfRUhICAuZyEmxkGXkwIEDAoBYt26dxeV+fn5Cp9MJnU4nxo0bJ4QQYuvWrUKlUonLly+blzt9+rQAIA4dOiSEEKJjx45i1KhRFusaOHCg6Nu3rxBCiC1btgi1Wi2uXbtmvn7Tpk1lFvLBgwdLzFgSAOKdd96xuKx79+7Fyvz7778XQUFBFc50fyELUfKWb0mKliv6XRZt4fbv37/M261Zs0b4+fmZf166dKnw9vYutlxJhTxx4kTzz9nZ2QKA2LRpkxBCiA8++EA0adLEYh0TJkxgIRM5KQ7qsgOHDh1CXFwcGjdujPz8fADA2bNnERYWhrCwMPNyjRo1go+PD86ePWteJjY21mJdsbGxFteHhYUhODjYfH2HDh3KzCIqeS6SNm3aWPx84sQJTJ06FR4eHuavUaNGISUlBbm5uVXKVFl79uzB0aNHsWzZMkRHR2PRokUW1//222/o3r07QkJC4OnpiRdeeAHp6elVGlDXrFkz8/c6nQ5eXl5ITU0FUPhZdtu2bS2Wj4mJqcIjIiJHwEFdMhIVFQWFQoH4+HiLyyMjIwEAbm7Snxy+Xr16UCgUpQ7c+judTmfxc3Z2NqZMmYInn3yy2LKurq5WyVieOnXqwMfHB/Xr10dqaioGDRqE3bt3AygcsPbYY49h9OjRmD59Onx9ffHHH39g5MiRKCgogLu7e6Xu6++D2BQKBUwmk9UeCxE5Dm4hy4ifnx969uyJL7/8Ejk5OWUu27BhQ1y5cgVXrlwxX3bmzBlkZGSgUaNG5mX27t1rcbu9e/daXH/lyhWkpKSYrz9w4ECZ9+vr64tevXrhq6++KjFjeYcetWrVCvHx8YiKiir2pVQqq5SpaHS00Wgsc7mSvP766zh16hR++uknAMDRo0dhMpkwZ84ctG/fHtHR0bh+/Xqx+6vKff1d/fr1ceTIEYvLDh8+/MDrJSL7xEKWmQULFsBgMKBNmzZYtWoVzp49i/j4eCxfvhznzp2DSlV4gvkePXqgadOmeP7553Hs2DEcOnQIQ4cORZcuXcy7iceOHYtly5Zh4cKFOH/+PObOnYt169ZhzJgx5nVER0dj2LBhOHHiBPbs2YMJEyaUm/Grr76C0WhETEwM1q5di/Pnz+Ps2bP44osvyt29/PHHH+O7777DlClTcPr0aZw9exYrV67ExIkTq5wpPDwcCoUCGzduRFpaGrKzs8t9DEXc3d0xatQoTJo0CUIIREVFQa/XY/78+bh06RK+//77Yru0IyIikJ2dje3bt+PWrVtVPjb8lVdewblz5/DBBx8gISEBq1evxrJlywAUbkkTkZOR+kNsKu769evijTfeEHXq1BEajUZ4eHiImJgYMWvWLJGTk2NezhqHPcXHx4tOnToJFxcXER0dLTZv3lzuYU9FGV9//XURHh4uXFxcREhIiOjfv7/YsWOHeZnS1rN582bRsWNH4ebmJry8vERMTIz45ptvKpzp74O6hBBi6tSpIjAwUCgUikod9iSEEJcvXxZqtVqsWrVKCCHE3LlzRVBQkHBzcxO9evUS3333XbHbvfrqq8LPz6/cw57+/vi9vb3F0qVLzT///bCnhQsXCgDi3r17JT4GInJcCiEqOUqHiGxm+vTpWLRokcVHEUTkHDioi0hCCxYsQNu2beHn54e9e/di1qxZeOONN6SORUQSYCETSej8+fOYNm0abt++jdq1a+P999/H+PHjpY5FRBLgLmsiIiIZ4ChrIiIiGWAhExERyQALmYiISAZYyERERDLAQiYiIpIBFjIREZEMsJCJiIhkgIVMREQkA/8PKBlWjnCberIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cr1 = (df[df.columns[0]] == 1.0).sum()\n",
    "cr0 = (df[df.columns[0]] == 0.0).sum()\n",
    "\n",
    "sizes = [cr1, cr0]\n",
    "labels = ['Good Credit Rating', 'Bad Credit Rating']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 10})\n",
    "plt.title('Credit Rating', fontsize = 20)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the dataset is imbalanced, having a much lower proportion of the class 0. So, we will have to use SMOTE(Synthetic Minority Oversampling Technique) to ensure good performance of the Binarized Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df,train_size=0.3,random_state=42)\n",
    "df_train, df_val = train_test_split(df_train,train_size=0.534,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_rating\n",
       "1    124\n",
       "0     36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['credit_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[df_train.columns[1:]]\n",
    "y_train = df_train[df_train.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using SMOTE to oversample the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 124, 0: 124})\n"
     ]
    }
   ],
   "source": [
    "k_neighbors = min(4, y_train.value_counts().min()-1)\n",
    "\n",
    "# Initialize SMOTE with the adjusted k_neighbors\n",
    "sm = SMOTE(k_neighbors=k_neighbors, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the dataset\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "# Check the resampled dataset shape\n",
    "print('Resampled dataset shape %s' % Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "df_train = pd.concat([x_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discarding columns having a single value throughout the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns having a single value throughout the training dataset would be redundant and should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column: credit_purpose_domestic_appliances\n",
      "Dropping column: credit_purpose_retraining\n"
     ]
    }
   ],
   "source": [
    "for col in x_train.columns.tolist():\n",
    "    if df_train[col].nunique() == 1:\n",
    "        print(\"Dropping column: {}\".format(col))\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        df_train.drop(col, axis=1, inplace=True)\n",
    "        df_test.drop(col, axis=1, inplace=True)\n",
    "        df_val.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping one column each for binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary features, a single column is sufficient to indicate the value of the feature. So for all binary features, we retain only one column each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['telephone_no','foreign_worker_no'])\n",
    "df_test = df_test.drop(columns=['telephone_no','foreign_worker_no'])\n",
    "df_val.drop(columns=['telephone_no','foreign_worker_no'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discarding one out of each pair of highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Feature 1, Feature 2, Correlation]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = df_train[df_train.columns[1::]].corr()\n",
    "\n",
    "# Step 2: Create a mask to get the upper # data (as pandas dataframes)triangle of the correlation matrix, excluding the diagonal\n",
    "upper_triangle_mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "upper_triangle_matrix = correlation_matrix.where(upper_triangle_mask)\n",
    "\n",
    "# Step 3: Extract pairs of features with the absolute correlation >= 0.9\n",
    "correlated_pairs = upper_triangle_matrix.stack().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "correlated_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "correlated_pairs = correlated_pairs[correlated_pairs['Correlation'].abs() >= 0.8]\n",
    "\n",
    "print(correlated_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features dropped: set()\n",
      "Shape of the new dataframe: (248, 58)\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = set()\n",
    "\n",
    "# Iterate over the correlated pairs\n",
    "for feature_1, feature_2, _ in correlated_pairs.values:\n",
    "    # If neither feature is already in the drop list, add one of them (say, feature_2)\n",
    "    if feature_1 not in features_to_drop and feature_2 not in features_to_drop:\n",
    "        features_to_drop.add(feature_2)\n",
    "\n",
    "# Step 5: Drop the identified features from the dataframe\n",
    "df_train.drop(columns=features_to_drop,inplace=True)\n",
    "df_test.drop(columns=features_to_drop,inplace=True)\n",
    "df_val.drop(columns=features_to_drop,inplace=True)\n",
    "\n",
    "print(\"Features dropped:\", features_to_drop)\n",
    "print(\"Shape of the new dataframe:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[df_train.columns[:-1]]\n",
    "x_test = df_test[df_test.columns[1:]]\n",
    "y_train = df_train[df_train.columns[-1]]\n",
    "y_test = df_test[df_test.columns[0]]\n",
    "x_val = df_val[df_val.columns[1:]]\n",
    "y_val = df_val[df_val.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keras_BNN:\n",
    "    def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.best_model = None\n",
    "        self.best_hp = None\n",
    "        self.weights_keras = []\n",
    "        self.bin_weights_keras = []\n",
    "        self.activations = []\n",
    "        self.correct_prediction_list = []\n",
    "\n",
    "    def build_model(self, hp):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(lq.layers.QuantDense(units=hp.Int('units_input', min_value=2, max_value=10, step=1),\n",
    "                                       input_shape=(self.x_train.shape[1],),\n",
    "                                       kernel_quantizer=\"ste_sign\",\n",
    "                                       kernel_constraint=\"weight_clip\",\n",
    "                                       use_bias=False))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "        num_layers = hp.Int('num_layers', min_value=0, max_value=0)\n",
    "        for i in range(num_layers):\n",
    "            model.add(lq.layers.QuantDense(units=hp.Int(f\"units_{i+1}\", min_value=2, max_value=10, step=1),\n",
    "                                           kernel_quantizer=\"ste_sign\",\n",
    "                                           kernel_constraint=\"weight_clip\",\n",
    "                                           input_quantizer=\"ste_sign\",\n",
    "                                           use_bias=False))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(lq.layers.QuantDense(1, activation='sigmoid',\n",
    "                                       kernel_quantizer=\"ste_sign\",\n",
    "                                       kernel_constraint=\"weight_clip\",\n",
    "                                       input_quantizer=\"ste_sign\",\n",
    "                                       use_bias=False))\n",
    "\n",
    "        model.compile(optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling=\"log\")),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def tune_model(self, max_trials=10, executions_per_trial=3):\n",
    "        tuner = RandomSearch(\n",
    "            self.build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=max_trials,\n",
    "            executions_per_trial=executions_per_trial,\n",
    "            directory='my_dir',\n",
    "            project_name='my_project2',\n",
    "            overwrite=True\n",
    "        )\n",
    "\n",
    "        tuner.search(self.x_train, self.y_train, epochs=20, validation_data=(self.x_val, self.y_val), callbacks=[self.early_stopping])\n",
    "        self.best_model = tuner.get_best_models(num_models=1)[0]\n",
    "        self.best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "        return self.best_model, self.best_hp\n",
    "\n",
    "    def extract_accuracies(self):\n",
    "        train_loss, train_acc = self.best_model.evaluate(self.x_train, self.y_train)\n",
    "        test_loss, test_acc = self.best_model.evaluate(self.x_test, self.y_test)\n",
    "        return train_loss, train_acc, test_loss, test_acc\n",
    "\n",
    "    def extract_weights(self):\n",
    "        best_hp_vals = self.best_hp.values\n",
    "        for i in range(1, 2):\n",
    "            if i > best_hp_vals['num_layers'] and (f'units_{i}' in best_hp_vals.keys()):\n",
    "                del best_hp_vals[f'units_{i}']\n",
    "\n",
    "        for i in range(0, 2 * (best_hp_vals['num_layers'] + 1) + 1, 2):\n",
    "            self.weights_keras.append(self.best_model.layers[i].get_weights()[0].T.tolist())\n",
    "\n",
    "        for i in range(0, 2 * (best_hp_vals['num_layers'] + 1) + 1, 2):\n",
    "            with lq.context.quantized_scope(True):\n",
    "                self.bin_weights_keras.append(self.best_model.layers[i].get_weights()[0].tolist())\n",
    "\n",
    "        return self.weights_keras, self.bin_weights_keras\n",
    "\n",
    "    def extract_activations(self):\n",
    "        layer_names = [layer.name for layer in self.best_model.layers]\n",
    "        activation_model = tf.keras.models.Model(\n",
    "            inputs=self.best_model.input,\n",
    "            outputs=[self.best_model.get_layer(name).output for name in layer_names]\n",
    "        )\n",
    "        self.activations = activation_model.predict(self.x_train)\n",
    "        \n",
    "        bin_activations = []\n",
    "        for activation in self.activations:\n",
    "            bin_activation = [[math.copysign(1, elem) for elem in a] for a in activation]\n",
    "            bin_activations.append(bin_activation)\n",
    "        \n",
    "        return self.activations, bin_activations\n",
    "\n",
    "    def extract_correct_prediction_list(self):\n",
    "        predictions = self.best_model.predict(self.x_train)\n",
    "        predicted_labels = np.round(predictions).astype(int)\n",
    "        correct_prediction = (predicted_labels == self.y_train.to_numpy().reshape(-1, 1)).astype(int)\n",
    "        self.correct_prediction_list = correct_prediction.flatten().tolist()\n",
    "\n",
    "        return self.correct_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.7357142567634583\n",
      "\n",
      "Best val_accuracy So Far: 0.7404761711756388\n",
      "Total elapsed time: 00h 00m 35s\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7914 - accuracy: 0.6169\n",
      "22/22 [==============================] - 0s 772us/step - loss: 0.7161 - accuracy: 0.5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7913529276847839, Train Accuracy: 0.6169354915618896\n",
      "Test Loss: 0.7161127328872681, Test Accuracy: 0.5600000023841858\n",
      "Best hyperparameter values: {'units_input': 10, 'num_layers': 0, 'learning_rate': 0.07654059321735311}\n",
      "[[[-1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0]], [[1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [-1.0]]]\n",
      "8/8 [==============================] - 0s 799us/step\n",
      "[[[-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0]], [[-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]]\n",
      "8/8 [==============================] - 0s 948us/step\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "trainer = Keras_BNN(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "best_model, best_hp = trainer.tune_model()\n",
    "\n",
    "best_model.save('best_model.h5')\n",
    "\n",
    "train_loss, train_acc, test_loss, test_acc = trainer.extract_accuracies()\n",
    "print(f\"Train Loss: {train_loss}, Train Accuracy: {train_acc}\")\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "print(\"Best hyperparameter values:\",best_hp.values)\n",
    "\n",
    "weights_keras, bin_weights_keras = trainer.extract_weights()\n",
    "print(bin_weights_keras)\n",
    "\n",
    "activations, bin_activations = trainer.extract_activations()\n",
    "print(bin_activations)\n",
    "\n",
    "correct_prediction_list = trainer.extract_correct_prediction_list()\n",
    "print(correct_prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that in the above case, the training accuracy is just 50%, which shows that the model is merely guessing and not learning any patterns in the data (it is assigning a single class label of 1 to all the training examples), and since the dataset is imbalanced with a much larger proportion of examples with label 1, it achieves a good test accuracy of about 69%. One reason for this could be the widely different ranges of the different features, which haven't been normalized. Hence, we try to normalize the features and then observe the behavior of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.6428571343421936\n",
      "\n",
      "Best val_accuracy So Far: 0.8142857154210409\n",
      "Total elapsed time: 00h 00m 36s\n",
      "8/8 [==============================] - 0s 968us/step - loss: 0.2897 - accuracy: 0.8952\n",
      "22/22 [==============================] - 0s 665us/step - loss: 0.8934 - accuracy: 0.6986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.28969618678092957, Train Accuracy: 0.8951612710952759\n",
      "Test Loss: 0.8934086561203003, Test Accuracy: 0.6985714435577393\n",
      "Best hyperparameter values: {'units_input': 5, 'num_layers': 0, 'learning_rate': 0.024829303642439165}\n",
      "[[[-1.0, -1.0, 1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0]]]\n",
      "8/8 [==============================] - 0s 852us/step\n",
      "[[[1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, -1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0]], [[1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, 1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, -1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, -1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [-1.0, 1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, 1.0, 1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0], [-1.0, -1.0, -1.0, -1.0, 1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]]\n",
      "8/8 [==============================] - 0s 642us/step\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "trainer = Keras_BNN(x_train_scaled, y_train, x_val_scaled, y_val, x_test_scaled, y_test)\n",
    "best_model, best_hp = trainer.tune_model()\n",
    "\n",
    "best_model.save('best_model.h5')\n",
    "\n",
    "train_loss, train_acc, test_loss, test_acc = trainer.extract_accuracies()\n",
    "print(f\"Train Loss: {train_loss}, Train Accuracy: {train_acc}\")\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "print(\"Best hyperparameter values:\",best_hp.values)\n",
    "\n",
    "weights_keras, bin_weights_keras = trainer.extract_weights()\n",
    "print(bin_weights_keras)\n",
    "\n",
    "activations, bin_activations = trainer.extract_activations()\n",
    "print(bin_activations)\n",
    "\n",
    "correct_prediction_list = trainer.extract_correct_prediction_list()\n",
    "print(correct_prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BNN Using CPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of constraints is as follows:\n",
    "\n",
    "$$ \n",
    "\n",
    "n_{0j}^{k} = x_{j}^{k} \\; \\forall j \\in N_{0},\\, k \\in \\mathcal{T}\\\\\n",
    "n_{\\ell j}^{k} = 2(w_{\\ell j}.n_{\\ell -1}^{k} \\geq 0) - 1 \\; \\forall \\ell \\in \\{1,\\ldots,L\\}, \\, j \\in N_{\\ell},\\, k \\in \\mathcal{T}\\\\\n",
    "\\rho |\\mathcal{T}| \\leq \\sum_{k=1}^{|\\mathcal{T}|}c_{k}, \\; 0<\\rho<1\\\\\n",
    "\n",
    "$$\n",
    "\n",
    "where, $\\rho$ represents approximately the fraction of training examples correctly classified by the Keras model, and $c_k$ is a boolean variable with value 1 indicating correct classification of training example $k$ and value 0 indicating incorrect classification of example $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docplex.cp.model import *\n",
    "context.solver.agent = 'local'\n",
    "#context.solver.local.execfile = 'Path to the binary cpoptimizer'\n",
    "context.solver.local.execfile = '/net/phorcys/data/roc/Logiciels/CPLEX_Studio201/cpoptimizer/bin/x86-64_linux/cpoptimizer'\n",
    "#context.params.set_attribute('Presolve', 'Off')\n",
    "context.params.set_attribute('Workers', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.replace(0,-1)\n",
    "y_test = y_test.replace(0,-1)\n",
    "y_val = y_val.replace(0,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Margin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function for this model is the sum of margins of all neurons in the BNN. Margin of a neurons is defined as the minimum of the absolute values of the pre-activations (unbinarized activation) of a neuron for all training examples, i.e\n",
    "\n",
    "$$\\max \\sum_{\\ell \\in \\{1,\\ldots,L\\}} \\sum_{j \\in N_{\\ell}} \\min_{k \\in \\{1,\\ldots,|\\mathcal{T}|\\}}|w_{\\ell j}.n_{\\ell -1}(x^k)|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #index of credit_amount column\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value of dataframe (present in credit_amount column)\n",
    "\n",
    "max_margin_mdl = CpoModel(name='German Credit CP Model MM')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_margin_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_margin_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#first (hidden) layer activations\n",
    "activations_1 = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#activations and weights for hidden layers \n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights to output layer\n",
    "activations_L = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [max_margin_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "                #correct_prediction_condition = max_margin_mdl.logical_and([activations_l[k][j] == y_train.iloc[k][j] for j in range(N[l])]) \n",
    "                #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "max_margin_mdl.add(max_margin_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for minimum training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #list containing all variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_margin_obj = max_margin_mdl.integer_var(0, 1000, name='max_margin_objective')\n",
    "margins_neurons = []\n",
    "sum_margins = 0\n",
    "for l in range(1,L+1):\n",
    "    margins_l = [] #list to store margins of all neurons of layer l\n",
    "    sum_margins_l = 0 #sum of margins of neurons of layer l\n",
    "    for j in range(N[l]):\n",
    "        activations_j = [] #list to store activations of neurons j in layer l\n",
    "        margin_j = 0 #margin of neuron j\n",
    "        for k in range(x_train.shape[0]):\n",
    "            elem = max_margin_mdl.scal_prod(weights[l-1][j], activations[l-1][k])\n",
    "            activations_j.append(max_margin_mdl.abs(elem))\n",
    "        margin_j = max_margin_mdl.min(activations_j) #definition of margin of a neuron\n",
    "        margins_l.append(margin_j)\n",
    "    sum_margins_l += max_margin_mdl.sum(margins_l)\n",
    "    sum_margins += sum_margins_l\n",
    "\n",
    "max_margin_mdl.add(max_margin_obj == sum_margins)\n",
    "max_margin_mdl.add(max_margin_mdl.maximize(max_margin_obj))\n",
    "\n",
    "#breaking the symmetry for weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_margin_mdl.add(max_margin_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_margin_solutions = max_margin_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute activations for dataset x\n",
    "def compute_activations(weights_solution, x):\n",
    "    activations = [x.values]  # Use the test set features as the initial activations\n",
    "\n",
    "    for layer_idx in range(1, len(N)):\n",
    "        prev_layer_activations = activations[-1]\n",
    "        current_layer_weights = weights_solution[layer_idx - 1]\n",
    "        \n",
    "        current_layer_activations = []\n",
    "        for sample_activations in prev_layer_activations:\n",
    "            layer_activations = []\n",
    "            for neuron_weights in current_layer_weights:\n",
    "                activation = sum(weight * sample_activation for weight, sample_activation in zip(neuron_weights, sample_activations))\n",
    "                # Apply sign function\n",
    "                if activation >= 0:\n",
    "                    layer_activations.append(1)\n",
    "                else:\n",
    "                    layer_activations.append(-1)\n",
    "            current_layer_activations.append(layer_activations)\n",
    "        \n",
    "        activations.append(current_layer_activations)\n",
    "    \n",
    "    return activations[-1]  # Return the activations of the output layer\n",
    "\n",
    "# Compare Predictions with True Labels and Compute Accuracy\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct_predictions = sum(pred == true for pred, true in zip(predictions, true_labels))\n",
    "    corr_class = predictions==true_labels\n",
    "    incorr_eg_idx = np.where(corr_class==False)[0].tolist() #indices of misclassified examples\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy, incorr_eg_idx\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 347 , No. of solutions found: 1 Is solution optimal: False , Optimality gap: (1.88184,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 349 , No. of solutions found: 2 Is solution optimal: False , Optimality gap: (1.86533,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 350 , No. of solutions found: 3 Is solution optimal: False , Optimality gap: (1.85714,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 352 , No. of solutions found: 4 Is solution optimal: False , Optimality gap: (1.84091,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 354 , No. of solutions found: 5 Is solution optimal: False , Optimality gap: (1.82486,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 358 , No. of solutions found: 6 Is solution optimal: False , Optimality gap: (1.7933,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 359 , No. of solutions found: 7 Is solution optimal: False , Optimality gap: (1.78552,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 363 , No. of solutions found: 8 Is solution optimal: False , Optimality gap: (1.75482,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 364 , No. of solutions found: 9 Is solution optimal: False , Optimality gap: (1.74725,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 411 , No. of solutions found: 10 Is solution optimal: False , Optimality gap: (1.43309,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 412 , No. of solutions found: 11 Is solution optimal: False , Optimality gap: (1.42718,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 414 , No. of solutions found: 12 Is solution optimal: False , Optimality gap: (1.41546,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 418 , No. of solutions found: 13 Is solution optimal: False , Optimality gap: (1.39234,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 422 , No. of solutions found: 14 Is solution optimal: False , Optimality gap: (1.36967,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 423 , No. of solutions found: 15 Is solution optimal: False , Optimality gap: (1.36407,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 424 , No. of solutions found: 16 Is solution optimal: False , Optimality gap: (1.35849,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 425 , No. of solutions found: 17 Is solution optimal: False , Optimality gap: (1.35294,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 447 , No. of solutions found: 18 Is solution optimal: False , Optimality gap: (1.23714,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 448 , No. of solutions found: 19 Is solution optimal: False , Optimality gap: (1.23214,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 449 , No. of solutions found: 20 Is solution optimal: False , Optimality gap: (1.22717,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 451 , No. of solutions found: 21 Is solution optimal: False , Optimality gap: (1.21729,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 452 , No. of solutions found: 22 Is solution optimal: False , Optimality gap: (1.21239,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 453 , No. of solutions found: 23 Is solution optimal: False , Optimality gap: (1.20751,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 814 , No. of solutions found: 24 Is solution optimal: False , Optimality gap: (0.228501,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 815 , No. of solutions found: 25 Is solution optimal: False , Optimality gap: (0.226994,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 816 , No. of solutions found: 26 Is solution optimal: False , Optimality gap: (0.22549,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 817 , No. of solutions found: 27 Is solution optimal: False , Optimality gap: (0.22399,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 818 , No. of solutions found: 28 Is solution optimal: False , Optimality gap: (0.222494,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 819 , No. of solutions found: 29 Is solution optimal: False , Optimality gap: (0.221001,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 820 , No. of solutions found: 30 Is solution optimal: False , Optimality gap: (0.219512,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 821 , No. of solutions found: 31 Is solution optimal: False , Optimality gap: (0.218027,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 823 , No. of solutions found: 32 Is solution optimal: False , Optimality gap: (0.215067,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 824 , No. of solutions found: 33 Is solution optimal: False , Optimality gap: (0.213592,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 828 , No. of solutions found: 34 Is solution optimal: False , Optimality gap: (0.207729,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 829 , No. of solutions found: 35 Is solution optimal: False , Optimality gap: (0.206273,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 830 , No. of solutions found: 36 Is solution optimal: False , Optimality gap: (0.204819,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 831 , No. of solutions found: 37 Is solution optimal: False , Optimality gap: (0.203369,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 833 , No. of solutions found: 38 Is solution optimal: False , Optimality gap: (0.20048,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 834 , No. of solutions found: 39 Is solution optimal: False , Optimality gap: (0.199041,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 835 , No. of solutions found: 40 Is solution optimal: False , Optimality gap: (0.197605,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 836 , No. of solutions found: 41 Is solution optimal: False , Optimality gap: (0.196172,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 837 , No. of solutions found: 42 Is solution optimal: False , Optimality gap: (0.194743,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 838 , No. of solutions found: 43 Is solution optimal: False , Optimality gap: (0.193317,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 839 , No. of solutions found: 44 Is solution optimal: False , Optimality gap: (0.191895,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 840 , No. of solutions found: 45 Is solution optimal: False , Optimality gap: (0.190476,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 841 , No. of solutions found: 46 Is solution optimal: False , Optimality gap: (0.189061,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 842 , No. of solutions found: 47 Is solution optimal: False , Optimality gap: (0.187648,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 843 , No. of solutions found: 48 Is solution optimal: False , Optimality gap: (0.18624,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 844 , No. of solutions found: 49 Is solution optimal: False , Optimality gap: (0.184834,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 845 , No. of solutions found: 50 Is solution optimal: False , Optimality gap: (0.183432,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 846 , No. of solutions found: 51 Is solution optimal: False , Optimality gap: (0.182033,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 847 , No. of solutions found: 52 Is solution optimal: False , Optimality gap: (0.180638,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 848 , No. of solutions found: 53 Is solution optimal: False , Optimality gap: (0.179245,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 872 , No. of solutions found: 54 Is solution optimal: False , Optimality gap: (0.146789,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 874 , No. of solutions found: 55 Is solution optimal: False , Optimality gap: (0.144165,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 875 , No. of solutions found: 56 Is solution optimal: False , Optimality gap: (0.142857,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 876 , No. of solutions found: 57 Is solution optimal: False , Optimality gap: (0.141553,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 878 , No. of solutions found: 58 Is solution optimal: False , Optimality gap: (0.138952,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 880 , No. of solutions found: 59 Is solution optimal: False , Optimality gap: (0.136364,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 881 , No. of solutions found: 60 Is solution optimal: False , Optimality gap: (0.135074,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 882 , No. of solutions found: 61 Is solution optimal: False , Optimality gap: (0.133787,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 990 , No. of solutions found: 62 Is solution optimal: False , Optimality gap: (0.010101,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 991 , No. of solutions found: 63 Is solution optimal: False , Optimality gap: (0.00908174,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 993 , No. of solutions found: 64 Is solution optimal: False , Optimality gap: (0.00704935,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 994 , No. of solutions found: 65 Is solution optimal: False , Optimality gap: (0.00603622,) , Test accuracy: 0.6657142857142857\n",
      "Objective value: 1000 , No. of solutions found: 66 Is solution optimal: False , Optimality gap: (0,) , Test accuracy: 0.6657142857142857\n",
      "No more solutions available.\n",
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = [] #list to store objective values of all solutions found\n",
    "runtimes = [] #list to store runtimes of all solutions found\n",
    "numsols = []\n",
    "test_acc = [] #list to store test accuracies of all solutions found\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_margin_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and \n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_margin_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_margin_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "max_margin_solution = max_margin_solutions.get_last_solution()\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=max_margin_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the objective values, test accuracies and runtimes to observe how all the intermediate solutions perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots of Performance of Max Margin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append the last point to extend up to runtime limit in case solver is unable to optimize a solution/prove optimality \n",
    "#extended_runtime = 1800\n",
    "#runtimes.append(extended_runtime)\n",
    "#objective_vals.append(objective_vals[-1])\n",
    "#test_acc.append(test_acc[-1])\n",
    "\n",
    "# Increase the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: Runtime vs Objective Values\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(runtimes, objective_vals, color='green', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Runtime (seconds)\")\n",
    "plt.ylabel(\"Objective Value (Sum of Margins)\")\n",
    "plt.title(\"Runtime vs Objective Value\")\n",
    "plt.xlim(0, 1800)\n",
    "\n",
    "# Subplot 2: Runtime vs Test Accuracies\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(runtimes, test_acc, color='blue', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Runtime (seconds)\")\n",
    "plt.ylabel(\"Test Accuracies\")\n",
    "plt.title(\"Runtime vs Test Accuracies\")\n",
    "plt.ylim(0.3, 1.0)\n",
    "plt.xlim(0, 1800)\n",
    "\n",
    "# Subplot 3: Objective Values vs Test Accuracies\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(objective_vals, test_acc, color='red', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Objective Values\")\n",
    "plt.ylabel(\"Test Accuracies\")\n",
    "plt.title(\"Objective Values vs Test Accuracies\")\n",
    "plt.ylim(0.3, 1.0)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that in this case, the solver takes some time to find the first feasible solution but is able to optimize is very fast once it is found, however, the test accuracies of all these solutions are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to give the objective a much larger upper bound and observe the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "#N=[x_train.shape[1],4,10]\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L))\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "max_margin_mdl = CpoModel(name='German Credit CP Model MM')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_margin_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_margin_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of activations of input layer\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#activations and incoming weights of first hidden layer\n",
    "activations_1 = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#activations and weights for further hidden layers \n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights of output layer\n",
    "activations_L = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [max_margin_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #max_margin_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "                #correct_prediction_condition = max_margin_mdl.logical_and([activations_l[k][j] == y_train.iloc[k][j] for j in range(N[l])]) \n",
    "                #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "max_margin_mdl.add(max_margin_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_margin_obj = max_margin_mdl.integer_var(0, 10000, name='max_margin_objective')\n",
    "margins_neurons = []\n",
    "sum_margins = 0\n",
    "for l in range(1,L+1):\n",
    "    margins_l = []\n",
    "    sum_margins_l = 0\n",
    "    for j in range(N[l]):\n",
    "        activations_j = []\n",
    "        margin_j = 0\n",
    "        for k in range(x_train.shape[0]):\n",
    "            elem = max_margin_mdl.scal_prod(weights[l-1][j], activations[l-1][k])\n",
    "            activations_j.append(max_margin_mdl.abs(elem))\n",
    "        margin_j = max_margin_mdl.min(activations_j)\n",
    "        margins_l.append(margin_j)\n",
    "    sum_margins_l += max_margin_mdl.sum(margins_l)\n",
    "    sum_margins += sum_margins_l\n",
    "\n",
    "max_margin_mdl.add(max_margin_obj == sum_margins)\n",
    "max_margin_mdl.add(max_margin_mdl.maximize(max_margin_obj))\n",
    "\n",
    "#breaking symmetry for weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_margin_mdl.add(max_margin_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_margin_solutions = max_margin_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 1010 , No. of solutions found: 1 Is solution optimal: False , Optimality gap: (1.07822,) , Test accuracy: 0.64\n",
      "Objective value: 1062 , No. of solutions found: 2 Is solution optimal: False , Optimality gap: (0.97646,) , Test accuracy: 0.64\n",
      "Objective value: 1063 , No. of solutions found: 3 Is solution optimal: False , Optimality gap: (0.9746,) , Test accuracy: 0.64\n",
      "Objective value: 1064 , No. of solutions found: 4 Is solution optimal: False , Optimality gap: (0.972744,) , Test accuracy: 0.64\n",
      "Objective value: 1065 , No. of solutions found: 5 Is solution optimal: False , Optimality gap: (0.970892,) , Test accuracy: 0.64\n",
      "Objective value: 1066 , No. of solutions found: 6 Is solution optimal: False , Optimality gap: (0.969043,) , Test accuracy: 0.64\n",
      "Objective value: 1067 , No. of solutions found: 7 Is solution optimal: False , Optimality gap: (0.967198,) , Test accuracy: 0.64\n",
      "Objective value: 1069 , No. of solutions found: 8 Is solution optimal: False , Optimality gap: (0.963517,) , Test accuracy: 0.64\n",
      "Objective value: 1070 , No. of solutions found: 9 Is solution optimal: False , Optimality gap: (0.961682,) , Test accuracy: 0.64\n",
      "Objective value: 1072 , No. of solutions found: 10 Is solution optimal: False , Optimality gap: (0.958022,) , Test accuracy: 0.64\n",
      "Objective value: 1073 , No. of solutions found: 11 Is solution optimal: False , Optimality gap: (0.956198,) , Test accuracy: 0.64\n",
      "Objective value: 1074 , No. of solutions found: 12 Is solution optimal: False , Optimality gap: (0.954376,) , Test accuracy: 0.64\n",
      "Objective value: 1075 , No. of solutions found: 13 Is solution optimal: False , Optimality gap: (0.952558,) , Test accuracy: 0.64\n",
      "Objective value: 1079 , No. of solutions found: 14 Is solution optimal: False , Optimality gap: (0.94532,) , Test accuracy: 0.64\n",
      "Objective value: 1080 , No. of solutions found: 15 Is solution optimal: False , Optimality gap: (0.943519,) , Test accuracy: 0.64\n",
      "Objective value: 1127 , No. of solutions found: 16 Is solution optimal: False , Optimality gap: (0.862467,) , Test accuracy: 0.64\n",
      "Objective value: 1131 , No. of solutions found: 17 Is solution optimal: False , Optimality gap: (0.85588,) , Test accuracy: 0.64\n",
      "Objective value: 1132 , No. of solutions found: 18 Is solution optimal: False , Optimality gap: (0.85424,) , Test accuracy: 0.64\n",
      "Objective value: 1184 , No. of solutions found: 19 Is solution optimal: False , Optimality gap: (0.772804,) , Test accuracy: 0.64\n",
      "Objective value: 1185 , No. of solutions found: 20 Is solution optimal: False , Optimality gap: (0.771308,) , Test accuracy: 0.64\n",
      "Objective value: 1186 , No. of solutions found: 21 Is solution optimal: False , Optimality gap: (0.769815,) , Test accuracy: 0.64\n",
      "Objective value: 1188 , No. of solutions found: 22 Is solution optimal: False , Optimality gap: (0.766835,) , Test accuracy: 0.64\n",
      "Objective value: 1189 , No. of solutions found: 23 Is solution optimal: False , Optimality gap: (0.765349,) , Test accuracy: 0.64\n",
      "Objective value: 1190 , No. of solutions found: 24 Is solution optimal: False , Optimality gap: (0.763866,) , Test accuracy: 0.64\n",
      "Objective value: 1191 , No. of solutions found: 25 Is solution optimal: False , Optimality gap: (0.762385,) , Test accuracy: 0.64\n",
      "Objective value: 1192 , No. of solutions found: 26 Is solution optimal: False , Optimality gap: (0.760906,) , Test accuracy: 0.64\n",
      "Objective value: 1193 , No. of solutions found: 27 Is solution optimal: False , Optimality gap: (0.75943,) , Test accuracy: 0.64\n",
      "Objective value: 1194 , No. of solutions found: 28 Is solution optimal: False , Optimality gap: (0.757956,) , Test accuracy: 0.64\n",
      "Objective value: 1196 , No. of solutions found: 29 Is solution optimal: False , Optimality gap: (0.755017,) , Test accuracy: 0.64\n",
      "Objective value: 1198 , No. of solutions found: 30 Is solution optimal: False , Optimality gap: (0.752087,) , Test accuracy: 0.64\n",
      "Objective value: 1199 , No. of solutions found: 31 Is solution optimal: False , Optimality gap: (0.750626,) , Test accuracy: 0.64\n",
      "Objective value: 1201 , No. of solutions found: 32 Is solution optimal: False , Optimality gap: (0.74771,) , Test accuracy: 0.64\n",
      "Objective value: 1204 , No. of solutions found: 33 Is solution optimal: False , Optimality gap: (0.743355,) , Test accuracy: 0.64\n",
      "Objective value: 1206 , No. of solutions found: 34 Is solution optimal: False , Optimality gap: (0.740464,) , Test accuracy: 0.64\n",
      "Objective value: 1207 , No. of solutions found: 35 Is solution optimal: False , Optimality gap: (0.739022,) , Test accuracy: 0.64\n",
      "Objective value: 1208 , No. of solutions found: 36 Is solution optimal: False , Optimality gap: (0.737583,) , Test accuracy: 0.64\n",
      "Objective value: 1209 , No. of solutions found: 37 Is solution optimal: False , Optimality gap: (0.736146,) , Test accuracy: 0.64\n",
      "Objective value: 1211 , No. of solutions found: 38 Is solution optimal: False , Optimality gap: (0.733278,) , Test accuracy: 0.64\n",
      "Objective value: 1212 , No. of solutions found: 39 Is solution optimal: False , Optimality gap: (0.731848,) , Test accuracy: 0.64\n",
      "Objective value: 1216 , No. of solutions found: 40 Is solution optimal: False , Optimality gap: (0.726151,) , Test accuracy: 0.64\n",
      "Objective value: 1224 , No. of solutions found: 41 Is solution optimal: False , Optimality gap: (0.714869,) , Test accuracy: 0.64\n",
      "Objective value: 1225 , No. of solutions found: 42 Is solution optimal: False , Optimality gap: (0.713469,) , Test accuracy: 0.64\n",
      "Objective value: 1226 , No. of solutions found: 43 Is solution optimal: False , Optimality gap: (0.712072,) , Test accuracy: 0.64\n",
      "Objective value: 1227 , No. of solutions found: 44 Is solution optimal: False , Optimality gap: (0.710676,) , Test accuracy: 0.64\n",
      "Objective value: 1228 , No. of solutions found: 45 Is solution optimal: False , Optimality gap: (0.709283,) , Test accuracy: 0.64\n",
      "Objective value: 1229 , No. of solutions found: 46 Is solution optimal: False , Optimality gap: (0.707893,) , Test accuracy: 0.64\n",
      "Objective value: 1567 , No. of solutions found: 47 Is solution optimal: False , Optimality gap: (0.339502,) , Test accuracy: 0.64\n",
      "Objective value: 1568 , No. of solutions found: 48 Is solution optimal: False , Optimality gap: (0.338648,) , Test accuracy: 0.64\n",
      "Objective value: 1569 , No. of solutions found: 49 Is solution optimal: False , Optimality gap: (0.337795,) , Test accuracy: 0.64\n",
      "Objective value: 1570 , No. of solutions found: 50 Is solution optimal: False , Optimality gap: (0.336943,) , Test accuracy: 0.64\n",
      "Objective value: 1574 , No. of solutions found: 51 Is solution optimal: False , Optimality gap: (0.333545,) , Test accuracy: 0.64\n",
      "Objective value: 1617 , No. of solutions found: 52 Is solution optimal: False , Optimality gap: (0.298083,) , Test accuracy: 0.64\n",
      "Objective value: 1621 , No. of solutions found: 53 Is solution optimal: False , Optimality gap: (0.29488,) , Test accuracy: 0.64\n",
      "Objective value: 1623 , No. of solutions found: 54 Is solution optimal: False , Optimality gap: (0.293284,) , Test accuracy: 0.64\n",
      "Objective value: 1627 , No. of solutions found: 55 Is solution optimal: False , Optimality gap: (0.290104,) , Test accuracy: 0.64\n",
      "Objective value: 1629 , No. of solutions found: 56 Is solution optimal: False , Optimality gap: (0.288521,) , Test accuracy: 0.64\n",
      "Objective value: 1630 , No. of solutions found: 57 Is solution optimal: False , Optimality gap: (0.28773,) , Test accuracy: 0.64\n",
      "Objective value: 1634 , No. of solutions found: 58 Is solution optimal: False , Optimality gap: (0.284578,) , Test accuracy: 0.64\n",
      "Objective value: 1643 , No. of solutions found: 59 Is solution optimal: False , Optimality gap: (0.277541,) , Test accuracy: 0.64\n",
      "Objective value: 1645 , No. of solutions found: 60 Is solution optimal: False , Optimality gap: (0.275988,) , Test accuracy: 0.64\n",
      "Objective value: 1647 , No. of solutions found: 61 Is solution optimal: False , Optimality gap: (0.274438,) , Test accuracy: 0.64\n",
      "Objective value: 1648 , No. of solutions found: 62 Is solution optimal: False , Optimality gap: (0.273665,) , Test accuracy: 0.64\n",
      "Objective value: 1650 , No. of solutions found: 63 Is solution optimal: False , Optimality gap: (0.272121,) , Test accuracy: 0.64\n",
      "Objective value: 1652 , No. of solutions found: 64 Is solution optimal: False , Optimality gap: (0.270581,) , Test accuracy: 0.64\n",
      "Objective value: 1656 , No. of solutions found: 65 Is solution optimal: False , Optimality gap: (0.267512,) , Test accuracy: 0.64\n",
      "Objective value: 1657 , No. of solutions found: 66 Is solution optimal: False , Optimality gap: (0.266747,) , Test accuracy: 0.64\n",
      "Objective value: 1658 , No. of solutions found: 67 Is solution optimal: False , Optimality gap: (0.265983,) , Test accuracy: 0.64\n",
      "Objective value: 1659 , No. of solutions found: 68 Is solution optimal: False , Optimality gap: (0.26522,) , Test accuracy: 0.64\n",
      "Objective value: 1665 , No. of solutions found: 69 Is solution optimal: False , Optimality gap: (0.260661,) , Test accuracy: 0.64\n",
      "Objective value: 1666 , No. of solutions found: 70 Is solution optimal: False , Optimality gap: (0.259904,) , Test accuracy: 0.64\n",
      "Objective value: 1667 , No. of solutions found: 71 Is solution optimal: False , Optimality gap: (0.259148,) , Test accuracy: 0.64\n",
      "Objective value: 1668 , No. of solutions found: 72 Is solution optimal: False , Optimality gap: (0.258393,) , Test accuracy: 0.64\n",
      "Objective value: 1669 , No. of solutions found: 73 Is solution optimal: False , Optimality gap: (0.257639,) , Test accuracy: 0.64\n",
      "Objective value: 1673 , No. of solutions found: 74 Is solution optimal: False , Optimality gap: (0.254632,) , Test accuracy: 0.64\n",
      "Objective value: 1674 , No. of solutions found: 75 Is solution optimal: False , Optimality gap: (0.253883,) , Test accuracy: 0.64\n",
      "Objective value: 1675 , No. of solutions found: 76 Is solution optimal: False , Optimality gap: (0.253134,) , Test accuracy: 0.64\n",
      "Objective value: 1676 , No. of solutions found: 77 Is solution optimal: False , Optimality gap: (0.252387,) , Test accuracy: 0.64\n",
      "Objective value: 1677 , No. of solutions found: 78 Is solution optimal: False , Optimality gap: (0.25164,) , Test accuracy: 0.64\n",
      "Objective value: 1678 , No. of solutions found: 79 Is solution optimal: False , Optimality gap: (0.250894,) , Test accuracy: 0.6614285714285715\n",
      "No more solutions available.\n",
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = [] #list to store objective values of all solutions\n",
    "runtimes = [] #list to store runtime values for all solutions found\n",
    "numsols = []\n",
    "test_acc = [] #list to store test accuracies of all solutions found\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_margin_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and test accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test) #compute output activations (predictions) of test set\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values) #compute test accuracy and number of misclassified test examples\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_margin_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_margin_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "#computing the exact training accuracy of the last obtained solution\n",
    "max_margin_solution = max_margin_solutions.get_last_solution()\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=max_margin_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783.25"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_margin_solution.get_solver_infos()['TotalTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the solver isn't able to optimize the solution, but the best solution found generalizes only as much as the solution found before, not better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Weight Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the objective function used is the sum of absolute weights, i.e\n",
    "\n",
    "$$\\min \\sum_{\\ell \\in \\{1,\\ldots,L\\}} \\sum_{i \\in N_{\\ell - 1}} \\sum_{j \\in N_{\\ell}} |w_{i\\ell j}|$$\n",
    "\n",
    "where $w_{i\\ell j}$ denotes the weight of the connection between neuron $j$ of layer $\\ell$ to neuron $i$ from layer $\\ell - 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n",
      " ! --------------------------------------------------- CP Optimizer 20.1.0.0 --\n",
      " ! Minimization problem - 16163 variables, 17610 constraints\n",
      " ! Presolve      : 1 extractable eliminated\n",
      " ! TimeLimit            = 600\n",
      " ! Workers              = 12\n",
      " ! LogVerbosity         = Terse\n",
      " ! SearchType           = Restart\n",
      " ! Initial process time : 0.08s (0.08s extraction + 0.00s propagation)\n",
      " !  . Log search space  : 2203.8 (before), 2203.8 (after)\n",
      " !  . Memory usage      : 11.2 MB (before), 11.2 MB (after)\n",
      " ! Using parallel search with 12 workers.\n",
      " ! ----------------------------------------------------------------------------\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      "                        0      16163                 -\n",
      " + New bound is 0\n",
      "                        0       2027    1            -\n",
      " + New bound is 1\n",
      "                     3406       1833   12   F     0 != w_4_1_1\n",
      " + New bound is 2\n",
      " *           230     178k  75.22s       5      (gap is 99.13%)\n",
      " *           229     190k  80.58s      12      (gap is 99.13%)\n",
      " *           228     190k  80.58s      12      (gap is 99.12%)\n",
      " *           227     190k  80.59s      12      (gap is 99.12%)\n",
      " *           226     190k  80.59s      12      (gap is 99.12%)\n",
      " *           225     190k  80.59s      12      (gap is 99.11%)\n",
      " *           224     190k  80.59s      12      (gap is 99.11%)\n",
      " *           223     190k  80.59s      12      (gap is 99.10%)\n",
      " *           222     196k  80.99s       7      (gap is 99.10%)\n",
      " *           221     190k  80.99s      12      (gap is 99.10%)\n",
      " *           220     191k  80.99s      12      (gap is 99.09%)\n",
      " *           219     191k  80.99s      12      (gap is 99.09%)\n",
      " *           218     191k  80.99s      12      (gap is 99.08%)\n",
      " *           217     191k  80.99s      12      (gap is 99.08%)\n",
      " *           216     184k  81.40s       6      (gap is 99.07%)\n",
      " *           215     184k  81.40s       6      (gap is 99.07%)\n",
      " *           214     184k  81.40s       6      (gap is 99.07%)\n",
      " ! Time = 81.40s, Average fail depth = 384, Memory usage = 411.9 MB\n",
      " ! Current bound is 2 (gap is 99.07%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           213     184k  81.40s       6      (gap is 99.06%)\n",
      " *           212     185k  81.40s       6      (gap is 99.06%)\n",
      " *           211     185k  81.76s       6      (gap is 99.05%)\n",
      " *           210     185k  81.76s       6      (gap is 99.05%)\n",
      " *           209     185k  81.76s       6      (gap is 99.04%)\n",
      " *           208     185k  81.76s       6      (gap is 99.04%)\n",
      " *           207     186k  81.76s       6      (gap is 99.03%)\n",
      " *           206     186k  81.76s       6      (gap is 99.03%)\n",
      " *           205     198k  82.15s       4      (gap is 99.02%)\n",
      " *           204     198k  82.15s       4      (gap is 99.02%)\n",
      " *           203     198k  82.15s       4      (gap is 99.01%)\n",
      " *           202     198k  82.58s       4      (gap is 99.01%)\n",
      " *           201     199k  82.58s       4      (gap is 99.00%)\n",
      " *           200     199k  82.58s       4      (gap is 99.00%)\n",
      " *           199     188k  82.58s       6      (gap is 98.99%)\n",
      " *           198     188k  82.58s       6      (gap is 98.99%)\n",
      " *           197     188k  82.58s       6      (gap is 98.98%)\n",
      " *           196     188k  82.58s       6      (gap is 98.98%)\n",
      " *           195     200k  82.97s       4      (gap is 98.97%)\n",
      " *           194     201k  82.97s       7      (gap is 98.97%)\n",
      " ! Time = 82.97s, Average fail depth = 417, Memory usage = 552.8 MB\n",
      " ! Current bound is 2 (gap is 98.97%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           193     201k  82.97s       7      (gap is 98.96%)\n",
      " *           192     202k  83.34s       7      (gap is 98.96%)\n",
      " *           191     199k  83.34s       8      (gap is 98.95%)\n",
      " *           190     199k  83.34s       8      (gap is 98.95%)\n",
      " *           189     202k  83.67s       4      (gap is 98.94%)\n",
      " *           188     191k  83.67s       6      (gap is 98.94%)\n",
      " *           187     191k  83.67s       6      (gap is 98.93%)\n",
      " *           186     204k  83.67s       9      (gap is 98.92%)\n",
      " *           185     204k  83.67s       9      (gap is 98.92%)\n",
      " *           184     204k  84.04s       9      (gap is 98.91%)\n",
      " *           183     205k  84.04s       9      (gap is 98.91%)\n",
      " *           182     205k  84.04s       9      (gap is 98.90%)\n",
      " *           181     205k  84.04s       9      (gap is 98.90%)\n",
      " *           180     205k  84.04s       9      (gap is 98.89%)\n",
      " *           179     205k  84.04s       9      (gap is 98.88%)\n",
      " *           178     205k  84.38s       9      (gap is 98.88%)\n",
      " *           177     193k  84.75s       6      (gap is 98.87%)\n",
      " *           176     193k  84.75s       6      (gap is 98.86%)\n",
      " *           175     193k  84.76s       6      (gap is 98.86%)\n",
      " *           174     194k  84.76s       6      (gap is 98.85%)\n",
      " ! Time = 84.76s, Average fail depth = 416, Memory usage = 686.9 MB\n",
      " ! Current bound is 2 (gap is 98.85%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           173     194k  84.76s       6      (gap is 98.84%)\n",
      " *           172     194k  84.76s       6      (gap is 98.84%)\n",
      " *           171     194k  85.10s       6      (gap is 98.83%)\n",
      " *           170     201k  85.10s      12      (gap is 98.82%)\n",
      " *           169     201k  85.10s      12      (gap is 98.82%)\n",
      " *           168     201k  85.11s      12      (gap is 98.81%)\n",
      " *           167     202k  85.11s      12      (gap is 98.80%)\n",
      " *           166     202k  85.11s      12      (gap is 98.80%)\n",
      " *           165     202k  85.11s      12      (gap is 98.79%)\n",
      " *           164     207k  85.49s       3      (gap is 98.78%)\n",
      " *           162     196k  85.49s       6      (gap is 98.77%)\n",
      " *           161     196k  85.49s       6      (gap is 98.76%)\n",
      " *           160     204k  85.81s      12      (gap is 98.75%)\n",
      " *           159     208k  85.81s       3      (gap is 98.74%)\n",
      " *           158     208k  85.81s       3      (gap is 98.73%)\n",
      " *           157     208k  85.81s       3      (gap is 98.73%)\n",
      " *           156     196k  85.81s       6      (gap is 98.72%)\n",
      " *           155     196k  85.83s       6      (gap is 98.71%)\n",
      " *           154     196k  85.83s       6      (gap is 98.70%)\n",
      " *           153     196k  85.83s       6      (gap is 98.69%)\n",
      " ! Time = 85.83s, Average fail depth = 320, Memory usage = 681.8 MB\n",
      " ! Current bound is 2 (gap is 98.69%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           152     197k  85.83s       6      (gap is 98.68%)\n",
      " *           151     197k  86.13s       6      (gap is 98.68%)\n",
      " *           150     211k  86.13s       9      (gap is 98.67%)\n",
      " *           149     211k  86.13s       9      (gap is 98.66%)\n",
      " *           148     211k  86.13s       9      (gap is 98.65%)\n",
      " *           147     211k  86.47s       9      (gap is 98.64%)\n",
      " *           146     207k  86.74s      12      (gap is 98.63%)\n",
      " *           145     195k  87.09s      11      (gap is 98.62%)\n",
      " *           144     196k  87.09s      11      (gap is 98.61%)\n",
      " *           143     212k  87.09s       3      (gap is 98.60%)\n",
      " *           142     212k  87.09s       3      (gap is 98.59%)\n",
      " *           141     213k  87.10s       7      (gap is 98.58%)\n",
      " *           140     210k  87.10s       8      (gap is 98.57%)\n",
      " *           139     206k  87.10s      10      (gap is 98.56%)\n",
      " *           138     209k  87.42s      12      (gap is 98.55%)\n",
      " *           137     209k  87.42s      12      (gap is 98.54%)\n",
      " *           136     209k  87.42s      12      (gap is 98.53%)\n",
      " *           135     209k  87.42s      12      (gap is 98.52%)\n",
      " *           134     207k  87.42s      10      (gap is 98.51%)\n",
      " *           133     207k  87.42s      10      (gap is 98.50%)\n",
      " ! Time = 87.42s, Average fail depth = 320, Memory usage = 827.6 MB\n",
      " ! Current bound is 2 (gap is 98.50%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           132     210k  87.76s      12      (gap is 98.48%)\n",
      " *           131     210k  87.76s      12      (gap is 98.47%)\n",
      " *           130     210k  87.77s      12      (gap is 98.46%)\n",
      " *           129     210k  87.77s      12      (gap is 98.45%)\n",
      " *           128     202k  87.77s       6      (gap is 98.44%)\n",
      " *           127     202k  87.77s       6      (gap is 98.43%)\n",
      " *           126     202k  87.77s       6      (gap is 98.41%)\n",
      " *           125     202k  87.77s       6      (gap is 98.40%)\n",
      " *           124     203k  87.77s       6      (gap is 98.39%)\n",
      " *           123     203k  87.77s       6      (gap is 98.37%)\n",
      " *           122     203k  87.77s       6      (gap is 98.36%)\n",
      " *           121     203k  88.06s       6      (gap is 98.35%)\n",
      " *           120     212k  88.38s      12      (gap is 98.33%)\n",
      " *           119     212k  88.38s      12      (gap is 98.32%)\n",
      " *           118     206k  88.38s       5      (gap is 98.31%)\n",
      " *           117     206k  88.67s       5      (gap is 98.29%)\n",
      " *           116     211k  89.00s      10      (gap is 98.28%)\n",
      " *           115     211k  89.00s      10      (gap is 98.26%)\n",
      " *           114     211k  89.00s      10      (gap is 98.25%)\n",
      " *           113     212k  89.47s      10      (gap is 98.23%)\n",
      " ! Time = 89.47s, Average fail depth = 318, Memory usage = 560.6 MB\n",
      " ! Current bound is 2 (gap is 98.23%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *           112     207k  89.47s       6      (gap is 98.21%)\n",
      " *           111     207k  89.47s       6      (gap is 98.20%)\n",
      " *           110     208k  89.47s       6      (gap is 98.18%)\n",
      " *           109     208k  89.47s       6      (gap is 98.17%)\n",
      " *           108     210k  89.79s       5      (gap is 98.15%)\n",
      " *           107     210k  89.79s       5      (gap is 98.13%)\n",
      " *           106     210k  90.04s       5      (gap is 98.11%)\n",
      " *           105     223k  90.33s       9      (gap is 98.10%)\n",
      " *           104     224k  90.33s       9      (gap is 98.08%)\n",
      " *           103     212k  90.58s       5      (gap is 98.06%)\n",
      " *           102     212k  90.58s       5      (gap is 98.04%)\n",
      " *           101     213k  90.58s       5      (gap is 98.02%)\n",
      " *           100     220k  90.86s      12      (gap is 98.00%)\n",
      " *            99     213k  90.86s       5      (gap is 97.98%)\n",
      " *            98     213k  90.86s       6      (gap is 97.96%)\n",
      " *            97     213k  91.15s       6      (gap is 97.94%)\n",
      " *            96     224k  91.79s       8      (gap is 97.92%)\n",
      " *            95     225k  91.79s       8      (gap is 97.89%)\n",
      " *            94     225k  91.81s       8      (gap is 97.87%)\n",
      " *            93     225k  91.81s       8      (gap is 97.85%)\n",
      " ! Time = 91.81s, Average fail depth = 285, Memory usage = 548.9 MB\n",
      " ! Current bound is 2 (gap is 97.85%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *            92     226k  92.13s       8      (gap is 97.83%)\n",
      " *            91     224k  93.08s      10      (gap is 97.80%)\n",
      " *            90     225k  93.40s      10      (gap is 97.78%)\n",
      " *            89     223k  94.06s       5      (gap is 97.75%)\n",
      " *            88     223k  94.06s       5      (gap is 97.73%)\n",
      " *            87     223k  94.06s       5      (gap is 97.70%)\n",
      " *            86     223k  94.36s       5      (gap is 97.67%)\n",
      " *            85     238k  94.95s       9      (gap is 97.65%)\n",
      " *            84     226k  94.95s       5      (gap is 97.62%)\n",
      " *            83     227k  95.24s       5      (gap is 97.59%)\n",
      " *            82     237k  95.59s       8      (gap is 97.56%)\n",
      " *            81     229k  96.17s       6      (gap is 97.53%)\n",
      " *            80     232k  97.13s       5      (gap is 97.50%)\n",
      " *            79     232k  97.13s       6      (gap is 97.47%)\n",
      " *            78     241k  97.13s       8      (gap is 97.44%)\n",
      " *            77     241k  97.42s       8      (gap is 97.40%)\n",
      " *            76     239k  97.75s       2      (gap is 97.37%)\n",
      " *            75     240k  97.75s       2      (gap is 97.33%)\n",
      " *            74     240k  98.08s       2      (gap is 97.30%)\n",
      " *            73     250k  98.99s       3      (gap is 97.26%)\n",
      " ! Time = 98.99s, Average fail depth = 282, Memory usage = 550.8 MB\n",
      " ! Current bound is 2 (gap is 97.26%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *            72     253k  100.08s      9      (gap is 97.22%)\n",
      " *            71     240k  100.42s      1      (gap is 97.18%)\n",
      " *            70     241k  100.42s      1      (gap is 97.14%)\n",
      " *            69     241k  100.42s      1      (gap is 97.10%)\n",
      " *            68     241k  100.72s      1      (gap is 97.06%)\n",
      " *            67     256k  101.06s      9      (gap is 97.01%)\n",
      " *            66     256k  101.06s      9      (gap is 96.97%)\n",
      " *            65     256k  101.06s      9      (gap is 96.92%)\n",
      " *            64     259k  102.33s      9      (gap is 96.88%)\n",
      " *            63     264k  104.33s      4      (gap is 96.83%)\n",
      " *            62     271k  106.79s      9      (gap is 96.77%)\n",
      " *            61     266k  107.92s     10      (gap is 96.72%)\n",
      " *            60     267k  108.56s     10      (gap is 96.67%)\n",
      " *            59     264k  108.56s      5      (gap is 96.61%)\n",
      " *            58     277k  108.83s      9      (gap is 96.55%)\n",
      " *            57     278k  109.17s      9      (gap is 96.49%)\n",
      " *            56     280k  109.94s      9      (gap is 96.43%)\n",
      " *            55     286k  112.72s      4      (gap is 96.36%)\n",
      " *            54     283k  117.18s     11      (gap is 96.30%)\n",
      " *            53     296k  118.29s      8      (gap is 96.23%)\n",
      " ! Time = 118.29s, Average fail depth = 272, Memory usage = 583.8 MB\n",
      " ! Current bound is 2 (gap is 96.23%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *            52     289k  119.52s      6      (gap is 96.15%)\n",
      " *            51     301k  122.20s      2      (gap is 96.08%)\n",
      " *            50     308k  125.00s      2      (gap is 96.00%)\n",
      " *            49     311k  126.17s      2      (gap is 95.92%)\n",
      " *            48     306k  127.02s      6      (gap is 95.83%)\n",
      " *            47     312k  129.04s      5      (gap is 95.74%)\n",
      " *            46     335k  134.54s      9      (gap is 95.65%)\n",
      " *            45     323k  134.92s     10      (gap is 95.56%)\n",
      " *            44     325k  136.09s     10      (gap is 95.45%)\n",
      " *            43     330k  139.02s      6      (gap is 95.35%)\n",
      " *            42     338k  141.94s      5      (gap is 95.24%)\n",
      " *            41     343k  144.72s      5      (gap is 95.12%)\n",
      " *            40     353k  148.29s      1      (gap is 95.00%)\n",
      " *            39     354k  150.49s      6      (gap is 94.87%)\n",
      " *            38     384k  154.97s      7      (gap is 94.74%)\n",
      " *            37     384k  155.44s      7      (gap is 94.59%)\n",
      " *            36     405k  166.90s      7      (gap is 94.44%)\n",
      " *            35     402k  168.33s      8      (gap is 94.29%)\n",
      " *            34     497k  219.10s      2      (gap is 94.12%)\n",
      " *            33     491k  226.13s      6      (gap is 93.94%)\n",
      " ! Time = 226.13s, Average fail depth = 247, Memory usage = 592.5 MB\n",
      " ! Current bound is 2 (gap is 93.94%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *            32     491k  226.15s      6      (gap is 93.75%)\n",
      " *            31     491k  226.15s      6      (gap is 93.55%)\n",
      " *            30     492k  226.61s      6      (gap is 93.33%)\n",
      " *            29     492k  227.08s      6      (gap is 93.10%)\n",
      " *            28     493k  227.49s      6      (gap is 92.86%)\n",
      " *            27     521k  243.61s      6      (gap is 92.59%)\n",
      " *            26     548k  259.38s      5      (gap is 92.31%)\n",
      " *            25     576k  276.86s     10      (gap is 92.00%)\n",
      " *            24     647k  323.13s      8      (gap is 91.67%)\n",
      " *            23     687k  327.72s      2      (gap is 91.30%)\n",
      " *            22     690k  353.49s      8      (gap is 90.91%)\n",
      " *            21     878k  420.79s      3      (gap is 90.48%)\n",
      " *            20     892k  455.94s      7      (gap is 90.00%)\n",
      " *            19     831k  458.15s     10      (gap is 89.47%)\n",
      " *            18     832k  458.74s     10      (gap is 88.89%)\n",
      " *            17     855k  474.02s     10      (gap is 88.24%)\n",
      " *            16     902k  507.61s     10      (gap is 87.50%)\n",
      " ! ----------------------------------------------------------------------------\n",
      " ! Search terminated by limit, 214 solutions found.\n",
      " ! Best objective         : 16 (gap is 87.50%)\n",
      " ! Best bound             : 2\n",
      " ! ----------------------------------------------------------------------------\n",
      " ! Number of branches     : 12569952\n",
      " ! Number of fails        : 4068598\n",
      " ! Total memory usage     : 557.9 MB (556.1 MB CP Optimizer + 1.8 MB Concert)\n",
      " ! Time spent in solve    : 600.10s (600.03s engine + 0.08s extraction)\n",
      " ! Search speed (br. / s) : 20948.9\n",
      " ! ----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1\n",
    "\n",
    "max_obj_val=0 #maximum possible value of the min weight objective\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1]) #max value is when absolute value of every weight variable is 1, i.e all weights are non-zero\n",
    "max_obj_val\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model')\n",
    "\n",
    "#initialize lists for storing weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0, x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#activations and weights for hidden layers \n",
    "for l in range(1, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and weights for output layer\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "                #correct_prediction_condition = min_weight_mdl.logical_and([activations_l[k][j] == y_train.iloc[k][j] for j in range(N[l])]) \n",
    "                #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for atleast 75% of training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "#define the objective\n",
    "min_weight_obj = min_weight_mdl.integer_var(0, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solution = min_weight_mdl.solve(TimeLimit=600,LogVerbosity='Terse',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller time limit is used in this case since it was initially observed that the solver would very quickly find a solution close to the lower bound but spend a long time trying to prove optimality/find better solutions (with unrealistically small values of the min weight objective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "#compute exact training accuracy of the last solution found\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=min_weight_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5857142857142857\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "# Compute test accuracy of the solution\n",
    "\n",
    "# Extract the Weights from the Solution\n",
    "weights_solution = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_solution[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        weights_solution[layer_idx].append([min_weight_solution[weight_var] for weight_var in neuron_weights]) \n",
    "\n",
    "# Compute the activations for the test set\n",
    "output_activations = compute_activations(weights_solution, x_test)\n",
    "\n",
    "# Assume y_test is your true labels for the test set\n",
    "test_accuracy,incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print(len(incorrect_egs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Accuracy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the objective function used is the training accuracy, i.e\n",
    "\n",
    "$$\\max \\left( \\frac{\\sum_{k=1}^{|\\mathcal{T}|}c_k }{|\\mathcal{T}|} \\right)$$\n",
    "where $c_k$ is a boolean variable with value 1 indicating correct classification of example $k$ of the training set, and value 0 indicating misclassification of example $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n",
      " ! --------------------------------------------------- CP Optimizer 20.1.0.0 --\n",
      " ! Maximization problem - 16162 variables, 17613 constraints\n",
      " ! Presolve      : 3 extractables eliminated\n",
      " ! TimeLimit            = 1800\n",
      " ! Workers              = 12\n",
      " ! LogVerbosity         = Terse\n",
      " ! SearchType           = Restart\n",
      " ! Initial process time : 0.22s (0.22s extraction + 0.00s propagation)\n",
      " !  . Log search space  : 2195.6 (before), 2195.6 (after)\n",
      " !  . Memory usage      : 11.6 MB (before), 11.6 MB (after)\n",
      " ! Using parallel search with 12 workers.\n",
      " ! ----------------------------------------------------------------------------\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      "                        0       2026                 -\n",
      " + New bound is 100.0000\n",
      "                        0       2026    1            -\n",
      " + New bound is 99.59716\n",
      " *      49.99999      717  0.65s        7      (gap is 99.19%)\n",
      " *      50.40322    27157  13.75s      12      (gap is 97.60%)\n",
      " *      50.80645    27295  13.75s      12      (gap is 96.03%)\n",
      " *      51.20967    27707  14.08s      12      (gap is 94.49%)\n",
      " *      51.61290    30158  14.90s      12      (gap is 92.97%)\n",
      " *      52.01612    28242  14.90s       5      (gap is 91.47%)\n",
      " *      52.41935    29585  15.24s       1      (gap is 90.00%)\n",
      " *      52.82258    29620  15.72s       3      (gap is 88.55%)\n",
      " *      53.22580    28271  15.97s       4      (gap is 87.12%)\n",
      " *      53.62903    28412  15.97s       4      (gap is 85.72%)\n",
      " *      54.03225    29605  16.40s       4      (gap is 84.33%)\n",
      " *      54.43548    36299  17.40s      12      (gap is 82.96%)\n",
      " *      54.83870    36541  17.40s      12      (gap is 81.62%)\n",
      " *      55.24193    37416  17.88s      12      (gap is 80.29%)\n",
      " *      55.64516    39357  18.22s       2      (gap is 78.99%)\n",
      " *      56.04838    39962  18.59s       2      (gap is 77.70%)\n",
      " *      56.45161    40439  18.97s       1      (gap is 76.43%)\n",
      " *      56.85483    41398  19.31s       1      (gap is 75.18%)\n",
      " ! Time = 19.31s, Average fail depth = 488, Memory usage = 463.4 MB\n",
      " ! Current bound is 99.59716 (gap is 75.18%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      57.25806    40353  20.02s       9      (gap is 73.94%)\n",
      " *      57.66129    42547  20.26s      10      (gap is 72.73%)\n",
      " *      58.06451    42359  20.27s       3      (gap is 71.53%)\n",
      " *      58.46774    46378  20.97s       2      (gap is 70.35%)\n",
      " *      58.87096    46566  20.97s       2      (gap is 69.18%)\n",
      " *      59.67741    44749  21.65s       9      (gap is 66.89%)\n",
      " *      60.08064    47178  22.84s       9      (gap is 65.77%)\n",
      " *      60.48387    51220  23.60s       2      (gap is 64.67%)\n",
      " *      60.88709    51813  24.00s       2      (gap is 63.58%)\n",
      " *      61.29032    46491  24.31s       6      (gap is 62.50%)\n",
      " *      61.69354    46805  24.56s       6      (gap is 61.44%)\n",
      " *      62.09677    52952  25.04s       8      (gap is 60.39%)\n",
      " *      62.49999    52884  25.84s       3      (gap is 59.36%)\n",
      " *      62.90322    53433  25.85s       3      (gap is 58.33%)\n",
      " *      63.30645    53896  26.13s       3      (gap is 57.33%)\n",
      " *      63.70967    54388  27.22s       5      (gap is 56.33%)\n",
      " *      64.11290    53323  27.22s       9      (gap is 55.35%)\n",
      " *      64.51612    52609  27.86s      10      (gap is 54.38%)\n",
      " *      64.91935    53929  28.58s       4      (gap is 53.42%)\n",
      " *      65.32258    62832  29.40s       8      (gap is 52.47%)\n",
      " ! Time = 29.40s, Average fail depth = 389, Memory usage = 531.2 MB\n",
      " ! Current bound is 99.59716 (gap is 52.47%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      66.12903    55177  30.13s      10      (gap is 50.61%)\n",
      " *      66.53225    65495  30.52s       8      (gap is 49.70%)\n",
      " *      66.93548    59973  30.86s       5      (gap is 48.80%)\n",
      " *      67.74193    63042  31.15s      12      (gap is 47.02%)\n",
      " *      68.14516    64486  31.97s      12      (gap is 46.15%)\n",
      " *      68.54838    67489  33.85s      12      (gap is 45.29%)\n",
      " *      68.95161    69415  34.18s       2      (gap is 44.45%)\n",
      " *      69.35483    69683  34.18s       2      (gap is 43.61%)\n",
      " *      70.16129    65071  35.06s       9      (gap is 41.95%)\n",
      " *      70.56451    65527  36.24s       4      (gap is 41.14%)\n",
      " *      70.96774    68783  37.84s       4      (gap is 40.34%)\n",
      " *      71.37096    70943  38.24s       7      (gap is 39.55%)\n",
      " *      71.77419    79389  38.74s      11      (gap is 38.76%)\n",
      " *      72.17741    80107  39.04s      11      (gap is 37.99%)\n",
      " *      72.58064    81686  39.75s       8      (gap is 37.22%)\n",
      " *      72.98387    82553  40.22s       8      (gap is 36.46%)\n",
      " *      73.38709    74269  42.24s      10      (gap is 35.71%)\n",
      " *      73.79032    76378  43.58s      10      (gap is 34.97%)\n",
      " *      74.19354    77557  44.49s      10      (gap is 34.24%)\n",
      " *      74.59677    89877  44.95s      12      (gap is 33.51%)\n",
      " ! Time = 44.97s, Average fail depth = 299, Memory usage = 562.2 MB\n",
      " ! Current bound is 99.59716 (gap is 33.51%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      74.99999    81268  46.83s      10      (gap is 32.80%)\n",
      " *      75.40322    87766  48.06s       6      (gap is 32.09%)\n",
      " *      75.80645    88138  48.06s       6      (gap is 31.38%)\n",
      " *      76.20967    89501  49.01s       6      (gap is 30.69%)\n",
      " *      76.61290    95654  49.42s       2      (gap is 30.00%)\n",
      " *      77.01612    95741  49.42s       2      (gap is 29.32%)\n",
      " *      77.41935    96520  49.86s       2      (gap is 28.65%)\n",
      " *      77.82258    97681  50.65s       2      (gap is 27.98%)\n",
      " *      78.22580    86983  51.15s      10      (gap is 27.32%)\n",
      " *      78.62903    87266  51.15s      10      (gap is 26.67%)\n",
      " *      79.03225    87887  51.52s      10      (gap is 26.02%)\n",
      " *      79.43548    91808  52.11s       4      (gap is 25.38%)\n",
      " *      79.83870    93002  52.61s       4      (gap is 24.75%)\n",
      " *      80.24193    93146  52.61s       4      (gap is 24.12%)\n",
      " *      80.64516    99739  53.58s       5      (gap is 23.50%)\n",
      " *      81.04838     104k  54.47s      12      (gap is 22.89%)\n",
      " *      81.45161    93365  55.92s      10      (gap is 22.28%)\n",
      " *      81.85483    96238  57.26s      10      (gap is 21.68%)\n",
      " *      82.25806    96790  57.68s      10      (gap is 21.08%)\n",
      " *      82.66129     107k  58.22s       5      (gap is 20.49%)\n",
      " ! Time = 58.22s, Average fail depth = 277, Memory usage = 584.2 MB\n",
      " ! Current bound is 99.59716 (gap is 20.49%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      83.06451     107k  58.22s       5      (gap is 19.90%)\n",
      " *      83.46774     104k  59.15s       6      (gap is 19.32%)\n",
      " *      83.87096     110k  60.04s      11      (gap is 18.75%)\n",
      " *      84.27419     123k  63.04s       1      (gap is 18.18%)\n",
      " *      84.67741     109k  63.58s       6      (gap is 17.62%)\n",
      " *      85.08064     117k  64.99s       2      (gap is 17.06%)\n",
      " *      85.48387     124k  69.72s       2      (gap is 16.51%)\n",
      " *      85.88709     128k  70.20s      12      (gap is 15.96%)\n",
      " *      86.29032     116k  70.60s       7      (gap is 15.42%)\n",
      " *      86.69354     117k  71.76s       7      (gap is 14.88%)\n",
      " *      87.09677     131k  72.74s      12      (gap is 14.35%)\n",
      " *      87.49999     131k  73.31s      12      (gap is 13.83%)\n",
      " *      87.90322     137k  74.33s       1      (gap is 13.30%)\n",
      " *      88.30645     136k  78.22s       5      (gap is 12.79%)\n",
      " *      88.70967     131k  81.24s       6      (gap is 12.27%)\n",
      " *      89.11290     139k  85.60s       4      (gap is 11.77%)\n",
      " *      89.51612     144k  89.29s       4      (gap is 11.26%)\n",
      " *      89.91935     145k  90.86s       4      (gap is 10.76%)\n",
      " *      90.32258     147k  92.00s       6      (gap is 10.27%)\n",
      " *      90.72580     156k  96.92s      11      (gap is 9.78%)\n",
      " ! Time = 96.92s, Average fail depth = 230, Memory usage = 583.8 MB\n",
      " ! Current bound is 99.59716 (gap is 9.78%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      91.12903     160k  103.18s      3      (gap is 9.29%)\n",
      " *      91.53225     164k  104.83s      9      (gap is 8.81%)\n",
      " *      91.93548     164k  104.83s      9      (gap is 8.33%)\n",
      " *      92.33870     177k  110.18s      5      (gap is 7.86%)\n",
      " *      92.74193     180k  111.79s      5      (gap is 7.39%)\n",
      " *      93.14516     166k  113.25s     10      (gap is 6.93%)\n",
      " *      93.54838     168k  115.00s     10      (gap is 6.47%)\n",
      " *      93.95161     184k  116.15s      7      (gap is 6.01%)\n",
      " *      94.35483     186k  117.26s      7      (gap is 5.56%)\n",
      " *      94.75806     187k  117.70s      5      (gap is 5.11%)\n",
      " *      95.56451     193k  121.97s      2      (gap is 4.22%)\n",
      " *      95.96774     215k  140.92s     12      (gap is 3.78%)\n",
      " *      96.37096     217k  142.83s     12      (gap is 3.35%)\n",
      " *      96.77419     220k  145.38s     12      (gap is 2.92%)\n",
      " *      97.17741     216k  156.29s     10      (gap is 2.49%)\n",
      " *      97.58064     228k  160.72s      6      (gap is 2.07%)\n",
      " *      97.98387     238k  164.13s      7      (gap is 1.65%)\n",
      " *      98.38709     241k  167.42s      7      (gap is 1.23%)\n",
      " *      98.79032     246k  173.02s      5      (gap is 0.82%)\n",
      " *      99.19354     250k  177.04s      5      (gap is 0.41%)\n",
      " ! Time = 177.04s, Average fail depth = 190, Memory usage = 589.9 MB\n",
      " ! Current bound is 99.59716 (gap is 0.41%)\n",
      " !          Best Branches  Non-fixed    W       Branch decision\n",
      " *      99.59677     253k  180.22s      8      (gap is 0.00%)\n",
      " ! ----------------------------------------------------------------------------\n",
      " ! Search completed, 119 solutions found.\n",
      " ! Best objective         : 99.59677 (optimal - effective tol. is 0.00995968)\n",
      " ! Best bound             : 99.59716\n",
      " ! ----------------------------------------------------------------------------\n",
      " ! Number of branches     : 3012670\n",
      " ! Number of fails        : 903698\n",
      " ! Total memory usage     : 552.8 MB (551.0 MB CP Optimizer + 1.8 MB Concert)\n",
      " ! Time spent in solve    : 180.76s (180.54s engine + 0.22s extraction)\n",
      " ! Search speed (br. / s) : 16687.0\n",
      " ! ----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #index of the credit_amount column\n",
    "x_max = x_train.max()[idx_cred_amt] #max value of the dataframe (present in credit amount column)\n",
    "\n",
    "max_acc_mdl = CpoModel(name='German Credit CP Model 3')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_acc_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_acc_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #defnition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#first hidden layer activations\n",
    "activations_1 = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#activations and weights for further hidden layers \n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights to output layer\n",
    "activations_L = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "corr_pred = [max_acc_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "max_acc_mdl.add(max_acc_mdl.sum(corr_pred)<x_train.shape[0])\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_acc_mdl.add(maximize(max_acc_mdl.sum(corr_pred)*100/x_train.shape[0]))\n",
    "            \n",
    "# breaking the symmetry for incoming weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_acc_mdl.add(max_acc_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_acc_solution = max_acc_mdl.solve(TimeLimit=1800,LogVerbosity='Terse',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "#Compute test accuracy of the solution\n",
    "\n",
    "#Extract the Weights from the Solution\n",
    "weights_solution = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_solution[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        weights_solution[layer_idx].append([max_acc_solution[weight_var] for weight_var in neuron_weights])\n",
    "\n",
    "output_activations = compute_activations(weights_solution, x_test) #compute output activations (predictions) of the test set\n",
    "test_accuracy,incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values) #compute test accuracy and indices of misclassified examples\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print(len(incorrect_egs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Robustness Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit amount and credit duration variables are used to define the perturbation set:\n",
    "\n",
    "$$ S_\\epsilon (x) = \\{y \\in \\mathcal{T}| y[{ \\mathcal{F}\\backslash \\{cred\\_ amt,cred\\_ dur \\}}] = x[{ \\mathcal{F}\\backslash \\{cred\\_ amt,cred\\_ dur\\}}], |x[cred\\_ amt]-y[cred\\_ amt]| \\leq \\epsilon_{cred\\_ amt}, |x[cred\\_ dur]-y[cred\\_ dur]|\\leq \\epsilon_{cred\\_ dur} \\}$$\n",
    "\n",
    "The definition of robustness implies that $\\forall y \\in S_\\epsilon(x), F(y)=F(x)$, where $F(x)$ denotes the output (prediction) of the BNN for input $x$. \n",
    "\n",
    "OBSERVATION: $\\forall \\ell \\in \\{1,\\ldots,L\\}, n_\\ell (x) = n_\\ell (y) \\implies n_{\\ell+1}(x) = n_{\\ell+1}(y)$. \n",
    "\n",
    "So, the robustness constraint can be simplified to: $\\forall y \\in S_\\epsilon(x), n_1(y)=n_1(x)$.\n",
    "\n",
    "Further, if we define for each sample $x$ and neuron j in layer 1:\n",
    "$$\n",
    "x_{j}^{up}[i] = \\begin{cases}\n",
    "x[i]-\\epsilon, & w_{i1j}\\geq 0\\\\\n",
    "x[i]+\\epsilon, & w_{i1j}<0\n",
    "\\end{cases}\n",
    "$$\n",
    "                \n",
    "$$\n",
    "x_{j}^{down}[i] = \\begin{cases}\n",
    "x[i]+\\epsilon, & w_{i1j}\\geq 0\\\\\n",
    "x[i]-\\epsilon, & w_{i1j}<0\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "So, if neuron j is activated for sample $x$ and $x_{j}^{up}$, it is activated $\\forall y \\in S_{\\epsilon}(x)$. If j is not activated for $x$ and $x_{j}^{down}$, then it is not activated $\\forall y \\in S_{\\epsilon}(x)$. \n",
    "\n",
    "Finally, the robustness constraints can be simplified to: $\\forall x \\in \\mathcal{T}, n_1(x_j^{up})=n_1(x_j^{down})=n_1(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Margin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "max_margin_mdl = CpoModel(name='German Credit CP Model 2')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_margin_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_margin_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#define the lists containing x_up and x_down for each neuron j in layer 1, for all training examples x\n",
    "pert_up = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "\n",
    "#define the lists containing input layer activations for each example of pert_up and pert_down defined above\n",
    "activations_0_up = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            #definition of input layer activations\n",
    "            max_margin_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i]) \n",
    "            max_margin_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first hidden layer activations for training examples\n",
    "activations_1 = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations for each example of pert_up and pert_down defined above\n",
    "activations_1_up = [[[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down \n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down \n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features do not change in the perturbed examples \n",
    "                max_margin_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                max_margin_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definitions of layer 1 activations\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            max_margin_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of example x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [max_margin_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #max_margin_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "                #correct_prediction_condition = max_margin_mdl.logical_and([activations_l[k][j] == y_train.iloc[k][j] for j in range(N[l])]) \n",
    "                #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "max_margin_mdl.add(max_margin_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for minimum training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_margin_obj = max_margin_mdl.integer_var(0, 1000, name='max_margin_objective')\n",
    "margins_neurons = []\n",
    "sum_margins = 0\n",
    "for l in range(1,L+1):\n",
    "    margins_l = [] #list containing margins of all neurons in layer l\n",
    "    sum_margins_l = 0 #sum of margins of layer l\n",
    "    for j in range(N[l]):\n",
    "        activations_j = [] #list containing activations of neuron j in layer l\n",
    "        margin_j = 0 #margin value of neuron j\n",
    "        for k in range(x_train.shape[0]):\n",
    "            elem = max_margin_mdl.scal_prod(weights[l-1][j], activations[l-1][k])\n",
    "            activations_j.append(max_margin_mdl.abs(elem))\n",
    "        margin_j = max_margin_mdl.min(activations_j) #definition of margin\n",
    "        margins_l.append(margin_j)\n",
    "    sum_margins_l += max_margin_mdl.sum(margins_l)\n",
    "    sum_margins += sum_margins_l\n",
    "\n",
    "max_margin_mdl.add(max_margin_obj == sum_margins)\n",
    "max_margin_mdl.add(max_margin_mdl.maximize(max_margin_obj))\n",
    "\n",
    "#breaking the symmetry for weights incoming to the first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_margin_mdl.add(max_margin_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "\n",
    "# Start interactive search\n",
    "max_margin_solutions = max_margin_mdl.start_search(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Activations for Dataset x\n",
    "def compute_activations(weights_solution, x):\n",
    "    activations = [x.values]  # Use the dataset features as the initial activations\n",
    "\n",
    "    for layer_idx in range(1, len(N)):\n",
    "        prev_layer_activations = activations[-1]\n",
    "        current_layer_weights = weights_solution[layer_idx - 1]\n",
    "        \n",
    "        current_layer_activations = []\n",
    "        for sample_activations in prev_layer_activations:\n",
    "            layer_activations = []\n",
    "            for neuron_weights in current_layer_weights:\n",
    "                activation = sum(weight * sample_activation for weight, sample_activation in zip(neuron_weights, sample_activations))\n",
    "                # Apply sign function\n",
    "                if activation >= 0:\n",
    "                    layer_activations.append(1)\n",
    "                else:\n",
    "                    layer_activations.append(-1)\n",
    "            current_layer_activations.append(layer_activations)\n",
    "        \n",
    "        activations.append(current_layer_activations)\n",
    "    \n",
    "    return activations[-1]  # Return the activations of the output layer\n",
    "\n",
    "# Compare Predictions with True Labels and Compute Accuracy\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct_predictions = sum(pred == true for pred, true in zip(predictions, true_labels))\n",
    "    corr_class = predictions==true_labels\n",
    "    incorr_eg_idx = np.where(corr_class==False)[0].tolist()\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy, incorr_eg_idx #return the accuracy and indices of misclassified examples\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 32 , No. of solutions found: 1 Is solution optimal: False , Optimality gap: (30.25,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 33 , No. of solutions found: 2 Is solution optimal: False , Optimality gap: (29.303,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 35 , No. of solutions found: 3 Is solution optimal: False , Optimality gap: (27.5714,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 37 , No. of solutions found: 4 Is solution optimal: False , Optimality gap: (26.027,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 38 , No. of solutions found: 5 Is solution optimal: False , Optimality gap: (25.3158,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 39 , No. of solutions found: 6 Is solution optimal: False , Optimality gap: (24.641,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 40 , No. of solutions found: 7 Is solution optimal: False , Optimality gap: (24,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 41 , No. of solutions found: 8 Is solution optimal: False , Optimality gap: (23.3902,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 42 , No. of solutions found: 9 Is solution optimal: False , Optimality gap: (22.8095,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 43 , No. of solutions found: 10 Is solution optimal: False , Optimality gap: (22.2558,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 44 , No. of solutions found: 11 Is solution optimal: False , Optimality gap: (21.7273,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 46 , No. of solutions found: 12 Is solution optimal: False , Optimality gap: (20.7391,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 48 , No. of solutions found: 13 Is solution optimal: False , Optimality gap: (19.8333,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 50 , No. of solutions found: 14 Is solution optimal: False , Optimality gap: (19,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 52 , No. of solutions found: 15 Is solution optimal: False , Optimality gap: (18.2308,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 54 , No. of solutions found: 16 Is solution optimal: False , Optimality gap: (17.5185,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 56 , No. of solutions found: 17 Is solution optimal: False , Optimality gap: (16.8571,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 57 , No. of solutions found: 18 Is solution optimal: False , Optimality gap: (16.5439,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 58 , No. of solutions found: 19 Is solution optimal: False , Optimality gap: (16.2414,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 59 , No. of solutions found: 20 Is solution optimal: False , Optimality gap: (15.9492,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 60 , No. of solutions found: 21 Is solution optimal: False , Optimality gap: (15.6667,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 61 , No. of solutions found: 22 Is solution optimal: False , Optimality gap: (15.3934,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 62 , No. of solutions found: 23 Is solution optimal: False , Optimality gap: (15.129,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 63 , No. of solutions found: 24 Is solution optimal: False , Optimality gap: (14.873,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 64 , No. of solutions found: 25 Is solution optimal: False , Optimality gap: (14.625,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 80 , No. of solutions found: 26 Is solution optimal: False , Optimality gap: (11.5,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 85 , No. of solutions found: 27 Is solution optimal: False , Optimality gap: (10.7647,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 86 , No. of solutions found: 28 Is solution optimal: False , Optimality gap: (10.6279,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 87 , No. of solutions found: 29 Is solution optimal: False , Optimality gap: (10.4943,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 89 , No. of solutions found: 30 Is solution optimal: False , Optimality gap: (10.236,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 90 , No. of solutions found: 31 Is solution optimal: False , Optimality gap: (10.1111,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 436 , No. of solutions found: 32 Is solution optimal: False , Optimality gap: (1.29358,) , Test accuracy: 0.5971428571428572\n",
      "Objective value: 464 , No. of solutions found: 33 Is solution optimal: False , Optimality gap: (1.15517,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 465 , No. of solutions found: 34 Is solution optimal: False , Optimality gap: (1.15054,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 466 , No. of solutions found: 35 Is solution optimal: False , Optimality gap: (1.14592,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 467 , No. of solutions found: 36 Is solution optimal: False , Optimality gap: (1.14133,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 469 , No. of solutions found: 37 Is solution optimal: False , Optimality gap: (1.1322,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 470 , No. of solutions found: 38 Is solution optimal: False , Optimality gap: (1.12766,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 471 , No. of solutions found: 39 Is solution optimal: False , Optimality gap: (1.12314,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 472 , No. of solutions found: 40 Is solution optimal: False , Optimality gap: (1.11864,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 473 , No. of solutions found: 41 Is solution optimal: False , Optimality gap: (1.11416,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 474 , No. of solutions found: 42 Is solution optimal: False , Optimality gap: (1.1097,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 475 , No. of solutions found: 43 Is solution optimal: False , Optimality gap: (1.10526,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 476 , No. of solutions found: 44 Is solution optimal: False , Optimality gap: (1.10084,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 477 , No. of solutions found: 45 Is solution optimal: False , Optimality gap: (1.09644,) , Test accuracy: 0.6228571428571429\n",
      "Objective value: 478 , No. of solutions found: 46 Is solution optimal: False , Optimality gap: (1.09205,) , Test accuracy: 0.6228571428571429\n",
      "Objective value: 479 , No. of solutions found: 47 Is solution optimal: False , Optimality gap: (1.08768,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 480 , No. of solutions found: 48 Is solution optimal: False , Optimality gap: (1.08333,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 501 , No. of solutions found: 49 Is solution optimal: False , Optimality gap: (0.996008,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 502 , No. of solutions found: 50 Is solution optimal: False , Optimality gap: (0.992032,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 503 , No. of solutions found: 51 Is solution optimal: False , Optimality gap: (0.988072,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 504 , No. of solutions found: 52 Is solution optimal: False , Optimality gap: (0.984127,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 505 , No. of solutions found: 53 Is solution optimal: False , Optimality gap: (0.980198,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 506 , No. of solutions found: 54 Is solution optimal: False , Optimality gap: (0.976285,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 507 , No. of solutions found: 55 Is solution optimal: False , Optimality gap: (0.972387,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 508 , No. of solutions found: 56 Is solution optimal: False , Optimality gap: (0.968504,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 509 , No. of solutions found: 57 Is solution optimal: False , Optimality gap: (0.964637,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 510 , No. of solutions found: 58 Is solution optimal: False , Optimality gap: (0.960784,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 515 , No. of solutions found: 59 Is solution optimal: False , Optimality gap: (0.941748,) , Test accuracy: 0.6257142857142857\n",
      "Objective value: 516 , No. of solutions found: 60 Is solution optimal: False , Optimality gap: (0.937984,) , Test accuracy: 0.6257142857142857\n",
      "No more solutions available.\n",
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "test_acc = []\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_margin_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and test accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_margin_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_margin_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "#Compute exact training accuracy of last solution obtained\n",
    "max_margin_solution = max_margin_solutions.get_last_solution()\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=max_margin_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654.2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_margin_solution.get_solver_infos()['TotalTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the solver is able to find multiple feasible solutions but is unable to optimize them within the time limit. So, in order to judge the quality of the intermediate solutions, we observe the plots of objective value, test accuracy and runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots of Performance of Max Margin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append the last point to extend up to runtime 1800\n",
    "extended_runtime = 1800\n",
    "runtimes.append(extended_runtime)\n",
    "objective_vals.append(objective_vals[-1])\n",
    "test_acc.append(test_acc[-1])\n",
    "\n",
    "# Increase the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: Runtime vs Objective Values\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(runtimes, objective_vals, color='green', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Runtime (seconds)\")\n",
    "plt.ylabel(\"Objective Value (Sum of Margins)\")\n",
    "plt.title(\"Runtime vs Objective Value\")\n",
    "#plt.xlim(0, 1800)\n",
    "\n",
    "# Subplot 2: Runtime vs Test Accuracies\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(runtimes, test_acc, color='blue', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Runtime (seconds)\")\n",
    "plt.ylabel(\"Test Accuracies\")\n",
    "plt.title(\"Runtime vs Test Accuracies\")\n",
    "plt.ylim(0.3, 1.0)\n",
    "#plt.xlim(0, 1800)\n",
    "\n",
    "# Subplot 3: Objective Values vs Test Accuracies\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(objective_vals, test_acc, color='red', marker='o', linewidth=2, markersize=5)\n",
    "plt.xlabel(\"Objective Values\")\n",
    "plt.ylabel(\"Test Accuracies\")\n",
    "plt.title(\"Objective Values vs Test Accuracies\")\n",
    "plt.ylim(0.3, 1.0)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "#N=[x_train.shape[1],4,10]\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L))\n",
    "max_obj_val=0\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1])\n",
    "max_obj_val\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months')\n",
    "epsilon_amt = 100\n",
    "epsilon_dur = 2\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model MW')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j])\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "pert_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            min_weight_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            min_weight_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "activations_1 = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "\n",
    "activations_1_up = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                min_weight_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                min_weight_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            min_weight_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i]))\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for atleast 75% of training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "\n",
    "min_weight_obj = min_weight_mdl.integer_var(0, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solutions = min_weight_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "test_acc = []\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = min_weight_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = min_weight_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "min_weight_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.78688524590164\n"
     ]
    }
   ],
   "source": [
    "min_weight_solution = min_weight_solutions.get_last_solution()\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=min_weight_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_weight_solution.get_solver_infos()['TotalTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Search Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "#N=[x_train.shape[1],4,10]\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L))\n",
    "max_obj_val=0\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1])\n",
    "max_obj_val\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months')\n",
    "epsilon_amt = 100\n",
    "epsilon_dur = 2\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model MW')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j])\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "pert_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            min_weight_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            min_weight_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "activations_1 = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "\n",
    "activations_1_up = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                min_weight_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                min_weight_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            min_weight_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i]))\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for atleast 75% of training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "# Flatten the nested lists of activation and weight variables\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "activations_flat = [flatten(layer) for layer in activations]\n",
    "weights_flat = [flatten(layer) for layer in weights]\n",
    "\n",
    "\n",
    "# Define the search phase 1\n",
    "search_phases = [search_phase(corr_pred)]\n",
    "for l in range(L, 0, -1):\n",
    "    search_phases.append(search_phase(activations_flat[l]))\n",
    "    search_phases.append(search_phase(weights_flat[l-1]))\n",
    "\n",
    "\n",
    "# Define the search phase 2\n",
    "#search_phases = [search_phase(corr_pred),search_phase(activations_flat[L])]\n",
    "#weights_act=[]\n",
    "#for l in range(L, 0, -1):\n",
    "    #weights_act = weights_flat[l-1] + activations_flat[l-1]\n",
    "    #search_phases.append(search_phase(weights_act))\n",
    "\n",
    "\n",
    "# Add search phases to the model\n",
    "min_weight_mdl.set_search_phases(search_phases)\n",
    "\n",
    "#define the objective\n",
    "min_weight_obj = min_weight_mdl.integer_var(10, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solutions = min_weight_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "test_acc = []\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = min_weight_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = min_weight_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "min_weight_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m----> 4\u001b[0m     train_acc\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmin_weight_solution\u001b[38;5;241m.\u001b[39mget_value(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorr_pred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m/\u001b[39mx_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "min_weight_solution = min_weight_solutions.get_last_solution()\n",
    "train_acc=0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=min_weight_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "#N=[x_train.shape[1],4,10]\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L))\n",
    "max_obj_val=0\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1])\n",
    "max_obj_val\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months')\n",
    "epsilon_amt = 100\n",
    "epsilon_dur = 2\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model MW')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j])\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "pert_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            min_weight_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            min_weight_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "activations_1 = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "\n",
    "activations_1_up = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                min_weight_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                min_weight_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            min_weight_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i]))\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for atleast 75% of training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "# Flatten the nested lists of activation and weight variables\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "activations_flat = [flatten(layer) for layer in activations]\n",
    "weights_flat = [flatten(layer) for layer in weights]\n",
    "\n",
    "\n",
    "# Define the search phase 1\n",
    "#search_phases = [search_phase(corr_pred)]\n",
    "#for l in range(L, 0, -1):\n",
    "    #search_phases.append(search_phase(activations_flat[l]))\n",
    "    #search_phases.append(search_phase(weights_flat[l-1]))\n",
    "\n",
    "\n",
    "# Define the search phase 2\n",
    "search_phases = [search_phase(corr_pred),search_phase(activations_flat[L])]\n",
    "weights_act=[]\n",
    "for l in range(L, 0, -1):\n",
    "    weights_act = weights_flat[l-1] + activations_flat[l-1]\n",
    "    search_phases.append(search_phase(weights_act))\n",
    "\n",
    "\n",
    "# Add search phases to the model\n",
    "min_weight_mdl.set_search_phases(search_phases)\n",
    "\n",
    "#define the objective\n",
    "min_weight_obj = min_weight_mdl.integer_var(10, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solutions = min_weight_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "test_acc = []\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = min_weight_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = min_weight_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "min_weight_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lower Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "#N=[x_train.shape[1],4,10]\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L))\n",
    "max_obj_val=0\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1])\n",
    "max_obj_val\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount')\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months')\n",
    "epsilon_amt = 100\n",
    "epsilon_dur = 2\n",
    "x_max = x_train.max()[idx_cred_amt]\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model MW')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j])\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "pert_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            min_weight_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            min_weight_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "activations_1 = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "\n",
    "activations_1_up = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                min_weight_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                min_weight_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            min_weight_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i]))\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.7*x_train.shape[0]) # constraint for atleast 75% of training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "\n",
    "#define the objective\n",
    "min_weight_obj = min_weight_mdl.integer_var(0, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solutions = min_weight_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 38 , No. of solutions found: 1 Is solution optimal: False , Optimality gap: (0.973684,) , Test accuracy: 0.6042857142857143\n",
      "Objective value: 19 , No. of solutions found: 2 Is solution optimal: False , Optimality gap: (0.947368,) , Test accuracy: 0.5857142857142857\n",
      "Objective value: 14 , No. of solutions found: 3 Is solution optimal: False , Optimality gap: (0.928571,) , Test accuracy: 0.59\n",
      "Objective value: 13 , No. of solutions found: 4 Is solution optimal: False , Optimality gap: (0.923077,) , Test accuracy: 0.59\n",
      "Objective value: 12 , No. of solutions found: 5 Is solution optimal: False , Optimality gap: (0.916667,) , Test accuracy: 0.59\n",
      "Objective value: 5 , No. of solutions found: 6 Is solution optimal: False , Optimality gap: (0.8,) , Test accuracy: 0.6171428571428571\n",
      "Objective value: 4 , No. of solutions found: 7 Is solution optimal: False , Optimality gap: (0.75,) , Test accuracy: 0.6171428571428571\n",
      "Objective value: 3 , No. of solutions found: 8 Is solution optimal: False , Optimality gap: (0.666667,) , Test accuracy: 0.6171428571428571\n",
      "Objective value: 2 , No. of solutions found: 9 Is solution optimal: False , Optimality gap: (0.5,) , Test accuracy: 0.6057142857142858\n",
      "No more solutions available.\n",
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "test_acc = []\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = min_weight_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test)\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = min_weight_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "min_weight_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Accuracy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "\n",
    "max_acc_mdl = CpoModel(name='German Credit CP Model MA')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_acc_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_acc_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of inout layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#lists containing perturbed examples x^j_up and x^j_down, for each neuron j of layer 1, for all training examples x\n",
    "pert_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            max_acc_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            max_acc_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first hidden layer activations for training examples \n",
    "activations_1 = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations for perturbed examples x^j_up and x^j_down\n",
    "activations_1_up = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features remain unchanged\n",
    "                max_acc_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                max_acc_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definition of first layer activations\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            max_acc_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for hidden layers\n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights of output layer\n",
    "activations_L = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "corr_pred = [max_acc_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "#max_acc_mdl.add(max_acc_mdl.sum(corr_pred)<x_train.shape[0])\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_acc_mdl.add(maximize(max_acc_mdl.sum(corr_pred)*100/x_train.shape[0]))\n",
    "            \n",
    "#breaking the symmetry for incoming weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_acc_mdl.add(max_acc_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_acc_solutions = max_acc_mdl.start_search(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 50 , No. of solutions found: 1 Is solution optimal: False , Optimality gap: (1,) , Test accuracy: 0.6757142857142857\n",
      "Objective value: 50.4032 , No. of solutions found: 2 Is solution optimal: False , Optimality gap: (0.984,) , Test accuracy: 0.6757142857142857\n",
      "Objective value: 61.2903 , No. of solutions found: 3 Is solution optimal: False , Optimality gap: (0.631579,) , Test accuracy: 0.65\n",
      "Objective value: 61.6935 , No. of solutions found: 4 Is solution optimal: False , Optimality gap: (0.620915,) , Test accuracy: 0.43857142857142856\n",
      "Objective value: 62.0968 , No. of solutions found: 5 Is solution optimal: False , Optimality gap: (0.61039,) , Test accuracy: 0.44\n",
      "Objective value: 62.5 , No. of solutions found: 6 Is solution optimal: False , Optimality gap: (0.6,) , Test accuracy: 0.44\n",
      "Objective value: 62.9032 , No. of solutions found: 7 Is solution optimal: False , Optimality gap: (0.589744,) , Test accuracy: 0.4471428571428571\n",
      "Objective value: 63.3065 , No. of solutions found: 8 Is solution optimal: False , Optimality gap: (0.579618,) , Test accuracy: 0.44285714285714284\n",
      "Objective value: 63.7097 , No. of solutions found: 9 Is solution optimal: False , Optimality gap: (0.56962,) , Test accuracy: 0.4442857142857143\n",
      "Objective value: 64.1129 , No. of solutions found: 10 Is solution optimal: False , Optimality gap: (0.559748,) , Test accuracy: 0.45\n",
      "Objective value: 64.5161 , No. of solutions found: 11 Is solution optimal: False , Optimality gap: (0.55,) , Test accuracy: 0.4514285714285714\n",
      "Objective value: 64.9194 , No. of solutions found: 12 Is solution optimal: False , Optimality gap: (0.540373,) , Test accuracy: 0.4514285714285714\n",
      "Objective value: 65.7258 , No. of solutions found: 13 Is solution optimal: False , Optimality gap: (0.521472,) , Test accuracy: 0.4514285714285714\n",
      "Objective value: 66.129 , No. of solutions found: 14 Is solution optimal: False , Optimality gap: (0.512195,) , Test accuracy: 0.45285714285714285\n",
      "Objective value: 66.5323 , No. of solutions found: 15 Is solution optimal: False , Optimality gap: (0.50303,) , Test accuracy: 0.45571428571428574\n",
      "Objective value: 66.9355 , No. of solutions found: 16 Is solution optimal: False , Optimality gap: (0.493976,) , Test accuracy: 0.44571428571428573\n",
      "Objective value: 67.3387 , No. of solutions found: 17 Is solution optimal: False , Optimality gap: (0.48503,) , Test accuracy: 0.45285714285714285\n",
      "Objective value: 67.7419 , No. of solutions found: 18 Is solution optimal: False , Optimality gap: (0.47619,) , Test accuracy: 0.4542857142857143\n",
      "Objective value: 68.1452 , No. of solutions found: 19 Is solution optimal: False , Optimality gap: (0.467456,) , Test accuracy: 0.4471428571428571\n",
      "Objective value: 68.5484 , No. of solutions found: 20 Is solution optimal: False , Optimality gap: (0.458824,) , Test accuracy: 0.45714285714285713\n",
      "Objective value: 68.9516 , No. of solutions found: 21 Is solution optimal: False , Optimality gap: (0.450292,) , Test accuracy: 0.45714285714285713\n",
      "Objective value: 69.3548 , No. of solutions found: 22 Is solution optimal: False , Optimality gap: (0.44186,) , Test accuracy: 0.45714285714285713\n",
      "Objective value: 69.7581 , No. of solutions found: 23 Is solution optimal: False , Optimality gap: (0.433526,) , Test accuracy: 0.4685714285714286\n",
      "Objective value: 70.1613 , No. of solutions found: 24 Is solution optimal: False , Optimality gap: (0.425287,) , Test accuracy: 0.47\n",
      "Objective value: 70.5645 , No. of solutions found: 25 Is solution optimal: False , Optimality gap: (0.417143,) , Test accuracy: 0.46714285714285714\n",
      "Objective value: 70.9677 , No. of solutions found: 26 Is solution optimal: False , Optimality gap: (0.409091,) , Test accuracy: 0.46285714285714286\n",
      "Objective value: 71.371 , No. of solutions found: 27 Is solution optimal: False , Optimality gap: (0.40113,) , Test accuracy: 0.4642857142857143\n",
      "Objective value: 71.7742 , No. of solutions found: 28 Is solution optimal: False , Optimality gap: (0.393258,) , Test accuracy: 0.4642857142857143\n",
      "Objective value: 72.1774 , No. of solutions found: 29 Is solution optimal: False , Optimality gap: (0.385475,) , Test accuracy: 0.46714285714285714\n",
      "Objective value: 72.5806 , No. of solutions found: 30 Is solution optimal: False , Optimality gap: (0.377778,) , Test accuracy: 0.4685714285714286\n",
      "Objective value: 72.9839 , No. of solutions found: 31 Is solution optimal: False , Optimality gap: (0.370166,) , Test accuracy: 0.4785714285714286\n",
      "Objective value: 73.3871 , No. of solutions found: 32 Is solution optimal: False , Optimality gap: (0.362637,) , Test accuracy: 0.4828571428571429\n",
      "Objective value: 73.7903 , No. of solutions found: 33 Is solution optimal: False , Optimality gap: (0.355191,) , Test accuracy: 0.4742857142857143\n",
      "Objective value: 74.1935 , No. of solutions found: 34 Is solution optimal: False , Optimality gap: (0.347826,) , Test accuracy: 0.47714285714285715\n",
      "Objective value: 74.5968 , No. of solutions found: 35 Is solution optimal: False , Optimality gap: (0.340541,) , Test accuracy: 0.48428571428571426\n",
      "Objective value: 75 , No. of solutions found: 36 Is solution optimal: False , Optimality gap: (0.333333,) , Test accuracy: 0.48428571428571426\n",
      "Objective value: 75.4032 , No. of solutions found: 37 Is solution optimal: False , Optimality gap: (0.326203,) , Test accuracy: 0.4757142857142857\n",
      "Objective value: 75.8065 , No. of solutions found: 38 Is solution optimal: False , Optimality gap: (0.319149,) , Test accuracy: 0.48\n",
      "Objective value: 76.2097 , No. of solutions found: 39 Is solution optimal: False , Optimality gap: (0.312169,) , Test accuracy: 0.5057142857142857\n",
      "Objective value: 76.6129 , No. of solutions found: 40 Is solution optimal: False , Optimality gap: (0.305263,) , Test accuracy: 0.5128571428571429\n",
      "Objective value: 77.0161 , No. of solutions found: 41 Is solution optimal: False , Optimality gap: (0.298429,) , Test accuracy: 0.47714285714285715\n",
      "Objective value: 77.4194 , No. of solutions found: 42 Is solution optimal: False , Optimality gap: (0.291667,) , Test accuracy: 0.5142857142857142\n",
      "Objective value: 78.2258 , No. of solutions found: 43 Is solution optimal: False , Optimality gap: (0.278351,) , Test accuracy: 0.5142857142857142\n",
      "Objective value: 78.629 , No. of solutions found: 44 Is solution optimal: False , Optimality gap: (0.271795,) , Test accuracy: 0.5157142857142857\n",
      "Objective value: 79.0323 , No. of solutions found: 45 Is solution optimal: False , Optimality gap: (0.265306,) , Test accuracy: 0.5271428571428571\n",
      "Objective value: 79.4355 , No. of solutions found: 46 Is solution optimal: False , Optimality gap: (0.258883,) , Test accuracy: 0.5285714285714286\n",
      "Objective value: 79.8387 , No. of solutions found: 47 Is solution optimal: False , Optimality gap: (0.252525,) , Test accuracy: 0.5285714285714286\n",
      "Objective value: 80.2419 , No. of solutions found: 48 Is solution optimal: False , Optimality gap: (0.246231,) , Test accuracy: 0.5285714285714286\n",
      "Objective value: 80.6452 , No. of solutions found: 49 Is solution optimal: False , Optimality gap: (0.24,) , Test accuracy: 0.5328571428571428\n",
      "Objective value: 81.0484 , No. of solutions found: 50 Is solution optimal: False , Optimality gap: (0.233831,) , Test accuracy: 0.5357142857142857\n",
      "Objective value: 81.4516 , No. of solutions found: 51 Is solution optimal: False , Optimality gap: (0.227723,) , Test accuracy: 0.5285714285714286\n",
      "Objective value: 81.8548 , No. of solutions found: 52 Is solution optimal: False , Optimality gap: (0.221675,) , Test accuracy: 0.5228571428571429\n",
      "Objective value: 82.2581 , No. of solutions found: 53 Is solution optimal: False , Optimality gap: (0.215686,) , Test accuracy: 0.52\n",
      "Objective value: 82.6613 , No. of solutions found: 54 Is solution optimal: False , Optimality gap: (0.209756,) , Test accuracy: 0.5228571428571429\n",
      "Objective value: 83.0645 , No. of solutions found: 55 Is solution optimal: False , Optimality gap: (0.203883,) , Test accuracy: 0.5185714285714286\n",
      "Objective value: 83.4677 , No. of solutions found: 56 Is solution optimal: False , Optimality gap: (0.198068,) , Test accuracy: 0.5185714285714286\n",
      "Objective value: 83.871 , No. of solutions found: 57 Is solution optimal: False , Optimality gap: (0.192308,) , Test accuracy: 0.5185714285714286\n",
      "Objective value: 84.2742 , No. of solutions found: 58 Is solution optimal: False , Optimality gap: (0.186603,) , Test accuracy: 0.5157142857142857\n",
      "Objective value: 84.6774 , No. of solutions found: 59 Is solution optimal: False , Optimality gap: (0.180952,) , Test accuracy: 0.5142857142857142\n",
      "Objective value: 85.0806 , No. of solutions found: 60 Is solution optimal: False , Optimality gap: (0.175355,) , Test accuracy: 0.5342857142857143\n",
      "Objective value: 85.8871 , No. of solutions found: 61 Is solution optimal: False , Optimality gap: (0.164319,) , Test accuracy: 0.5385714285714286\n",
      "Objective value: 86.2903 , No. of solutions found: 62 Is solution optimal: False , Optimality gap: (0.158879,) , Test accuracy: 0.5371428571428571\n",
      "Objective value: 86.6935 , No. of solutions found: 63 Is solution optimal: False , Optimality gap: (0.153488,) , Test accuracy: 0.5485714285714286\n",
      "Objective value: 87.0968 , No. of solutions found: 64 Is solution optimal: False , Optimality gap: (0.148148,) , Test accuracy: 0.5585714285714286\n",
      "Objective value: 87.5 , No. of solutions found: 65 Is solution optimal: False , Optimality gap: (0.142857,) , Test accuracy: 0.5671428571428572\n",
      "Objective value: 87.9032 , No. of solutions found: 66 Is solution optimal: False , Optimality gap: (0.137615,) , Test accuracy: 0.5771428571428572\n",
      "Objective value: 88.3065 , No. of solutions found: 67 Is solution optimal: False , Optimality gap: (0.13242,) , Test accuracy: 0.5657142857142857\n",
      "Objective value: 88.7097 , No. of solutions found: 68 Is solution optimal: False , Optimality gap: (0.127273,) , Test accuracy: 0.5671428571428572\n",
      "Objective value: 89.1129 , No. of solutions found: 69 Is solution optimal: False , Optimality gap: (0.122172,) , Test accuracy: 0.5757142857142857\n",
      "Objective value: 89.5161 , No. of solutions found: 70 Is solution optimal: False , Optimality gap: (0.117117,) , Test accuracy: 0.5771428571428572\n",
      "Objective value: 89.9194 , No. of solutions found: 71 Is solution optimal: False , Optimality gap: (0.112108,) , Test accuracy: 0.5628571428571428\n",
      "Objective value: 90.7258 , No. of solutions found: 72 Is solution optimal: False , Optimality gap: (0.102222,) , Test accuracy: 0.5685714285714286\n",
      "Objective value: 91.129 , No. of solutions found: 73 Is solution optimal: False , Optimality gap: (0.0973451,) , Test accuracy: 0.56\n",
      "Objective value: 91.9355 , No. of solutions found: 74 Is solution optimal: False , Optimality gap: (0.0877193,) , Test accuracy: 0.5671428571428572\n",
      "Objective value: 92.7419 , No. of solutions found: 75 Is solution optimal: False , Optimality gap: (0.0782609,) , Test accuracy: 0.58\n",
      "Objective value: 93.5484 , No. of solutions found: 76 Is solution optimal: False , Optimality gap: (0.0689655,) , Test accuracy: 0.5728571428571428\n",
      "Objective value: 93.9516 , No. of solutions found: 77 Is solution optimal: False , Optimality gap: (0.0643777,) , Test accuracy: 0.5628571428571428\n",
      "Objective value: 94.3548 , No. of solutions found: 78 Is solution optimal: False , Optimality gap: (0.0598291,) , Test accuracy: 0.6142857142857143\n",
      "Objective value: 94.7581 , No. of solutions found: 79 Is solution optimal: False , Optimality gap: (0.0553191,) , Test accuracy: 0.6128571428571429\n",
      "Objective value: 95.1613 , No. of solutions found: 80 Is solution optimal: False , Optimality gap: (0.0508475,) , Test accuracy: 0.6157142857142858\n",
      "Objective value: 95.5645 , No. of solutions found: 81 Is solution optimal: False , Optimality gap: (0.0464135,) , Test accuracy: 0.5885714285714285\n",
      "Objective value: 95.9677 , No. of solutions found: 82 Is solution optimal: False , Optimality gap: (0.0420168,) , Test accuracy: 0.6014285714285714\n",
      "Objective value: 96.371 , No. of solutions found: 83 Is solution optimal: False , Optimality gap: (0.0376569,) , Test accuracy: 0.6185714285714285\n",
      "Objective value: 96.7742 , No. of solutions found: 84 Is solution optimal: False , Optimality gap: (0.0333333,) , Test accuracy: 0.6157142857142858\n",
      "Objective value: 97.1774 , No. of solutions found: 85 Is solution optimal: False , Optimality gap: (0.0290456,) , Test accuracy: 0.6014285714285714\n",
      "Objective value: 97.5806 , No. of solutions found: 86 Is solution optimal: False , Optimality gap: (0.0247934,) , Test accuracy: 0.6385714285714286\n",
      "Objective value: 97.9839 , No. of solutions found: 87 Is solution optimal: False , Optimality gap: (0.0205761,) , Test accuracy: 0.6442857142857142\n",
      "Objective value: 98.3871 , No. of solutions found: 88 Is solution optimal: False , Optimality gap: (0.0163934,) , Test accuracy: 0.6457142857142857\n",
      "Objective value: 98.7903 , No. of solutions found: 89 Is solution optimal: False , Optimality gap: (0.0122449,) , Test accuracy: 0.6414285714285715\n",
      "Objective value: 99.1935 , No. of solutions found: 90 Is solution optimal: False , Optimality gap: (0.00813008,) , Test accuracy: 0.6442857142857142\n",
      "Objective value: 99.5968 , No. of solutions found: 91 Is solution optimal: False , Optimality gap: (0.00404858,) , Test accuracy: 0.6285714285714286\n",
      "Objective value: 100 , No. of solutions found: 92 Is solution optimal: False , Optimality gap: (2.2311e-14,) , Test accuracy: 0.65\n",
      "No more solutions available.\n",
      "Search ended successfully.\n"
     ]
    }
   ],
   "source": [
    "# Print intermediate solutions\n",
    "objective_vals = [] #list to store objective values of all solutions\n",
    "runtimes = [] #list to store runtimes of all solutions\n",
    "numsols = []\n",
    "test_acc = [] #list to store test accuracies of all solutions\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_acc_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and test accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_test) #compute output activations (predictions) of test set\n",
    "    test_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values) #compute test accuracy\n",
    "    \n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \"Is solution optimal:\", solution.is_solution_optimal(), \", Optimality gap:\", solution.get_objective_gaps(), \", Test accuracy:\", test_accuracy)\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_acc_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_acc_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.65\n",
      "245\n"
     ]
    }
   ],
   "source": [
    "#Print test accuracy and number of misclassified test examples of last solution obtained \n",
    "\n",
    "max_acc_solution = max_acc_solutions.get_last_result()\n",
    "# Step 1: Extract the Weights from the Solution\n",
    "weights_solution = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_solution[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        weights_solution[layer_idx].append([max_acc_solution[weight_var] for weight_var in neuron_weights])\n",
    "\n",
    "# Compute the activations for the test set\n",
    "output_activations = compute_activations(weights_solution, x_test)\n",
    "\n",
    "# Assume y_test is your true labels for the test set\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "test_accuracy,incorrect_egs = compute_accuracy(flatten(output_activations), y_test.values)\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print(len(incorrect_egs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791.67"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc_solution.get_solver_infos()['TotalTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Robust Models Using the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the plots of the test accuracies vs the optimal values of the different models, we make the following observations:\n",
    "1) The solver often found feasible solutions very fast (within a few seconds) but struggled to optimize or prove optimality of a solution within the time limit. Intermediate solutions, however, often perform reasonably well on the test set.\n",
    "2) The optimal solutions may not always generalize the best, i.e the test accuracies of the optimal or final solutions may not always be the highest.\n",
    "\n",
    "\n",
    "Moreover, the solutions for all models generalize similar to or slightly poorer than the keras-trained BNN even with extremely small training datasets and absolutely no use of the validation set. On the other hand, the good generalization of the Keras-trained BNN can be attributed to the large validation set used, as observed at the beginning of this notebook. So, in order to improve the generalization of our CPO-trained BNNs, we will make use of the validation set in the following two ways and see if the performance improves.\n",
    "1) The validation accuracies of the intermediate solutions are computed and the solution having the best validation accuracy is finally adopted ans is expected to generalize the best (i.e have the highest test accuracy too).\n",
    "2) For CP $_{a}$: From the validation set, a few examples that are misclassified by the CPO model are randomly sampled (an equal number from both classes), and more constraints are then added to the model to ensure correct classification of these examples. This process is repeated a few times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 1: SELECTING THE SOLUTION WITH BEST VALIDATION ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Margin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "max_margin_mdl = CpoModel(name='German Credit CP Model 2')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_margin_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_margin_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#define the lists containing x_up and x_down for each neuron j in layer 1, for all training examples x\n",
    "pert_up = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "\n",
    "#define the lists containing input layer activations for each example of pert_up and pert_down defined above\n",
    "activations_0_up = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[max_margin_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            #definition of input layer activations\n",
    "            max_margin_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i]) \n",
    "            max_margin_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first hidden layer activations for training examples\n",
    "activations_1 = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations for each example of pert_up and pert_down defined above\n",
    "activations_1_up = [[[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down \n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down \n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_margin_mdl.add(max_margin_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features do not change in the perturbed examples \n",
    "                max_margin_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                max_margin_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definitions of layer 1 activations\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            max_margin_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of example x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for hidden layers and output layer\n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "\n",
    "activations_L = [[max_margin_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_margin_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [max_margin_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #max_margin_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #max_margin_mdl.add(max_margin_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "                #correct_prediction_condition = max_margin_mdl.logical_and([activations_l[k][j] == y_train.iloc[k][j] for j in range(N[l])]) \n",
    "                #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_margin_mdl.add(max_margin_mdl.if_then(max_margin_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "max_margin_mdl.add(max_margin_mdl.sum(corr_pred)>0.8*x_train.shape[0]) # constraint for minimum training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_margin_obj = max_margin_mdl.integer_var(0, 1000, name='max_margin_objective')\n",
    "margins_neurons = []\n",
    "sum_margins = 0\n",
    "for l in range(1,L+1):\n",
    "    margins_l = [] #list containing margins of all neurons in layer l\n",
    "    sum_margins_l = 0 #sum of margins of layer l\n",
    "    for j in range(N[l]):\n",
    "        activations_j = [] #list containing activations of neuron j in layer l\n",
    "        margin_j = 0 #margin value of neuron j\n",
    "        for k in range(x_train.shape[0]):\n",
    "            elem = max_margin_mdl.scal_prod(weights[l-1][j], activations[l-1][k])\n",
    "            activations_j.append(max_margin_mdl.abs(elem))\n",
    "        margin_j = max_margin_mdl.min(activations_j) #definition of margin\n",
    "        margins_l.append(margin_j)\n",
    "    sum_margins_l += max_margin_mdl.sum(margins_l)\n",
    "    sum_margins += sum_margins_l\n",
    "\n",
    "max_margin_mdl.add(max_margin_obj == sum_margins)\n",
    "max_margin_mdl.add(max_margin_mdl.maximize(max_margin_obj))\n",
    "\n",
    "#breaking the symmetry for weights incoming to the first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_margin_mdl.add(max_margin_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "\n",
    "# Start interactive search\n",
    "max_margin_solutions = max_margin_mdl.start_search(SearchType='Restart', TimeLimit=3600, LogVerbosity='Quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 32 , No. of solutions found: 1 , Optimality gap: (30.25,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 33 , No. of solutions found: 2 , Optimality gap: (29.303,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 35 , No. of solutions found: 3 , Optimality gap: (27.5714,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 37 , No. of solutions found: 4 , Optimality gap: (26.027,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 38 , No. of solutions found: 5 , Optimality gap: (25.3158,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 39 , No. of solutions found: 6 , Optimality gap: (24.641,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 40 , No. of solutions found: 7 , Optimality gap: (24,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 41 , No. of solutions found: 8 , Optimality gap: (23.3902,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 42 , No. of solutions found: 9 , Optimality gap: (22.8095,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 43 , No. of solutions found: 10 , Optimality gap: (22.2558,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 44 , No. of solutions found: 11 , Optimality gap: (21.7273,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 46 , No. of solutions found: 12 , Optimality gap: (20.7391,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 48 , No. of solutions found: 13 , Optimality gap: (19.8333,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 50 , No. of solutions found: 14 , Optimality gap: (19,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 52 , No. of solutions found: 15 , Optimality gap: (18.2308,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 54 , No. of solutions found: 16 , Optimality gap: (17.5185,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 56 , No. of solutions found: 17 , Optimality gap: (16.8571,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 57 , No. of solutions found: 18 , Optimality gap: (16.5439,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 58 , No. of solutions found: 19 , Optimality gap: (16.2414,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 59 , No. of solutions found: 20 , Optimality gap: (15.9492,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 60 , No. of solutions found: 21 , Optimality gap: (15.6667,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 61 , No. of solutions found: 22 , Optimality gap: (15.3934,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 62 , No. of solutions found: 23 , Optimality gap: (15.129,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 63 , No. of solutions found: 24 , Optimality gap: (14.873,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 64 , No. of solutions found: 25 , Optimality gap: (14.625,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 80 , No. of solutions found: 26 , Optimality gap: (11.5,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 85 , No. of solutions found: 27 , Optimality gap: (10.7647,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 86 , No. of solutions found: 28 , Optimality gap: (10.6279,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 87 , No. of solutions found: 29 , Optimality gap: (10.4943,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 89 , No. of solutions found: 30 , Optimality gap: (10.236,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 90 , No. of solutions found: 31 , Optimality gap: (10.1111,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 436 , No. of solutions found: 32 , Optimality gap: (1.29358,) , Validation accuracy: 0.6071428571428571\n",
      "Objective value: 464 , No. of solutions found: 33 , Optimality gap: (1.15517,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 465 , No. of solutions found: 34 , Optimality gap: (1.15054,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 466 , No. of solutions found: 35 , Optimality gap: (1.14592,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 467 , No. of solutions found: 36 , Optimality gap: (1.14133,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 469 , No. of solutions found: 37 , Optimality gap: (1.1322,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 470 , No. of solutions found: 38 , Optimality gap: (1.12766,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 471 , No. of solutions found: 39 , Optimality gap: (1.12314,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 472 , No. of solutions found: 40 , Optimality gap: (1.11864,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 473 , No. of solutions found: 41 , Optimality gap: (1.11416,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 474 , No. of solutions found: 42 , Optimality gap: (1.1097,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 475 , No. of solutions found: 43 , Optimality gap: (1.10526,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 476 , No. of solutions found: 44 , Optimality gap: (1.10084,) , Validation accuracy: 0.5928571428571429\n",
      "Objective value: 477 , No. of solutions found: 45 , Optimality gap: (1.09644,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 478 , No. of solutions found: 46 , Optimality gap: (1.09205,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 479 , No. of solutions found: 47 , Optimality gap: (1.08768,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 480 , No. of solutions found: 48 , Optimality gap: (1.08333,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 501 , No. of solutions found: 49 , Optimality gap: (0.996008,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 502 , No. of solutions found: 50 , Optimality gap: (0.992032,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 503 , No. of solutions found: 51 , Optimality gap: (0.988072,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 504 , No. of solutions found: 52 , Optimality gap: (0.984127,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 505 , No. of solutions found: 53 , Optimality gap: (0.980198,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 506 , No. of solutions found: 54 , Optimality gap: (0.976285,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 507 , No. of solutions found: 55 , Optimality gap: (0.972387,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 508 , No. of solutions found: 56 , Optimality gap: (0.968504,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 509 , No. of solutions found: 57 , Optimality gap: (0.964637,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 510 , No. of solutions found: 58 , Optimality gap: (0.960784,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 515 , No. of solutions found: 59 , Optimality gap: (0.941748,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 516 , No. of solutions found: 60 , Optimality gap: (0.937984,) , Validation accuracy: 0.6928571428571428\n",
      "No more solutions available.\n",
      "Search ended successfully.\n",
      "Best Validation Accuracy: 0.6928571428571428\n",
      "Objective value of best solution: 516\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best solution\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "val_acc = []\n",
    "\n",
    "best_val_accuracy = 0\n",
    "solution_best = None\n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_margin_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and validation accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_val) #compute output activations (predictions) for validation set\n",
    "    val_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_val.values) #compute validation accuracy and number of misclassified examples\n",
    "\n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \", Optimality gap:\", solution.get_objective_gaps(), \", Validation accuracy:\", val_accuracy)\n",
    "    \n",
    "    # Update the best solution if the current one has a higher test accuracy\n",
    "    if val_accuracy >= best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        solution_best = solution\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_margin_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_margin_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")\n",
    "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
    "print(\"Objective value of best solution:\", solution_best.get_objective_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "#compute exact training accuracy of best solution\n",
    "train_acc_best = 0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc_best+=solution_best.get_value(f'corr_pred_{k}')\n",
    "print(train_acc_best*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the Best Solution: 0.6257142857142857\n",
      "262 incorrectly classified examples out of 700 test examples\n"
     ]
    }
   ],
   "source": [
    "#Compute test accuracy of best solution\n",
    "\n",
    "# Step 1: Extract the Weights from the Best Solution\n",
    "weights_sol_best = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_sol_best[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        # This extraction is based on solution_best, which is the best solution found\n",
    "        neuron_weights_sol = [solution_best[weight_var] for weight_var in neuron_weights]\n",
    "        weights_sol_best[layer_idx].append(neuron_weights_sol)\n",
    "\n",
    "# Step 2: Compute Activations for the Test Set\n",
    "output_activations_test = compute_activations(weights_sol_best, x_test)\n",
    "\n",
    "# Step 3: Compare Predictions with True Labels and Compute Accuracy\n",
    "test_accuracy_best, incorrect_egs_test = compute_accuracy(flatten(output_activations_test), y_test.values)\n",
    "\n",
    "# Output the result\n",
    "print(\"Test Accuracy of the Best Solution:\", test_accuracy_best)\n",
    "print(len(incorrect_egs_test), \"incorrectly classified examples out of\", y_test.shape[0], \"test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.24193548387096\n"
     ]
    }
   ],
   "source": [
    "# Compute exact training accuracy of last solution\n",
    "\n",
    "max_margin_solution = max_margin_solutions.get_last_solution()\n",
    "train_acc= 0\n",
    "for k in range(x_train.shape[0]):\n",
    "    train_acc+=max_margin_solution.get_value(f'corr_pred_{k}')\n",
    "print(train_acc*100/x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the Best Solution: 0.6257142857142857\n",
      "262 incorrectly classified examples out of 700 test examples\n"
     ]
    }
   ],
   "source": [
    "#Compute test accuracy of last solution\n",
    "\n",
    "# Step 1: Extract the Weights from the Best Solution\n",
    "weights_sol = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_sol[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        # This extraction is based on solution_best, which is the best solution found\n",
    "        neuron_weights_sol = [max_margin_solution[weight_var] for weight_var in neuron_weights]\n",
    "        weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "\n",
    "# Step 2: Compute Activations for the Test Set\n",
    "output_activations_test = compute_activations(weights_sol, x_test)\n",
    "\n",
    "# Step 3: Compare Predictions with True Labels and Compute Accuracy\n",
    "test_accuracy_best, incorrect_egs_test = compute_accuracy(flatten(output_activations_test), y_test.values)\n",
    "\n",
    "# Output the result\n",
    "print(\"Test Accuracy of the Best Solution:\", test_accuracy_best)\n",
    "print(len(incorrect_egs_test), \"incorrectly classified examples out of\", y_test.shape[0], \"test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "max_obj_val=0 #maximum possible value of min weight objective\n",
    "for i in range(L):\n",
    "    max_obj_val+=(N[i]*N[i+1]) #if all weights have value 1 or -1\n",
    "max_obj_val\n",
    "\n",
    "min_weight_mdl = CpoModel(name='German Credit CP Model MW')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[min_weight_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        min_weight_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of input layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#lists containing perturbed examples x^j_up and x^j_down for all neurons j in layer 1, for all training examples x\n",
    "pert_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[min_weight_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            #definition of input layer activations\n",
    "            min_weight_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            min_weight_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first layer activations for training examples\n",
    "activations_1 = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations of perturbed examples in pert_up and pert_down\n",
    "activations_1_up = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                min_weight_mdl.add(min_weight_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features of the perturbed examples do not change \n",
    "                min_weight_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                min_weight_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definition of first layer activations for x^j_up and x^j_down\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            min_weight_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for further hidden layers \n",
    "for l in range(2, L):\n",
    "    activations_l = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and weights incoming to output layer\n",
    "activations_L = [[min_weight_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[min_weight_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "corr_pred = [min_weight_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "        #min_weight_mdl.add(activations_L[k][j]==y_train.iloc[k][j])\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "    #min_weight_mdl.add(min_weight_mdl.if_then(activations_l[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "\n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        min_weight_mdl.add(min_weight_mdl.if_then(min_weight_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "            \n",
    "min_weight_mdl.add(min_weight_mdl.sum(corr_pred)>0.7*x_train.shape[0]) # constraint for minimum training accuracy  \n",
    "\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "all_vars = activation_vars+weight_vars #lists containing all variables\n",
    "abs_weights_vars = [min_weight_mdl.abs(prev_neuron) for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing absolute values of weight variables\n",
    "\n",
    "#define the objective\n",
    "min_weight_obj = min_weight_mdl.integer_var(0, max_obj_val, name='min_weight_objective')\n",
    "min_weight_mdl.add(min_weight_obj == min_weight_mdl.sum(abs_weights_vars))\n",
    "min_weight_mdl.add(min_weight_mdl.minimize(min_weight_obj))\n",
    "\n",
    "\n",
    "# Solve the model\n",
    "min_weight_solutions = min_weight_mdl.start_search(TimeLimit=1800,LogVerbosity='Quiet',SearchType='Restart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 38 , No. of solutions found: 1 , Optimality gap: (0.973684,) , Validation accuracy: 0.6214285714285714\n",
      "Objective value: 19 , No. of solutions found: 2 , Optimality gap: (0.947368,) , Validation accuracy: 0.5642857142857143\n",
      "Objective value: 14 , No. of solutions found: 3 , Optimality gap: (0.928571,) , Validation accuracy: 0.6357142857142857\n",
      "Objective value: 13 , No. of solutions found: 4 , Optimality gap: (0.923077,) , Validation accuracy: 0.6357142857142857\n",
      "Objective value: 12 , No. of solutions found: 5 , Optimality gap: (0.916667,) , Validation accuracy: 0.6357142857142857\n",
      "Objective value: 5 , No. of solutions found: 6 , Optimality gap: (0.8,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 4 , No. of solutions found: 7 , Optimality gap: (0.75,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 3 , No. of solutions found: 8 , Optimality gap: (0.666667,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 2 , No. of solutions found: 9 , Optimality gap: (0.5,) , Validation accuracy: 0.5857142857142857\n",
      "No more solutions available.\n",
      "Search ended successfully.\n",
      "Best Validation Accuracy: 0.6357142857142857\n",
      "Objective value of best solution: 12\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best solution\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "val_acc = []\n",
    "\n",
    "best_val_accuracy = 0 #best validation accuracy\n",
    "solution_best = None #store best solution obtained \n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = min_weight_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and validation accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_val) #compute output activations (predictions) for validation set\n",
    "    val_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_val.values) #compute validation accuracy and indices of misclassified validation examples\n",
    "\n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \", Optimality gap:\", solution.get_objective_gaps(), \", Validation accuracy:\", val_accuracy)\n",
    "    \n",
    "    # Update the best solution if the current one has a higher test accuracy\n",
    "    if val_accuracy >= best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        solution_best = solution\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = min_weight_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "min_weight_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")\n",
    "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
    "print(\"Objective value of best solution:\", solution_best.get_objective_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the Best Solution: 0.59\n",
      "287 incorrectly classified examples out of 700 test examples\n"
     ]
    }
   ],
   "source": [
    "#Compute test accuracy of best solution\n",
    "\n",
    "# Step 1: Extract the Weights from the Best Solution\n",
    "weights_sol_best = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_sol_best[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        # This extraction is based on solution_best, which is the best solution found\n",
    "        neuron_weights_sol = [solution_best[weight_var] for weight_var in neuron_weights]\n",
    "        weights_sol_best[layer_idx].append(neuron_weights_sol)\n",
    "\n",
    "# Step 2: Compute Activations for the Test Set\n",
    "output_activations_test = compute_activations(weights_sol_best, x_test)\n",
    "\n",
    "# Step 3: Compare Predictions with True Labels and Compute Accuracy\n",
    "test_accuracy_best, incorrect_egs_test = compute_accuracy(flatten(output_activations_test), y_test.values)\n",
    "\n",
    "# Output the result\n",
    "print(\"Test Accuracy of the Best Solution:\", test_accuracy_best)\n",
    "print(len(incorrect_egs_test), \"incorrectly classified examples out of\", y_test.shape[0], \"test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Accuracy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "\n",
    "max_acc_mdl = CpoModel(name='German Credit CP Model MA')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_acc_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_acc_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of inout layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#lists containing perturbed examples x^j_up and x^j_down, for each neuron j of layer 1, for all training examples x\n",
    "pert_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            max_acc_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            max_acc_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first hidden layer activations for training examples \n",
    "activations_1 = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations for perturbed examples x^j_up and x^j_down\n",
    "activations_1_up = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features remain unchanged\n",
    "                max_acc_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                max_acc_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definition of first layer activations\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            max_acc_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for hidden layers\n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights of output layer\n",
    "activations_L = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "corr_pred = [max_acc_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "#max_acc_mdl.add(max_acc_mdl.sum(corr_pred)<x_train.shape[0])\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_acc_mdl.add(maximize(max_acc_mdl.sum(corr_pred)*100/x_train.shape[0]))\n",
    "            \n",
    "#breaking the symmetry for incoming weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_acc_mdl.add(max_acc_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_acc_solutions = max_acc_mdl.start_search(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value: 50 , No. of solutions found: 1 , Optimality gap: (1,) , Validation accuracy: 0.7357142857142858\n",
      "Objective value: 50.4032 , No. of solutions found: 2 , Optimality gap: (0.984,) , Validation accuracy: 0.7357142857142858\n",
      "Objective value: 61.2903 , No. of solutions found: 3 , Optimality gap: (0.631579,) , Validation accuracy: 0.7\n",
      "Objective value: 61.6935 , No. of solutions found: 4 , Optimality gap: (0.620915,) , Validation accuracy: 0.40714285714285714\n",
      "Objective value: 62.0968 , No. of solutions found: 5 , Optimality gap: (0.61039,) , Validation accuracy: 0.40714285714285714\n",
      "Objective value: 62.5 , No. of solutions found: 6 , Optimality gap: (0.6,) , Validation accuracy: 0.4142857142857143\n",
      "Objective value: 62.9032 , No. of solutions found: 7 , Optimality gap: (0.589744,) , Validation accuracy: 0.39285714285714285\n",
      "Objective value: 63.3065 , No. of solutions found: 8 , Optimality gap: (0.579618,) , Validation accuracy: 0.40714285714285714\n",
      "Objective value: 63.7097 , No. of solutions found: 9 , Optimality gap: (0.56962,) , Validation accuracy: 0.4\n",
      "Objective value: 64.1129 , No. of solutions found: 10 , Optimality gap: (0.559748,) , Validation accuracy: 0.4\n",
      "Objective value: 64.5161 , No. of solutions found: 11 , Optimality gap: (0.55,) , Validation accuracy: 0.42142857142857143\n",
      "Objective value: 64.9194 , No. of solutions found: 12 , Optimality gap: (0.540373,) , Validation accuracy: 0.4\n",
      "Objective value: 65.7258 , No. of solutions found: 13 , Optimality gap: (0.521472,) , Validation accuracy: 0.4\n",
      "Objective value: 66.129 , No. of solutions found: 14 , Optimality gap: (0.512195,) , Validation accuracy: 0.40714285714285714\n",
      "Objective value: 66.5323 , No. of solutions found: 15 , Optimality gap: (0.50303,) , Validation accuracy: 0.42142857142857143\n",
      "Objective value: 66.9355 , No. of solutions found: 16 , Optimality gap: (0.493976,) , Validation accuracy: 0.39285714285714285\n",
      "Objective value: 67.3387 , No. of solutions found: 17 , Optimality gap: (0.48503,) , Validation accuracy: 0.4\n",
      "Objective value: 67.7419 , No. of solutions found: 18 , Optimality gap: (0.47619,) , Validation accuracy: 0.40714285714285714\n",
      "Objective value: 68.1452 , No. of solutions found: 19 , Optimality gap: (0.467456,) , Validation accuracy: 0.4\n",
      "Objective value: 68.5484 , No. of solutions found: 20 , Optimality gap: (0.458824,) , Validation accuracy: 0.39285714285714285\n",
      "Objective value: 68.9516 , No. of solutions found: 21 , Optimality gap: (0.450292,) , Validation accuracy: 0.39285714285714285\n",
      "Objective value: 69.3548 , No. of solutions found: 22 , Optimality gap: (0.44186,) , Validation accuracy: 0.39285714285714285\n",
      "Objective value: 69.7581 , No. of solutions found: 23 , Optimality gap: (0.433526,) , Validation accuracy: 0.4357142857142857\n",
      "Objective value: 70.1613 , No. of solutions found: 24 , Optimality gap: (0.425287,) , Validation accuracy: 0.4357142857142857\n",
      "Objective value: 70.5645 , No. of solutions found: 25 , Optimality gap: (0.417143,) , Validation accuracy: 0.42857142857142855\n",
      "Objective value: 70.9677 , No. of solutions found: 26 , Optimality gap: (0.409091,) , Validation accuracy: 0.44285714285714284\n",
      "Objective value: 71.371 , No. of solutions found: 27 , Optimality gap: (0.40113,) , Validation accuracy: 0.4714285714285714\n",
      "Objective value: 71.7742 , No. of solutions found: 28 , Optimality gap: (0.393258,) , Validation accuracy: 0.4714285714285714\n",
      "Objective value: 72.1774 , No. of solutions found: 29 , Optimality gap: (0.385475,) , Validation accuracy: 0.4785714285714286\n",
      "Objective value: 72.5806 , No. of solutions found: 30 , Optimality gap: (0.377778,) , Validation accuracy: 0.4785714285714286\n",
      "Objective value: 72.9839 , No. of solutions found: 31 , Optimality gap: (0.370166,) , Validation accuracy: 0.4928571428571429\n",
      "Objective value: 73.3871 , No. of solutions found: 32 , Optimality gap: (0.362637,) , Validation accuracy: 0.5071428571428571\n",
      "Objective value: 73.7903 , No. of solutions found: 33 , Optimality gap: (0.355191,) , Validation accuracy: 0.5\n",
      "Objective value: 74.1935 , No. of solutions found: 34 , Optimality gap: (0.347826,) , Validation accuracy: 0.5071428571428571\n",
      "Objective value: 74.5968 , No. of solutions found: 35 , Optimality gap: (0.340541,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 75 , No. of solutions found: 36 , Optimality gap: (0.333333,) , Validation accuracy: 0.55\n",
      "Objective value: 75.4032 , No. of solutions found: 37 , Optimality gap: (0.326203,) , Validation accuracy: 0.5571428571428572\n",
      "Objective value: 75.8065 , No. of solutions found: 38 , Optimality gap: (0.319149,) , Validation accuracy: 0.5642857142857143\n",
      "Objective value: 76.2097 , No. of solutions found: 39 , Optimality gap: (0.312169,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 76.6129 , No. of solutions found: 40 , Optimality gap: (0.305263,) , Validation accuracy: 0.5428571428571428\n",
      "Objective value: 77.0161 , No. of solutions found: 41 , Optimality gap: (0.298429,) , Validation accuracy: 0.5642857142857143\n",
      "Objective value: 77.4194 , No. of solutions found: 42 , Optimality gap: (0.291667,) , Validation accuracy: 0.5285714285714286\n",
      "Objective value: 78.2258 , No. of solutions found: 43 , Optimality gap: (0.278351,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 78.629 , No. of solutions found: 44 , Optimality gap: (0.271795,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 79.0323 , No. of solutions found: 45 , Optimality gap: (0.265306,) , Validation accuracy: 0.5214285714285715\n",
      "Objective value: 79.4355 , No. of solutions found: 46 , Optimality gap: (0.258883,) , Validation accuracy: 0.5285714285714286\n",
      "Objective value: 79.8387 , No. of solutions found: 47 , Optimality gap: (0.252525,) , Validation accuracy: 0.5285714285714286\n",
      "Objective value: 80.2419 , No. of solutions found: 48 , Optimality gap: (0.246231,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 80.6452 , No. of solutions found: 49 , Optimality gap: (0.24,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 81.0484 , No. of solutions found: 50 , Optimality gap: (0.233831,) , Validation accuracy: 0.5428571428571428\n",
      "Objective value: 81.4516 , No. of solutions found: 51 , Optimality gap: (0.227723,) , Validation accuracy: 0.5428571428571428\n",
      "Objective value: 81.8548 , No. of solutions found: 52 , Optimality gap: (0.221675,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 82.2581 , No. of solutions found: 53 , Optimality gap: (0.215686,) , Validation accuracy: 0.5357142857142857\n",
      "Objective value: 82.6613 , No. of solutions found: 54 , Optimality gap: (0.209756,) , Validation accuracy: 0.5214285714285715\n",
      "Objective value: 83.0645 , No. of solutions found: 55 , Optimality gap: (0.203883,) , Validation accuracy: 0.5714285714285714\n",
      "Objective value: 83.4677 , No. of solutions found: 56 , Optimality gap: (0.198068,) , Validation accuracy: 0.5714285714285714\n",
      "Objective value: 83.871 , No. of solutions found: 57 , Optimality gap: (0.192308,) , Validation accuracy: 0.5857142857142857\n",
      "Objective value: 84.2742 , No. of solutions found: 58 , Optimality gap: (0.186603,) , Validation accuracy: 0.5857142857142857\n",
      "Objective value: 84.6774 , No. of solutions found: 59 , Optimality gap: (0.180952,) , Validation accuracy: 0.5857142857142857\n",
      "Objective value: 85.0806 , No. of solutions found: 60 , Optimality gap: (0.175355,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 85.8871 , No. of solutions found: 61 , Optimality gap: (0.164319,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 86.2903 , No. of solutions found: 62 , Optimality gap: (0.158879,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 86.6935 , No. of solutions found: 63 , Optimality gap: (0.153488,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 87.0968 , No. of solutions found: 64 , Optimality gap: (0.148148,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 87.5 , No. of solutions found: 65 , Optimality gap: (0.142857,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 87.9032 , No. of solutions found: 66 , Optimality gap: (0.137615,) , Validation accuracy: 0.7071428571428572\n",
      "Objective value: 88.3065 , No. of solutions found: 67 , Optimality gap: (0.13242,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 88.7097 , No. of solutions found: 68 , Optimality gap: (0.127273,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 89.1129 , No. of solutions found: 69 , Optimality gap: (0.122172,) , Validation accuracy: 0.6857142857142857\n",
      "Objective value: 89.5161 , No. of solutions found: 70 , Optimality gap: (0.117117,) , Validation accuracy: 0.6142857142857143\n",
      "Objective value: 89.9194 , No. of solutions found: 71 , Optimality gap: (0.112108,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 90.7258 , No. of solutions found: 72 , Optimality gap: (0.102222,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 91.129 , No. of solutions found: 73 , Optimality gap: (0.0973451,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 91.9355 , No. of solutions found: 74 , Optimality gap: (0.0877193,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 92.7419 , No. of solutions found: 75 , Optimality gap: (0.0782609,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 93.5484 , No. of solutions found: 76 , Optimality gap: (0.0689655,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 93.9516 , No. of solutions found: 77 , Optimality gap: (0.0643777,) , Validation accuracy: 0.6857142857142857\n",
      "Objective value: 94.3548 , No. of solutions found: 78 , Optimality gap: (0.0598291,) , Validation accuracy: 0.6571428571428571\n",
      "Objective value: 94.7581 , No. of solutions found: 79 , Optimality gap: (0.0553191,) , Validation accuracy: 0.6714285714285714\n",
      "Objective value: 95.1613 , No. of solutions found: 80 , Optimality gap: (0.0508475,) , Validation accuracy: 0.7\n",
      "Objective value: 95.5645 , No. of solutions found: 81 , Optimality gap: (0.0464135,) , Validation accuracy: 0.6285714285714286\n",
      "Objective value: 95.9677 , No. of solutions found: 82 , Optimality gap: (0.0420168,) , Validation accuracy: 0.7071428571428572\n",
      "Objective value: 96.371 , No. of solutions found: 83 , Optimality gap: (0.0376569,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 96.7742 , No. of solutions found: 84 , Optimality gap: (0.0333333,) , Validation accuracy: 0.6928571428571428\n",
      "Objective value: 97.1774 , No. of solutions found: 85 , Optimality gap: (0.0290456,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 97.5806 , No. of solutions found: 86 , Optimality gap: (0.0247934,) , Validation accuracy: 0.7214285714285714\n",
      "Objective value: 97.9839 , No. of solutions found: 87 , Optimality gap: (0.0205761,) , Validation accuracy: 0.7214285714285714\n",
      "Objective value: 98.3871 , No. of solutions found: 88 , Optimality gap: (0.0163934,) , Validation accuracy: 0.7571428571428571\n",
      "Objective value: 98.7903 , No. of solutions found: 89 , Optimality gap: (0.0122449,) , Validation accuracy: 0.6642857142857143\n",
      "Objective value: 99.1935 , No. of solutions found: 90 , Optimality gap: (0.00813008,) , Validation accuracy: 0.7\n",
      "Objective value: 99.5968 , No. of solutions found: 91 , Optimality gap: (0.00404858,) , Validation accuracy: 0.6785714285714286\n",
      "Objective value: 100 , No. of solutions found: 92 , Optimality gap: (2.2311e-14,) , Validation accuracy: 0.7071428571428572\n",
      "No more solutions available.\n",
      "Search ended successfully.\n",
      "Best Validation Accuracy: 0.7571428571428571\n",
      "Objective value of best solution: 98.3871\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best solution\n",
    "objective_vals = []\n",
    "runtimes = []\n",
    "numsols = []\n",
    "val_acc = []\n",
    "\n",
    "best_val_accuracy = 0 #best validation accuracy\n",
    "solution_best = None #store best solution obtained \n",
    "\n",
    "# Fetch the first solution\n",
    "try:\n",
    "    solution = max_acc_solutions.next()\n",
    "except StopIteration:\n",
    "    solution = None\n",
    "\n",
    "#for each solution, print the objective value and validation accuracy\n",
    "while solution:\n",
    "    weights_sol = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_sol[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            neuron_weights_sol = [solution[weight_var] for weight_var in neuron_weights]\n",
    "            weights_sol[layer_idx].append(neuron_weights_sol)\n",
    "    \n",
    "    output_activations = compute_activations(weights_sol, x_val) #compute output activations (predictions) for validation set\n",
    "    val_accuracy, incorrect_egs = compute_accuracy(flatten(output_activations), y_val.values) #compute validation accuracy and indices of misclassified validation examples\n",
    "\n",
    "    oval = solution.get_objective_value()\n",
    "    rtime = solution.get_solver_infos()['TotalTime']\n",
    "    nsols = solution.get_solver_infos()['NumberOfSolutions']\n",
    "    \n",
    "    objective_vals.append(oval)\n",
    "    runtimes.append(rtime)\n",
    "    numsols.append(nsols)\n",
    "    val_acc.append(val_accuracy)\n",
    "    \n",
    "    print(\"Objective value:\", oval, \", No. of solutions found:\", nsols, \", Optimality gap:\", solution.get_objective_gaps(), \", Validation accuracy:\", val_accuracy)\n",
    "    \n",
    "    # Update the best solution if the current one has a higher test accuracy\n",
    "    if val_accuracy >= best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        solution_best = solution\n",
    "    \n",
    "    try:\n",
    "        # Try fetching the next solution\n",
    "        solution = max_acc_solutions.next()\n",
    "    except StopIteration:\n",
    "        print(\"No more solutions available.\")\n",
    "        solution = None\n",
    "    except CpoException as e:\n",
    "        # Check if the exception is due to no more solutions being available\n",
    "        if 'SearchWaiting' in str(e):\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# End search\n",
    "max_acc_solutions.end_search()\n",
    "\n",
    "print(\"Search ended successfully.\")\n",
    "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
    "print(\"Objective value of best solution:\", solution_best.get_objective_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the Best Solution: 0.6457142857142857\n",
      "248 incorrectly classified examples out of 700 test examples\n"
     ]
    }
   ],
   "source": [
    "#Compute test accuracy of best solution \n",
    "\n",
    "# Step 1: Extract the Weights from the Best Solution\n",
    "weights_sol_best = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_sol_best[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        # This extraction is based on solution_best, which is the best solution found\n",
    "        neuron_weights_sol = [solution_best[weight_var] for weight_var in neuron_weights]\n",
    "        weights_sol_best[layer_idx].append(neuron_weights_sol)\n",
    "\n",
    "# Step 2: Compute Activations for the Test Set\n",
    "output_activations_test = compute_activations(weights_sol_best, x_test)\n",
    "\n",
    "# Step 3: Compare Predictions with True Labels and Compute Accuracy\n",
    "test_accuracy_best, incorrect_egs_test = compute_accuracy(flatten(output_activations_test), y_test.values)\n",
    "\n",
    "# Output the result\n",
    "print(\"Test Accuracy of the Best Solution:\", test_accuracy_best)\n",
    "print(len(incorrect_egs_test), \"incorrectly classified examples out of\", y_test.shape[0], \"test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643.37"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc_solutions.get_last_result().get_solver_infos()['TotalTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH 2: Improving the Maximum Accuracy Model Using the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "#Define the array containing the number of neurons in each layer\n",
    "N = [x_train.shape[1], best_hp.values['units_input']]  \n",
    "for i in range(1,best_hp.values['num_layers']+1):\n",
    "    N.append(best_hp.values[f'units_{i}'])\n",
    "N.append(1) #1 neuron in output layer\n",
    "print(N)\n",
    "L = len(N) - 1  # index of last layer in the network\n",
    "assert all(N[l]>1 for l in range(L)) #number of neurons in all layers except the last one should be greater than 1 \n",
    "\n",
    "idx_cred_amt = x_train.columns.get_loc('credit_amount') #extract index of credit_amount column\n",
    "idx_cred_dur = x_train.columns.get_loc('credit_duration_months') #extract index of credit_duration column\n",
    "epsilon_amt = 100 #epsilon value for credit_amount feature\n",
    "epsilon_dur = 2 #epsilon value for credit_duration feature\n",
    "x_max = x_train.max()[idx_cred_amt] #extract maximum value in the dataframe (present in credit_amount column)\n",
    "\n",
    "\n",
    "max_acc_mdl = CpoModel(name='German Credit CP Model MA')\n",
    "\n",
    "#initialize lists for weights and activations\n",
    "weights = []\n",
    "activations = []\n",
    "\n",
    "# list containing activations for all neurons of input layer\n",
    "activations_0 = [[max_acc_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[0]):\n",
    "        max_acc_mdl.add(activations_0[k][j] == x_train.iloc[k][j]) #definition of inout layer activations\n",
    "\n",
    "\n",
    "activations.append(activations_0)\n",
    "\n",
    "#lists containing perturbed examples x^j_up and x^j_down, for each neuron j of layer 1, for all training examples x\n",
    "pert_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_up_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "pert_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt,name=f\"x_down_{i}_{j}_{k}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)]\n",
    "activations_0_up = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_up_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "activations_0_down = [[[max_acc_mdl.integer_var(0,x_max+epsilon_amt, name=f\"n_down_0_{i}_{k}_{j}\") for i in range(1,N[0]+1)] for j in range(1,N[1]+1)] for k in range(1,x_train.shape[0]+1)] #constraint for activations of input layer to be equal to feature values of input samples\n",
    "\n",
    "\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):    \n",
    "            max_acc_mdl.add(activations_0_up[k][j][i] == pert_up[k][j][i])\n",
    "            max_acc_mdl.add(activations_0_down[k][j][i] == pert_down[k][j][i])\n",
    "\n",
    "#first hidden layer activations for training examples \n",
    "activations_1 = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_1 = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "#for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) >= 0, activations_1[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0[k]) < 0, activations_1[k][j] == -1))\n",
    "     \n",
    "activations.append(activations_1)\n",
    "weights.append(weights_1)\n",
    "\n",
    "#first hidden layer activations for perturbed examples x^j_up and x^j_down\n",
    "activations_1_up = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_up_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "activations_1_down = [[[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_down_1_{i}_{k}_{j}\") for i in range(1,N[1]+1)] for j in range(1, N[1] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[1]):\n",
    "        for i in range(N[0]):\n",
    "            if i==idx_cred_amt:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_amt))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            elif i==idx_cred_dur:\n",
    "                #definitions of x^j_up and x^j_down\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_up[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_up[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]>0,pert_down[k][j][i]==x_train.iloc[k][i]+epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]<0,pert_down[k][j][i]==x_train.iloc[k][i]-epsilon_dur))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(weights_1[j][i]==0,(pert_up[k][j][i]==0) & (pert_down[k][j][i]==0)))\n",
    "            else:\n",
    "                #values of other features remain unchanged\n",
    "                max_acc_mdl.add(pert_up[k][j][i]==x_train.iloc[k][i])\n",
    "                max_acc_mdl.add(pert_down[k][j][i]==x_train.iloc[k][i])\n",
    "        for i in range(N[1]):\n",
    "            #definition of first layer activations\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) >= 0, activations_1_up[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_up[k][j]) < 0, activations_1_up[k][j][i] == -1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) >= 0, activations_1_down[k][j][i] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[i], activations_0_down[k][j]) < 0, activations_1_down[k][j][i] == -1))\n",
    "            max_acc_mdl.add((activations_1_up[k][j][i]==activations_1[k][i]) & (activations_1_down[k][j][i]==activations_1[k][i])) #robustness constraint: equality of first layer activations of x and x^j_up and x^j_down\n",
    "\n",
    "#activations and weights for hidden layers\n",
    "for l in range(2, L):\n",
    "    activations_l = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "    \n",
    "\n",
    "    weights_l = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(x_train.shape[0]):\n",
    "        for j in range(N[l]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) >= 0, activations_l[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations[l-1][k]) < 0, activations_l[k][j] == -1))\n",
    "     \n",
    "\n",
    "    activations.append(activations_l)\n",
    "    weights.append(weights_l)\n",
    "\n",
    "#activations and incoming weights of output layer\n",
    "activations_L = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1, x_train.shape[0] + 1)]\n",
    "weights_L = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "\n",
    "activations.append(activations_L)\n",
    "weights.append(weights_L)\n",
    "\n",
    "corr_pred = [max_acc_mdl.integer_var(0,1,name=f'corr_pred_{k}') for k in range(x_train.shape[0])] #boolean variable to map whether prediction for input k is correct\n",
    "for k in range(x_train.shape[0]):\n",
    "    for j in range(N[L]):\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) >= 0, activations_L[k][j] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations[L-1][k]) < 0, activations_L[k][j] == -1)) \n",
    "\n",
    "    if N[L]==1: #if output label has a single dimension\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]==y_train.iloc[k],corr_pred[k]==1)) #corr_pred=1 if training sample k is assigned the correct label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(activations_L[k][0]!=y_train.iloc[k],corr_pred[k]==0)) #corr_pred=0 if training sample k is not assigned the correct label\n",
    "    else: #if dimension of output label > 1 \n",
    "        #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_and([activations_L[k][j] == y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 1))\n",
    "        max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.logical_or([activations_L[k][j] != y_train.iloc[k][j] for j in range(N[L])]), corr_pred[k] == 0))\n",
    "#max_acc_mdl.add(max_acc_mdl.sum(corr_pred)<x_train.shape[0])\n",
    "\n",
    "\n",
    "activation_vars = [var for layer in activations for sample in layer for var in sample] #lists containing all activation variables\n",
    "weight_vars = [prev_neuron for layer in weights for neuron in layer for prev_neuron in neuron] #lists containing all weight variables\n",
    "\n",
    "\n",
    "# Define the objective\n",
    "max_acc_mdl.add(maximize(max_acc_mdl.sum(corr_pred)*100/x_train.shape[0]))\n",
    "            \n",
    "#breaking the symmetry for incoming weights of first layer\n",
    "for i in range(N[1]-1):\n",
    "    max_acc_mdl.add(max_acc_mdl.lexicographic(weights[0][i],weights[0][i+1]))\n",
    "\n",
    "# Solve the model\n",
    "max_acc_solution = max_acc_mdl.solve(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute activations of dataset x\n",
    "def compute_activations(weights_solution, x): \n",
    "    activations = [x.values]  # Use the dataset features as the initial activations\n",
    "\n",
    "    for layer_idx in range(1, len(N)):\n",
    "        prev_layer_activations = activations[-1]\n",
    "        current_layer_weights = weights_solution[layer_idx - 1]\n",
    "        \n",
    "        current_layer_activations = []\n",
    "        for sample_activations in prev_layer_activations:\n",
    "            layer_activations = []\n",
    "            for neuron_weights in current_layer_weights:\n",
    "                activation = sum(weight * sample_activation for weight, sample_activation in zip(neuron_weights, sample_activations))\n",
    "                # Apply sign function\n",
    "                if activation >= 0:\n",
    "                    layer_activations.append(1)\n",
    "                else:\n",
    "                    layer_activations.append(-1)\n",
    "            current_layer_activations.append(layer_activations)\n",
    "        \n",
    "        activations.append(current_layer_activations)\n",
    "    \n",
    "    return activations[-1]  # Return the activations of the output layer\n",
    "\n",
    "# Compare Predictions with True Labels and Compute Accuracy\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    correct_predictions = sum(pred == true for pred, true in zip(predictions, true_labels))\n",
    "    corr_class = predictions==true_labels\n",
    "    incorr_eg_idx = np.where(corr_class==False)[0].tolist()\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    return accuracy, incorr_eg_idx\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [1], [1]]\n",
      "[[-1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [1], [-1], [1], [1], [-1], [-1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [-1], [-1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [-1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [-1], [-1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [-1], [1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [-1], [-1], [-1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [-1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [1], [-1], [1], [-1], [1], [1], [1], [-1], [1], [1], [1], [1], [-1], [1], [1]]\n",
      "Val Accuracy: 0.7071428571428572\n",
      "41 incorrectly classified out of 140\n",
      "Test Accuracy: 0.65\n",
      "245 incorrectly classified out of 700\n"
     ]
    }
   ],
   "source": [
    "#extract weights from the solution\n",
    "weights_solution = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_solution[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        weights_solution[layer_idx].append([max_acc_solution[weight_var] for weight_var in neuron_weights]) \n",
    "\n",
    "# Compute the activations for the validation set\n",
    "output_activations = compute_activations(weights_solution, x_val)\n",
    "print(output_activations)\n",
    "test_activations = compute_activations(weights_solution, x_test)\n",
    "print(test_activations)\n",
    "\n",
    "# compute validation accuracy\n",
    "val_accuracy,incorrect_egs_val = compute_accuracy(flatten(output_activations), y_val.values)\n",
    "print('Val Accuracy:',val_accuracy)\n",
    "print(len(incorrect_egs_val),\"incorrectly classified out of\",y_val.shape[0])\n",
    "\n",
    "# compute test accuracy\n",
    "test_accuracy,incorrect_egs_test = compute_accuracy(flatten(test_activations),y_test.values)\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print(len(incorrect_egs_test),\"incorrectly classified out of\",y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the model with new constraints for misclassified examples\n",
    "def update_model_with_misclassified(mdl, x, y, misclassified_indices):\n",
    "    activations_msc = []\n",
    "    weights = []\n",
    "    x_msc = [x.values.tolist()[i] for i in misclassified_indices]\n",
    "    y_msc = [y.values.tolist()[i] for i in misclassified_indices]\n",
    "    x_max = max(np.max(np.array(x_msc),axis=0)[idx_cred_amt],x_train.max()[idx_cred_amt])\n",
    "    activations_0_msc = [[max_acc_mdl.integer_var(0,x_max, name=f\"n_0_{j}_{k}\") for j in range(1,N[0]+1)] for k in range(1,len(misclassified_indices)+1)]\n",
    "    for k in range(len(x_msc)):\n",
    "        for j in range(N[0]):\n",
    "            max_acc_mdl.add(activations_0_msc[k][j] == x_msc[k][j])\n",
    "\n",
    "\n",
    "    activations_msc.append(activations_0_msc)\n",
    "\n",
    "    activations_1_msc = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_1_{j}_{k}\") for j in range(1, N[1] + 1)] for k in range(1,len(misclassified_indices)+1)]\n",
    "    weights_1 = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_1_{j}\") for i in range(1, N[0] + 1)] for j in range(1, N[1] + 1)]\n",
    "    \n",
    "    #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "    for k in range(len(x_msc)):\n",
    "        for j in range(N[1]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0_msc[k]) >= 0, activations_1_msc[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_1[j], activations_0_msc[k]) < 0, activations_1_msc[k][j] == -1))\n",
    "     \n",
    "    activations_msc.append(activations_1_msc)\n",
    "    weights.append(weights_1)\n",
    "\n",
    "    #activations and weights for hidden layers and output layer\n",
    "    for l in range(2, L):\n",
    "        activations_l_msc = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{l}_{j}_{k}\") for j in range(1, N[l] + 1)] for k in range(1,len(misclassified_indices)+1)]\n",
    "        weights_l = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{l}_{j}\") for i in range(1, N[l - 1] + 1)] for j in range(1, N[l] + 1)]\n",
    "    \n",
    "        #for each training sample, defining the activations of current layer as the sign of dot product of incoming weights and activations of previous layer\n",
    "        for k in range(len(x_msc)):\n",
    "            for j in range(N[l]):\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations_msc[l-1][k]) >= 0, activations_l_msc[k][j] == 1))\n",
    "                max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_l[j], activations_msc[l-1][k]) < 0, activations_l_msc[k][j] == -1))\n",
    "     \n",
    "        activations_msc.append(activations_l_msc)\n",
    "        weights.append(weights_l)\n",
    "\n",
    "\n",
    "    activations_L_msc = [[max_acc_mdl.integer_var(domain=(-1,1), name=f\"n_{L}_{j}_{k}\") for j in range(1, N[L] + 1)] for k in range(1,len(misclassified_indices)+1)]\n",
    "    weights_L = [[max_acc_mdl.integer_var(domain=(-1,0,1), name=f\"w_{i}_{L}_{j}\") for i in range(1, N[L - 1] + 1)] for j in range(1, N[L] + 1)]\n",
    "\n",
    "    for k in range(len(x_msc)):\n",
    "        for j in range(N[L]):\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations_msc[L-1][k]) >= 0, activations_L_msc[k][j] == 1))\n",
    "            max_acc_mdl.add(max_acc_mdl.if_then(max_acc_mdl.scal_prod(weights_L[j], activations_msc[L-1][k]) < 0, activations_L_msc[k][j] == -1)) \n",
    "\n",
    "        if N[L]==1: #if output label has a single dimension\n",
    "            max_acc_mdl.add(activations_L_msc[k][0]==y_msc[k]) \n",
    "        else: #if dimension of output label > 1 \n",
    "            #prediction is correct if activations of all neurons in output layer match the corresponding features of the true label\n",
    "            max_acc_mdl.add(max_acc_mdl.logical_and([activations_L_msc[k][j] == y_msc[k][j] for j in range(N[L])]))\n",
    "\n",
    "    activations_msc.append(activations_L_msc)\n",
    "    weights.append(weights_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Validation Accuracy: 0.7071428571428572\n",
      "41 incorrectly classified examples out of 140 validation examples\n",
      "[1, 3, 4, 5, 15, 16, 32, 38, 42, 55, 68, 71, 72, 73, 74, 78, 84, 95, 97, 100, 113, 120]\n",
      "[0, 21, 23, 26, 27, 37, 47, 51, 62, 70, 80, 81, 89, 99, 103, 109, 122, 127, 132]\n",
      "Iteration 2\n",
      "Validation Accuracy: 0.6928571428571428\n",
      "43 incorrectly classified examples out of 140 validation examples\n",
      "[3, 4, 16, 32, 33, 38, 42, 72, 73, 78, 84, 95, 97, 100, 116, 120, 135, 136]\n",
      "[6, 7, 10, 17, 21, 23, 25, 26, 35, 43, 51, 65, 67, 69, 79, 82, 89, 111, 114, 119, 123, 126, 127, 138, 139]\n"
     ]
    },
    {
     "ename": "CpoSolverException",
     "evalue": "Nothing to read from local solver process. Process seems to have been stopped (rc=-9).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCpoSolverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m update_model_with_misclassified(max_acc_mdl, x_val, y_val, selected_misclassified)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Solve the updated model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m max_acc_solution \u001b[38;5;241m=\u001b[39m \u001b[43mmax_acc_mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSearchType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRestart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTimeLimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLogVerbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuiet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#max_acc_solutions = max_acc_mdl.start_search(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#print(\"Final solution:\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#print(\"Objective value:\", max_acc_solution.get_objective_value())\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#print(\"Is solution optimal:\", max_acc_solution.is_solution_optimal())\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#print(\"Solver Infos:\",max_acc_solution.get_solver_infos())\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/model.py:1289\u001b[0m, in \u001b[0;36mCpoModel.solve\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Solves the model.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \n\u001b[1;32m   1248\u001b[0m \u001b[38;5;124;03mThis method solves the model using the appropriate :class:`~docplex.cp.solver.solver.CpoSolver`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;124;03m    :class:`~docplex.cp.utils.CpoException`: (or derived) if error.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_solver(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1289\u001b[0m msol \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m solver\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m msol\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver.py:714\u001b[0m, in \u001b[0;36mCpoSolver.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mlog_exceptions:\n\u001b[1;32m    713\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_status(STATUS_IDLE)\n\u001b[1;32m    716\u001b[0m stime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m stime\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver.py:707\u001b[0m, in \u001b[0;36mCpoSolver.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m stime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     msol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;66;03m# Check if aborted in the mean time\u001b[39;00m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_status_aborted():\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver_local.py:213\u001b[0m, in \u001b[0;36mCpoSolverLocal.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_message(CMD_SOLVE_MODEL)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Wait JSON result\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m jsol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_json_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEVT_SOLVE_RESULT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Build result object\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_result_object(CpoSolveResult, jsol)\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver_local.py:572\u001b[0m, in \u001b[0;36mCpoSolverLocal._wait_json_result\u001b[0;34m(self, evt)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Wait for a JSON result while forwarding logs if any.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    evt: Event to wait for\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    JSON solution string, decoded from UTF8\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Wait JSON result\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Store last json result\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_last_json_result_string(data)\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver_local.py:467\u001b[0m, in \u001b[0;36mCpoSolverLocal._wait_event\u001b[0;34m(self, xevt)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Read events\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Read and process next message\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     evt, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evt \u001b[38;5;241m==\u001b[39m xevt:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver_local.py:631\u001b[0m, in \u001b[0;36mCpoSolverLocal._read_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Read a message from the solver process\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    Tuple (evt, data)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Read message header\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (frame[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0xCA\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (frame[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0xFE\u001b[39m):\n\u001b[1;32m    633\u001b[0m     erline \u001b[38;5;241m=\u001b[39m frame \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_error_message()\n",
      "File \u001b[0;32m~/Documents/BNNs/venv/lib/python3.8/site-packages/docplex/cp/solver/solver_local.py:691\u001b[0m, in \u001b[0;36mCpoSolverLocal._read_frame\u001b[0;34m(self, nbb)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m             rc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CpoSolverException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNothing to read from local solver process. Process seems to have been stopped (rc=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(rc))\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CpoSolverException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead only \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m bytes when \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(data), nbb))\n",
      "\u001b[0;31mCpoSolverException\u001b[0m: Nothing to read from local solver process. Process seems to have been stopped (rc=-9)."
     ]
    }
   ],
   "source": [
    "num_iterations = 3\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Iteration {iteration + 1}\")\n",
    "\n",
    "    # Compute activations for the validation set\n",
    "    weights_solution = {}\n",
    "    for layer_idx, layer_weights in enumerate(weights):\n",
    "        weights_solution[layer_idx] = []\n",
    "        for neuron_weights in layer_weights:\n",
    "            weights_solution[layer_idx].append([max_acc_solution[weight_var] for weight_var in neuron_weights])\n",
    "    val_activations = compute_activations(weights_solution, x_val)\n",
    "\n",
    "\n",
    "    # Compute accuracy on the validation set & identify misclassified examples\n",
    "    val_accuracy,incorrect_egs_val= compute_accuracy(flatten(val_activations), y_val.values)\n",
    "    print('Validation Accuracy:',val_accuracy)\n",
    "    print(len(incorrect_egs_val),\"incorrectly classified examples out of\",y_val.shape[0],\"validation examples\")\n",
    "\n",
    "\n",
    "    # If no misclassifications remain, break the loop\n",
    "    if not incorrect_egs_val:\n",
    "        print(\"No more misclassified examples.\")\n",
    "        break\n",
    "\n",
    "    # Select equal number of misclassified examples from each class\n",
    "    class_0_misclassified = [i for i in incorrect_egs_val if y_val.iloc[i] == -1]\n",
    "    class_1_misclassified = [i for i in incorrect_egs_val if y_val.iloc[i] == 1]\n",
    "    print(class_0_misclassified)\n",
    "    print(class_1_misclassified)\n",
    "\n",
    "    if (len(class_0_misclassified)==0 & len(class_1_misclassified)==0):\n",
    "        print(\"No more misclassified examples from both classes.\")\n",
    "        break\n",
    "\n",
    "    num_to_select = 5\n",
    "    selected_misclassified = random.sample(class_0_misclassified, num_to_select) + random.sample(class_1_misclassified, num_to_select)\n",
    "    \n",
    "\n",
    "    # Add selected misclassified examples to the training set\n",
    "    update_model_with_misclassified(max_acc_mdl, x_val, y_val, selected_misclassified)\n",
    "\n",
    "    # Solve the updated model\n",
    "    max_acc_solution = max_acc_mdl.solve(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')\n",
    "    #max_acc_solutions = max_acc_mdl.start_search(SearchType='Restart', TimeLimit=1800, LogVerbosity='Quiet')\n",
    "    #print(\"Final solution:\")\n",
    "    #print(\"Objective value:\", max_acc_solution.get_objective_value())\n",
    "    #print(\"Is solution optimal:\", max_acc_solution.is_solution_optimal())\n",
    "    #print(\"Solver Infos:\",max_acc_solution.get_solver_infos())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute test accuracy for the final solution\n",
    "#extract weights from the solution\n",
    "weights_solution = {}\n",
    "for layer_idx, layer_weights in enumerate(weights):\n",
    "    weights_solution[layer_idx] = []\n",
    "    for neuron_weights in layer_weights:\n",
    "        weights_solution[layer_idx].append([max_acc_solutions[weight_var] for weight_var in neuron_weights]) \n",
    "\n",
    "# Compute the activations for the test set\n",
    "test_activations = compute_activations(weights_solution, x_test)\n",
    "\n",
    "# Assume y_test is your true labels for the test set\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "test_accuracy,incorrect_egs = compute_accuracy(flatten(test_activations), y_test.values)\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "print(len(incorrect_egs),\"incorrectly classified examples out of\",y_test.shape[0],\"test examples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
